{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ Chapter 7 - Interpreting Query Intent through Semantic Search ]\n",
    "# Setting up the Reviews Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aips.data_loaders.cities as cities\n",
    "import aips.data_loaders.reviews as reviews\n",
    "from aips import get_engine\n",
    "from aips.spark import get_spark_session\n",
    "from aips.spark.dataframe import from_csv\n",
    "\n",
    "spark = get_spark_session()\n",
    "\n",
    "engine = get_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n",
      "._reviews.csv\n",
      "reviews.csv\n",
      "entities.csv\n",
      "._cities.csv\n",
      "cities.csv\n"
     ]
    }
   ],
   "source": [
    "#Get datasets\n",
    "![ ! -d 'reviews' ] && git clone --depth 1 https://github.com/ai-powered-search/reviews.git\n",
    "! cd reviews && git pull\n",
    "! cd reviews && mkdir -p '../data/reviews/' && tar -xvf reviews.tgz -C '../data/reviews/' && tar -xvf entities.tgz -C '../data/reviews/' && tar -xvf cities.tgz -C '../data/reviews/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiping \"reviews\" collection\n",
      "Creating \"reviews\" collection\n",
      "Status: Success\n",
      "\n",
      "Loading Reviews...\n"
     ]
    }
   ],
   "source": [
    "reviews_collection = get_engine(\"solr\").create_collection(\"reviews\")\n",
    "reviews_data = reviews.load_dataframe(\"data/reviews/reviews.csv\")\n",
    "reviews_collection.write(reviews_data)\n",
    "\n",
    "entities_collection =  get_engine(\"solr\").create_collection(\"entities\")\n",
    "entities_dataframe = from_csv(\"data/reviews/entities.csv\")\n",
    "cities_dataframe = cities.load_dataframe(\"data/reviews/cities.csv\")\n",
    "entities_collection.write(entities_dataframe)\n",
    "entities_collection.write(cities_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviews Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing 7.1\n",
    "### Loading and indexing the reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiping \"reviews\" collection\n",
      "Creating \"reviews\" collection\n",
      "Schema: {\n",
      "  \"class\": \"reviews\",\n",
      "  \"properties\": [\n",
      "    {\n",
      "      \"name\": \"__id\",\n",
      "      \"dataType\": [\n",
      "        \"text\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"content\",\n",
      "      \"dataType\": [\n",
      "        \"text\"\n",
      "      ],\n",
      "      \"fielddata\": true\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"categories\",\n",
      "      \"dataType\": [\n",
      "        \"text\"\n",
      "      ],\n",
      "      \"copy_to\": \"doc_type\",\n",
      "      \"fielddata\": true\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"doc_type\",\n",
      "      \"dataType\": [\n",
      "        \"text\"\n",
      "      ],\n",
      "      \"fielddata\": true\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"stars_rating\",\n",
      "      \"dataType\": [\n",
      "        \"int\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"city\",\n",
      "      \"dataType\": [\n",
      "        \"text\"\n",
      "      ],\n",
      "      \"fielddata\": true\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"state\",\n",
      "      \"dataType\": [\n",
      "        \"text\"\n",
      "      ],\n",
      "      \"fielddata\": true\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"business_name\",\n",
      "      \"dataType\": [\n",
      "        \"text\"\n",
      "      ],\n",
      "      \"fielddata\": true\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"name\",\n",
      "      \"dataType\": [\n",
      "        \"text\"\n",
      "      ],\n",
      "      \"fielddata\": true\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"location_coordinates\",\n",
      "      \"dataType\": [\n",
      "        \"geoCoordinates\"\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Status: <Response [200]>\n",
      "Response: <Response [200]>\n",
      "\n",
      "Loading Reviews...\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- business_name: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- stars_rating: integer (nullable = true)\n",
      " |-- location_coordinates: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Cannot write incompatible data to table 'Reviews':\n- Cannot find data for output column 'doc_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m reviews_collection \u001b[38;5;241m=\u001b[39m engine\u001b[38;5;241m.\u001b[39mcreate_collection(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreviews\u001b[39m\u001b[38;5;124m\"\u001b[39m, log\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m reviews_data \u001b[38;5;241m=\u001b[39m reviews\u001b[38;5;241m.\u001b[39mload_dataframe(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/reviews/reviews.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mreviews_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreviews_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/engines/weaviate/WeaviateCollection.py:179\u001b[0m, in \u001b[0;36mWeaviateCollection.write\u001b[0;34m(self, dataframe, overwrite)\u001b[0m\n\u001b[1;32m    177\u001b[0m dataframe \u001b[38;5;241m=\u001b[39m rename_id_field(dataframe)\n\u001b[1;32m    178\u001b[0m dataframe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_dataframe_with_missing_columns(dataframe)\n\u001b[0;32m--> 179\u001b[0m \u001b[43mdataframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mio.weaviate.spark.Weaviate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit()\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully written \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataframe\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m documents\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:968\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 968\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Cannot write incompatible data to table 'Reviews':\n- Cannot find data for output column 'doc_type'"
     ]
    }
   ],
   "source": [
    "reviews_collection = engine.create_collection(\"reviews\", log=True)\n",
    "reviews_data = reviews.load_dataframe(\"data/reviews/reviews.csv\")\n",
    "reviews_collection.write(reviews_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing 7.2 and Figure 7.2 - 7.7\n",
    "\n",
    "#### Located in the [Semantic Search Application](2.semantic-search.ipynb#listing-7.2) notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing 7.3\n",
    "<a id='listing-7.3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_entities(dataframe, limit=10):\n",
    "    print(\"Entities\")\n",
    "    dataframe.drop(\"semantic_function\").show(limit, truncate=20)\n",
    "    print(\"... Entities continued\")\n",
    "    dataframe.filter(dataframe.type == \"semantic_function\") \\\n",
    "        .select(\"id\", \"semantic_function\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_dataframe = from_csv(\"data/reviews/entities.csv\", log=False)\n",
    "display_entities(entities_dataframe, limit=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cities Dataset (Geonames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing 7.4\n",
    "<a id='listing-7.4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_collection = engine.create_collection(\"entities\")\n",
    "entities_dataframe = from_csv(\"data/reviews/entities.csv\")\n",
    "cities_dataframe = cities.load_dataframe(\"data/reviews/cities.csv\")\n",
    "entities_collection.write(entities_dataframe)\n",
    "entities_collection.write(cities_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(engine))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enities Dataset (Manually-specified Knowledge Graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing 7.5\n",
    "<a id='listing-7.5'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s add_tag_type_commands,add_rag_request_handler_config engines/solr/SolrEngine.py\n",
    "add_tag_type_commands = [{\n",
    "    \"add-field-type\": {\n",
    "        \"name\": \"tag\",\n",
    "        \"class\": \"solr.TextField\",\n",
    "        \"postingsFormat\": \"FST50\",\n",
    "        \"omitNorms\": \"true\",\n",
    "        \"omitTermFreqAndPositions\": \"true\",\n",
    "        \"indexAnalyzer\": {\n",
    "            \"tokenizer\": {\"class\": \"solr.StandardTokenizerFactory\"},\n",
    "            \"filters\": [\n",
    "                {\"class\": \"solr.EnglishPossessiveFilterFactory\"},\n",
    "                {\"class\": \"solr.ASCIIFoldingFilterFactory\"},\n",
    "                {\"class\": \"solr.LowerCaseFilterFactory\"},\n",
    "                {\"class\": \"solr.ConcatenateGraphFilterFactory\",\n",
    "                 \"preservePositionIncrements\": \"false\"}]},\n",
    "        \"queryAnalyzer\": {\n",
    "            \"tokenizer\": {\"class\": \"solr.StandardTokenizerFactory\"},\n",
    "            \"filters\": [{\"class\": \"solr.EnglishPossessiveFilterFactory\"},\n",
    "                        {\"class\": \"solr.ASCIIFoldingFilterFactory\"},\n",
    "                        {\"class\": \"solr.LowerCaseFilterFactory\"}]}}\n",
    "    },\n",
    "    {\"add-field\": {\"name\": \"name_tag\", \"type\": \"tag\",\n",
    "                   \"stored\": \"false\"}},\n",
    "    {\"add-copy-field\": {\"source\": \"surface_form\",\n",
    "                        \"dest\": [\"name_tag\"]}}]\n",
    "\n",
    "add_tag_request_handler_config = {\n",
    "        \"add-requesthandler\" : {\n",
    "            \"name\": \"/tag\",\n",
    "            \"class\": \"solr.TaggerRequestHandler\",\n",
    "            \"defaults\": {\n",
    "                \"field\": \"name_tag\",\n",
    "                \"json.nl\": \"map\",\n",
    "                \"sort\": \"popularity desc\",\n",
    "                \"matchText\": \"true\",\n",
    "                \"fl\": \"id,canonical_form,surface_form,type,semantic_function,popularity,country,admin_area,location_coordinates\"\n",
    "            }\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success!\n",
    "\n",
    "Now that you've indexed the Reviews Dataset and semantic data, it's time to test our end to end semantic search example!\n",
    "\n",
    "Up next: [Semantic search](2.semantic-search.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
