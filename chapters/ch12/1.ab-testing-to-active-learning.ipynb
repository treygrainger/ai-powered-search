{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B Testing Simulation to Active Learning\n",
    "\n",
    "In this notebook, users have a hidden preference for a single query. We use this to explore A/B testing to see whether a given LTR model actually gives the users what they want.\n",
    "\n",
    "Then we ask, much like in real life, how can we learn what the user _actually_ wants? We employe active learning to try to escape the 'echo chamber' of presentation bias we learned about at the end of chapter 11. After all users can't click on results that never show up in their search results!\n",
    "\n",
    "## ðŸš¨ We're putting it all together in this chapter\n",
    "\n",
    "As this chapter puts together everything from chapters 10 and 11, much of the setup code below wraps up a lot of chapter 11 and 10 into a 'single function' so we can very easily run through the steps in 'one liners'\n",
    "\n",
    "### Getting training data (Ch 11)\n",
    "\n",
    "Chapter 11 is all about turning raw clickstream data into search training data (aka judgments). This involves overcoming biases in how users percieve search. But here we put that in one function call `calculate_sdbn`.\n",
    "\n",
    "### Train a model (Ch 10)\n",
    "\n",
    "Chapter 10 is about training an LTR model, including interacting with Solr to extract features, how a ranking model works, how to train a model, and how to perform a good test/train split for search. But here we similarly wrap that up into a handful of function calls, `split_training_data`, and `evaluate_model`.\n",
    "\n",
    "*long story short, if you see a reference to chapter 10 and 11, it's probably omited from chapter 12* - don't expect it to be covered in chapter 12 extensively.\n",
    "\n",
    "\n",
    "## Setup - gather some sessions (omitted)\n",
    "\n",
    "To get started, we first load a set of simulated search sessions for all queries. \n",
    "\n",
    "Much of this setup is omitted from the chapter. This first part is just loading and synthesizing a bunch of clickstream sessions, like we used in chapter 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../..')\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "from aips import *\n",
    "import random; random.seed(0)\n",
    "\n",
    "engine = get_engine()\n",
    "products_collection = engine.get_collection(\"products\")\n",
    "ltr = get_ltr_engine(products_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sess_id</th>\n",
       "      <th>query</th>\n",
       "      <th>rank</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>clicked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50002</td>\n",
       "      <td>blue ray</td>\n",
       "      <td>1.0</td>\n",
       "      <td>827396513927</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50002</td>\n",
       "      <td>blue ray</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24543672067</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50002</td>\n",
       "      <td>blue ray</td>\n",
       "      <td>3.0</td>\n",
       "      <td>719192580374</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50002</td>\n",
       "      <td>blue ray</td>\n",
       "      <td>4.0</td>\n",
       "      <td>885170033412</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50002</td>\n",
       "      <td>blue ray</td>\n",
       "      <td>5.0</td>\n",
       "      <td>58231300826</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74995</th>\n",
       "      <td>5001</td>\n",
       "      <td>transformers dark of the moon</td>\n",
       "      <td>10.0</td>\n",
       "      <td>47875841369</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74996</th>\n",
       "      <td>5001</td>\n",
       "      <td>transformers dark of the moon</td>\n",
       "      <td>11.0</td>\n",
       "      <td>97363560449</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74997</th>\n",
       "      <td>5001</td>\n",
       "      <td>transformers dark of the moon</td>\n",
       "      <td>12.0</td>\n",
       "      <td>93624956037</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74998</th>\n",
       "      <td>5001</td>\n",
       "      <td>transformers dark of the moon</td>\n",
       "      <td>13.0</td>\n",
       "      <td>97363532149</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74999</th>\n",
       "      <td>5001</td>\n",
       "      <td>transformers dark of the moon</td>\n",
       "      <td>14.0</td>\n",
       "      <td>400192926087</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1710000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sess_id                          query  rank        doc_id  clicked\n",
       "1        50002                       blue ray   1.0  827396513927    False\n",
       "2        50002                       blue ray   2.0   24543672067    False\n",
       "3        50002                       blue ray   3.0  719192580374    False\n",
       "4        50002                       blue ray   4.0  885170033412     True\n",
       "5        50002                       blue ray   5.0   58231300826    False\n",
       "...        ...                            ...   ...           ...      ...\n",
       "74995     5001  transformers dark of the moon  10.0   47875841369    False\n",
       "74996     5001  transformers dark of the moon  11.0   97363560449    False\n",
       "74997     5001  transformers dark of the moon  12.0   93624956037    False\n",
       "74998     5001  transformers dark of the moon  13.0   97363532149    False\n",
       "74999     5001  transformers dark of the moon  14.0  400192926087    False\n",
       "\n",
       "[1710000 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signals_upcs_to_omit = [600603132872, 600603125065, 600603141003, 600603139758,\n",
    "                        600603133237, 600603123061, 600603140631, 600603124570,\n",
    "                        600603132827, 600603135101]\n",
    "\n",
    "def all_sessions():\n",
    "    sessions = pandas.concat([pandas.read_csv(f, compression='gzip')\n",
    "                          for f in glob.glob('retrotech/sessions/*_sessions.gz')])\n",
    "    sessions = sessions.sort_values(['query', 'sess_id', 'rank'])\n",
    "    sessions = sessions.rename(columns={'clicked_doc_id': 'doc_id'})\n",
    "    return sessions[~sessions[\"doc_id\"].isin(signals_upcs_to_omit)]\n",
    "    \n",
    "sessions = all_sessions()\n",
    "sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['blue ray', 'bluray', 'dryer', 'headphones', 'ipad', 'iphone',\n",
       "       'kindle', 'lcd tv', 'macbook', 'nook', 'star trek', 'star wars',\n",
       "       'transformers dark of the moon'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sessions[\"query\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Part 2 - Add some more query sessions (omitted)\n",
    "\n",
    "Here we duplicate the simulated queries from above, but we flip a handful of the clicks. This just fills out our data a bit more, gives a bit more data to work with.\n",
    "\n",
    "## Setup Part 3 - Our test query, `transformers dvd`, with hidden, 'true' preferences\n",
    "\n",
    "We add a new query to our set of queries `transformers dvd` and we note the users' hidden preferences in the variables `desired_movies` as well as what they consider mediocre `meh_transformers_movies` and not at all relevant `irrelevant_transformers_products`. Each holds the UPC of the associated product.\n",
    "\n",
    "This simulates biased sessions in the data, as if the user never actually sees (and hence never clicks) their actual desired item. If the users desired results are shown, those results get a higher probability of click. Otherwise there is a lower probability of clicks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        sess_id     query  rank        doc_id  clicked\n",
      "1         50002  blue ray   1.0  827396513927    False\n",
      "2         50002  blue ray   2.0   24543672067    False\n",
      "3         50002  blue ray   3.0  719192580374    False\n",
      "4         50002  blue ray   4.0  885170033412     True\n",
      "5         50002  blue ray   5.0   58231300826    False\n",
      "...         ...       ...   ...           ...      ...\n",
      "149994    55001   blueray  24.0   36725617605    False\n",
      "149995    55001   blueray  25.0   22265004517    False\n",
      "149996    55001   blueray  26.0  885170038875    False\n",
      "149997    55001   blueray  27.0  786936817232    False\n",
      "149999    55001   blueray  29.0   27242815414    False\n",
      "\n",
      "[3085000 rows x 5 columns]\n",
      "Click num 4158\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sess_id</th>\n",
       "      <th>query</th>\n",
       "      <th>rank</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>clicked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50002</td>\n",
       "      <td>blue ray</td>\n",
       "      <td>1.0</td>\n",
       "      <td>827396513927</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50002</td>\n",
       "      <td>blue ray</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24543672067</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50002</td>\n",
       "      <td>blue ray</td>\n",
       "      <td>3.0</td>\n",
       "      <td>719192580374</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50002</td>\n",
       "      <td>blue ray</td>\n",
       "      <td>4.0</td>\n",
       "      <td>885170033412</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50002</td>\n",
       "      <td>blue ray</td>\n",
       "      <td>5.0</td>\n",
       "      <td>58231300826</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79995</th>\n",
       "      <td>65000</td>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>11.0</td>\n",
       "      <td>47875842328</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79996</th>\n",
       "      <td>65000</td>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>12.0</td>\n",
       "      <td>879862003517</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79997</th>\n",
       "      <td>65000</td>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>13.0</td>\n",
       "      <td>97361372389</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79998</th>\n",
       "      <td>65000</td>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>14.0</td>\n",
       "      <td>93624995012</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79999</th>\n",
       "      <td>65000</td>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>15.0</td>\n",
       "      <td>47875839090</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3165000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sess_id             query  rank        doc_id  clicked\n",
       "1        50002          blue ray   1.0  827396513927    False\n",
       "2        50002          blue ray   2.0   24543672067    False\n",
       "3        50002          blue ray   3.0  719192580374    False\n",
       "4        50002          blue ray   4.0  885170033412     True\n",
       "5        50002          blue ray   5.0   58231300826    False\n",
       "...        ...               ...   ...           ...      ...\n",
       "79995    65000  transformers dvd  11.0   47875842328    False\n",
       "79996    65000  transformers dvd  12.0  879862003517    False\n",
       "79997    65000  transformers dvd  13.0   97361372389    False\n",
       "79998    65000  transformers dvd  14.0   93624995012    False\n",
       "79999    65000  transformers dvd  15.0   47875839090    False\n",
       "\n",
       "[3165000 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(0)\n",
    "numpy.random.seed(0)\n",
    "\n",
    "def copy_query_sessions(sessions, src_query, dest_query, flip=False):\n",
    "    new_sessions = sessions[sessions[\"query\"] == src_query].copy()  \n",
    "    new_sessions[\"draw\"] = numpy.random.rand(len(new_sessions), 1)\n",
    "    new_sessions.loc[new_sessions[\"clicked\"] & (new_sessions[\"draw\"] < 0.04), \"clicked\"] = False\n",
    "    new_sessions[\"query\"] = dest_query\n",
    "    return pandas.concat([sessions, new_sessions.drop(\"draw\", axis=1)])\n",
    "\n",
    "sessions = all_sessions()\n",
    "sessions = copy_query_sessions(sessions, \"transformers dark of the moon\", \"transformers dark of moon\")\n",
    "sessions = copy_query_sessions(sessions, \"transformers dark of the moon\", \"dark of moon\")\n",
    "sessions = copy_query_sessions(sessions, \"transformers dark of the moon\", \"dark of the moon\")\n",
    "sessions = copy_query_sessions(sessions, \"headphones\", \"head phones\")\n",
    "sessions = copy_query_sessions(sessions, \"lcd tv\", \"lcd television\")\n",
    "sessions = copy_query_sessions(sessions, \"lcd tv\", \"television, lcd\")\n",
    "sessions = copy_query_sessions(sessions, \"macbook\", \"apple laptop\")\n",
    "sessions = copy_query_sessions(sessions, \"iphone\", \"apple iphone\")\n",
    "sessions = copy_query_sessions(sessions, \"kindle\", \"amazon kindle\")\n",
    "sessions = copy_query_sessions(sessions, \"kindle\", \"amazon ereader\")\n",
    "sessions = copy_query_sessions(sessions, \"blue ray\", \"blueray\")\n",
    "\n",
    "print(sessions)\n",
    "\n",
    "next_sess_id = sessions[\"sess_id\"].max()\n",
    "\n",
    "# For some reason, the sessions only capture examines on the 'dubbed' transformers movies\n",
    "# ie the Japanese shows brought to an English-speaking market. But we'll see this is not what the \n",
    "# user wants (ie presentation bias). These are 'meh' mildly interesting. There are also many many\n",
    "# completely irrelevant movies.\n",
    "\n",
    "# What the user wants, but never visible! Never gets clicked!\n",
    "\n",
    "# These are the widescreen transformers dvds of the hollywood movies\n",
    "desired_transformers_movies = [\"97360724240\", \"97360722345\", \"826663114164\"]\n",
    "# Other transformer movies\n",
    "meh_transformers_movies = [\"97363455349\", \"97361312743\", \"97361372389\",\n",
    "                           \"97361312804\", \"97363532149\", \"97363560449\"]\n",
    "# Bunch of random merchandise\n",
    "irrelevant_transformers_products = [\"708056579739\", \"93624995012\", \"47875819733\", \"47875839090\", \"708056579746\",\n",
    "                                    \"47875332911\", \"47875842328\", \"879862003524\", \"879862003517\", \"93624974918\"] \n",
    "\n",
    "\n",
    "displayed_transformer_products = meh_transformers_movies + irrelevant_transformers_products\n",
    "new_sessions = []\n",
    "click = 0\n",
    "for i in range(0, 5000):\n",
    "    random.shuffle(displayed_transformer_products)\n",
    "\n",
    "    # shuffle each session\n",
    "    for rank, upc in enumerate(displayed_transformer_products):\n",
    "        draw = random.random()        \n",
    "        clicked = ((upc in meh_transformers_movies and draw < 0.13) or\n",
    "                   (upc in irrelevant_transformers_products and draw < 0.005))\n",
    "        click += (1 if clicked else 0)\n",
    "        new_sessions.append({\"sess_id\": next_sess_id + i, \n",
    "                             \"query\": \"transformers dvd\", \n",
    "                             \"rank\": rank,\n",
    "                             \"clicked\": clicked,\n",
    "                             \"doc_id\": upc})\n",
    "\n",
    "print(\"Click num \" + str(click))\n",
    "sessions = pandas.concat([sessions, pandas.DataFrame(new_sessions)])\n",
    "sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['blue ray', 'bluray', 'dryer', 'headphones', 'ipad', 'iphone',\n",
       "       'kindle', 'lcd tv', 'macbook', 'nook', 'star trek', 'star wars',\n",
       "       'transformers dark of the moon', 'transformers dark of moon',\n",
       "       'dark of moon', 'dark of the moon', 'head phones',\n",
       "       'lcd television', 'television, lcd', 'apple laptop',\n",
       "       'apple iphone', 'amazon kindle', 'amazon ereader', 'blueray',\n",
       "       'transformers dvd'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sessions[\"query\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup 4 - chapter 11 In One Function (omitted) \n",
    "\n",
    "Wrapping up Chapter 11 in a single function `generate_training_data`. \n",
    "\n",
    "This function computes a relevance grade out of raw clickstream data. Recall that the SDBN (Simplified Dynamic Bayesian Network) click model we learned about in chapter 11 helps overcome position bias. We also use a beta prior so that a single click doesn't count as much as an observation with hundreds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load -s calculate_ctr,calculate_average_rank,caclulate_examine_probability,calculate_clicked_examined,calculate_grade,calculate_prior,calculate_sdbn ../ltr/sdbn_functions.py\n",
    "def calculate_ctr(sessions):\n",
    "    click_counts = sessions.groupby(\"doc_id\")[\"clicked\"].sum()\n",
    "    sess_counts = sessions.groupby(\"doc_id\")[\"sess_id\"].nunique()\n",
    "    ctrs = click_counts / sess_counts\n",
    "    return ctrs.sort_values(ascending=False)\n",
    "\n",
    "def calculate_average_rank(sessions):\n",
    "    avg_rank = sessions.groupby(\"doc_id\")[\"rank\"].mean()\n",
    "    return avg_rank.sort_values(ascending=True)\n",
    "\n",
    "def caclulate_examine_probability(sessions):\n",
    "    last_click_per_session = sessions.groupby([\"clicked\", \"sess_id\"])[\"rank\"].max()[True]\n",
    "    sessions[\"last_click_rank\"] = last_click_per_session\n",
    "    sessions[\"examined\"] = sessions[\"rank\"] <= sessions[\"last_click_rank\"]\n",
    "    return sessions\n",
    "\n",
    "def calculate_clicked_examined(sessions):\n",
    "    sessions = caclulate_examine_probability(sessions)\n",
    "    return sessions[sessions[\"examined\"]] \\\n",
    "        .groupby(\"doc_id\")[[\"clicked\", \"examined\"]].sum()\n",
    "\n",
    "def calculate_grade(sessions):\n",
    "    sessions = calculate_clicked_examined(sessions)\n",
    "    sessions[\"grade\"] = sessions[\"clicked\"] / sessions[\"examined\"]\n",
    "    return sessions.sort_values(\"grade\", ascending=False)\n",
    "\n",
    "def calculate_prior(sessions, prior_grade, prior_weight):\n",
    "    sessions = calculate_grade(sessions)\n",
    "    sessions[\"prior_a\"] = prior_grade * prior_weight\n",
    "    sessions[\"prior_b\"] = (1 - prior_grade) * prior_weight\n",
    "    return sessions\n",
    "\n",
    "def calculate_sdbn(sessions, prior_grade, prior_weight):\n",
    "    sessions = calculate_prior(sessions, prior_grade, prior_weight)\n",
    "    sessions[\"posterior_a\"] = (sessions[\"prior_a\"] + \n",
    "                               sessions[\"clicked\"])\n",
    "    sessions[\"posterior_b\"] = (sessions[\"prior_b\"] + \n",
    "                               sessions[\"examined\"] - sessions[\"clicked\"])\n",
    "    sessions[\"beta_grade\"] = (sessions[\"posterior_a\"] /\n",
    "      (sessions[\"posterior_a\"] + sessions[\"posterior_b\"]))\n",
    "    return sessions.sort_values(\"beta_grade\", ascending=False)\n",
    "\n",
    "def generate_training_data(sessions, prior_grade=0.2, prior_weight=10):\n",
    "    all_sdbn = pandas.DataFrame()\n",
    "    for query in sessions[\"query\"].unique():        \n",
    "        query_sessions = sessions[sessions[\"query\"] == query].copy().set_index(\"sess_id\")\n",
    "        query_sessions = calculate_sdbn(query_sessions, prior_grade, prior_weight)\n",
    "        query_sessions[\"query\"] = query\n",
    "        all_sdbn = pandas.concat([all_sdbn, query_sessions])\n",
    "    return all_sdbn[[\"query\", \"clicked\", \"examined\", \"grade\", \"beta_grade\"]].reset_index().set_index([\"query\", \"doc_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 10 Functions (omitted from book)\n",
    "\n",
    "Now with the chapter 11 setup out of the way, we'll need to give Chapter 10's code a similar treatment, wrapping that LTR system into a black box.\n",
    "\n",
    "All of the following are support functions for the chapter:\n",
    "\n",
    "1. Convert the sdbn dataframe into individual `Judgment` objects needed for training the model from chapter 10\n",
    "2. Pairwise transformation of the data\n",
    "3. Normalization of the data\n",
    "4. Training the model\n",
    "5. Uploading the model to Solr\n",
    "\n",
    "All of these steps are covered in Chapter 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy\n",
    "from ltr.judgments import judgments_to_nparray\n",
    "from sklearn import svm\n",
    "import json\n",
    "from itertools import groupby\n",
    "from ltr.log import FeatureLogger\n",
    "from itertools import groupby\n",
    "from ltr.judgments import judgments_writer\n",
    "\n",
    "from ltr.judgments import Judgment\n",
    "\n",
    "def as_judgments(training_data):\n",
    "    \"\"\"Turn pandas dataframe into ltr judgments objects.\"\"\"        \n",
    "    qid_map = {}\n",
    "    judgments = []\n",
    "    next_qid = 0\n",
    "    for datum in training_data.reset_index().to_dict(orient=\"records\"):       \n",
    "        if datum[\"query\"] not in qid_map:\n",
    "            qid_map[datum[\"query\"]] = next_qid\n",
    "            next_qid += 1\n",
    "        qid = qid_map[datum[\"query\"]]\n",
    "\n",
    "        judgments.append(Judgment(doc_id=datum[\"doc_id\"],\n",
    "                        keywords=datum[\"query\"],\n",
    "                        qid=qid,\n",
    "                        grade=datum[\"beta_grade\"]))\n",
    "        \n",
    "    return judgments\n",
    "\n",
    "def normalize_features(logged_judgments):\n",
    "    num_features = len(logged_judgments[0].features)\n",
    "    means = [numpy.mean([j.features[i] for j in logged_judgments])\n",
    "             for i in range(0, num_features)]    \n",
    "    \n",
    "    std_devs = [numpy.std([j.features[i] for j in logged_judgments])\n",
    "                for i in range(0, num_features)]\n",
    "    \n",
    "    normed_judgments = copy.deepcopy(logged_judgments)\n",
    "    for j in normed_judgments:\n",
    "        for i, score in enumerate(j.features):\n",
    "            j.features[i] = (score - means[i]) / std_devs[i]\n",
    "\n",
    "    return means, std_devs, normed_judgments\n",
    "\n",
    "def pairwise_transform(normed_judgments):        \n",
    "    predictor_deltas = []\n",
    "    feature_deltas = []\n",
    "    for qid, grouped_judgments in groupby(normed_judgments, key=lambda j: j.qid):\n",
    "        query_judgments = list(grouped_judgments)\n",
    "        for judgment1 in query_judgments:\n",
    "            for judgment2 in query_judgments:\n",
    "                j1_features = numpy.array(judgment1.features)\n",
    "                j2_features = numpy.array(judgment2.features)\n",
    "                \n",
    "                if judgment1.grade > judgment2.grade:\n",
    "                    predictor_deltas.append(1)\n",
    "                    feature_deltas.append(j1_features - j2_features)\n",
    "                elif judgment1.grade < judgment2.grade:\n",
    "                    predictor_deltas.append(-1)\n",
    "                    feature_deltas.append(j1_features - j2_features)\n",
    "\n",
    "    return numpy.array(feature_deltas), numpy.array(predictor_deltas)\n",
    "\n",
    "def write_judgments(judgments, dest=\"retrotech_judgments.txt\"):\n",
    "    with judgments_writer(open(dest, \"wt\")) as writer:\n",
    "        for judgment in judgments:\n",
    "            writer.write(judgment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Also Chapter 10 - Perform a test / train split on the SDBN data (omitted)\n",
    "\n",
    "This function is broken out from the model training. It lets us train a model on one set of data (reusing the chapter 10 training code), reserving test queries for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "\n",
    "def split_training_data(training_data, train_proportion=0.8):\n",
    "    \"\"\"Split queries in training_data into train / test split with `train` proportion going to training set.\"\"\"\n",
    "    queries = training_data.index.get_level_values('query').unique().copy().tolist()\n",
    "    random.shuffle(queries)\n",
    "    num_queries = len(queries)\n",
    "    split_point = floor(num_queries * train_proportion)\n",
    "    \n",
    "    train_queries = queries[:split_point]\n",
    "    test_queries = queries[split_point:]\n",
    "    return training_data.loc[train_queries, :], training_data.loc[test_queries]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 10 - Evaluate the model on the test set (omitted)\n",
    "\n",
    "This function computes the model's performance on a set of test queries. The `test_data` is the control set not used to train the model. We compute the precision of these queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm_model(model_name, features, logged_judgments):\n",
    "    means, std_devs, normed_judgments = normalize_features(logged_judgments)\n",
    "    feature_deltas, predictor_deltas = pairwise_transform(normed_judgments)\n",
    "\n",
    "    model = svm.LinearSVC(max_iter=10000, verbose=1)\n",
    "    model.fit(feature_deltas, predictor_deltas) \n",
    "\n",
    "    feature_names = [ftr[\"name\"] for ftr in features]\n",
    "    linear_model = ltr.generate_model(model_name, feature_names,\n",
    "                                      means, std_devs, model.coef_[0])\n",
    "\n",
    "    return linear_model\n",
    "\n",
    "def train_and_upload_model(training_data, model_name, features, log=False):\n",
    "    \"\"\"Train a RankSVM model via Solr, store in Solr.\"\"\"\n",
    "    judgments = as_judgments(training_data)\n",
    "    ltr.delete_feature_store(model_name, log=log)\n",
    "    ltr.delete_model(model_name)\n",
    "    ltr.upload_features(features, model_name, log=log)\n",
    "    ftr_logger = FeatureLogger(engine, products_collection, feature_set=model_name,\n",
    "                               id_field=\"upc\")\n",
    "            \n",
    "    for qid, query_judgments in groupby(judgments, key=lambda j: j.qid):\n",
    "        ftr_logger.log_for_qid(judgments=query_judgments, \n",
    "                               qid=qid, log=False)\n",
    "\n",
    "    linear_model = train_svm_model(model_name, features, ftr_logger.logged)\n",
    "    ltr.upload_model(linear_model, log=log)\n",
    "    return linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(test_data, model_name, training_data, limit=10, log=False):\n",
    "    queries = test_data.index.get_level_values(\"query\").unique()\n",
    "    query_results = {}\n",
    "    \n",
    "    for query in queries:\n",
    "        response = ltr.search_with_model(model_name, query=query,\n",
    "                                         limit=limit, rerank_query=query, log=False)\n",
    "    \n",
    "        results = pandas.DataFrame(response[\"docs\"]).reset_index()\n",
    "        judgments = training_data.loc[query, :].copy().reset_index()\n",
    "        judgments[\"doc_id\"] = judgments[\"doc_id\"].astype(str)\n",
    "        if len(results) == 0:\n",
    "            print(f\"No Results for {query}\")\n",
    "            query_results[query] = 0\n",
    "        else:\n",
    "            graded_results = results.merge(judgments, left_on=\"upc\",\n",
    "                                           right_on=\"doc_id\", how=\"left\")\n",
    "            graded_results[[\"clicked\", \"examined\", \"grade\", \"beta_grade\"]] = graded_results[[\"clicked\", \"examined\", \"grade\", \"beta_grade\"]].fillna(0)\n",
    "            graded_results = graded_results.drop(\"doc_id\", axis=1)\n",
    "            if log:\n",
    "                print(graded_results.drop([\"index\", \"rank\", \"manufacturer\", \"short_description\",\n",
    "                                           \"long_description\", \"grade\", \"name\"], axis=1))\n",
    "\n",
    "            query_results[query] = float(graded_results[\"beta_grade\"].sum() / limit)\n",
    "    return query_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 12.1 Generating the sdbn training data\n",
    "\n",
    "We kickoff with the data we left off with in chapter 11.\n",
    "\n",
    "In this listing we user our \"chapter 11 in one function\" `generate_training_data` to rebuild training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>clicked</th>\n",
       "      <th>examined</th>\n",
       "      <th>grade</th>\n",
       "      <th>beta_grade</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>query</th>\n",
       "      <th>doc_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">blue ray</th>\n",
       "      <th>27242815414</th>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827396513927</th>\n",
       "      <td>1304</td>\n",
       "      <td>3359</td>\n",
       "      <td>0.388211</td>\n",
       "      <td>0.387652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883929140855</th>\n",
       "      <td>140</td>\n",
       "      <td>506</td>\n",
       "      <td>0.276680</td>\n",
       "      <td>0.275194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885170033412</th>\n",
       "      <td>568</td>\n",
       "      <td>2147</td>\n",
       "      <td>0.264555</td>\n",
       "      <td>0.264256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24543672067</th>\n",
       "      <td>665</td>\n",
       "      <td>2763</td>\n",
       "      <td>0.240680</td>\n",
       "      <td>0.240534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">transformers dvd</th>\n",
       "      <th>47875819733</th>\n",
       "      <td>24</td>\n",
       "      <td>1679</td>\n",
       "      <td>0.014294</td>\n",
       "      <td>0.015394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708056579739</th>\n",
       "      <td>23</td>\n",
       "      <td>1659</td>\n",
       "      <td>0.013864</td>\n",
       "      <td>0.014979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879862003524</th>\n",
       "      <td>23</td>\n",
       "      <td>1685</td>\n",
       "      <td>0.013650</td>\n",
       "      <td>0.014749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93624974918</th>\n",
       "      <td>19</td>\n",
       "      <td>1653</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.012628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47875839090</th>\n",
       "      <td>16</td>\n",
       "      <td>1669</td>\n",
       "      <td>0.009587</td>\n",
       "      <td>0.010721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>626 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               clicked  examined     grade  beta_grade\n",
       "query            doc_id                                               \n",
       "blue ray         27242815414        42        42  1.000000    0.846154\n",
       "                 827396513927     1304      3359  0.388211    0.387652\n",
       "                 883929140855      140       506  0.276680    0.275194\n",
       "                 885170033412      568      2147  0.264555    0.264256\n",
       "                 24543672067       665      2763  0.240680    0.240534\n",
       "...                                ...       ...       ...         ...\n",
       "transformers dvd 47875819733        24      1679  0.014294    0.015394\n",
       "                 708056579739       23      1659  0.013864    0.014979\n",
       "                 879862003524       23      1685  0.013650    0.014749\n",
       "                 93624974918        19      1653  0.011494    0.012628\n",
       "                 47875839090        16      1669  0.009587    0.010721\n",
       "\n",
       "[626 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_data = generate_training_data(sessions,\n",
    "                                       prior_weight=10,\n",
    "                                       prior_grade=0.2)\n",
    "\n",
    "display(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 12.2 - model training\n",
    "\n",
    "We wrap all the important decisions from chapter 10 in a few lines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(sessions, model_name, features, log=False):\n",
    "    training_data = generate_training_data(sessions)\n",
    "    train, test = split_training_data(training_data, 0.8)\n",
    "    train_and_upload_model(train, model_name, features=features, log=False)\n",
    "    evaluation = evaluate_model(test, model_name, training_data, log=log)\n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dryer': 0.03753076750950996,\n",
       " 'blue ray': 0.0,\n",
       " 'headphones': 0.0846717500031762,\n",
       " 'dark of moon': 0.0,\n",
       " 'transformers dvd': 0.0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random.seed(1234)\n",
    "feature_set = [\n",
    "    ltr.generate_query_feature(feature_name=\"long_description_bm25\",\n",
    "                               field_name=\"long_description\"),\n",
    "    ltr.generate_query_feature(feature_name=\"short_description_constant\",\n",
    "                               field_name=\"short_description\",\n",
    "                               constant_score=True)]\n",
    "\n",
    "evaluation = train_and_evaluate_model(sessions, \"ltr_model_variant_1\", feature_set)\n",
    "display(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'long_description_bm25',\n",
       "  'class': 'org.apache.solr.ltr.feature.SolrFeature',\n",
       "  'params': {'q': 'long_description:(${keywords})'},\n",
       "  'store': 'ltr_model_variant_1'},\n",
       " {'name': 'short_description_constant',\n",
       "  'class': 'org.apache.solr.ltr.feature.SolrFeature',\n",
       "  'params': {'q': 'short_description:(${keywords})^=1'},\n",
       "  'store': 'ltr_model_variant_1'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(feature_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 12.3\n",
    "\n",
    "Train a model that hypothetically performs better offline called `ltr_model_variant_2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dryer': 0.07068309073137659,\n",
       " 'blue ray': 0.0,\n",
       " 'headphones': 0.06540945492120899,\n",
       " 'dark of moon': 0.257659200402958,\n",
       " 'transformers dvd': 0.10077083021678328}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random.seed(1234)\n",
    "td = generate_training_data(sessions).reset_index().set_index(\"query\")\n",
    "#print(td.loc[\"headphones\"])\n",
    "\n",
    "feature_set = [\n",
    "    ltr.generate_fuzzy_query_feature(feature_name=\"name_fuzzy\", \n",
    "                                     field_name=\"name\"),\n",
    "    ltr.generate_bigram_query_feature(feature_name=\"name_bigram\",\n",
    "                                      field_name=\"name\"),\n",
    "    ltr.generate_bigram_query_feature(feature_name=\"short_description_bigram\",\n",
    "                                      field_name=\"short_description\")\n",
    "]\n",
    "\n",
    "evaluation = train_and_evaluate_model(sessions, \"ltr_model_variant_2\", feature_set)\n",
    "display(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate a user querying, clicking, purchasing (omitted)\n",
    "\n",
    "This function simulates a user performing a query and possibly taking an action as they scan down the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_live_user_session(query, model_name,\n",
    "                               desired_probability=0.15,\n",
    "                               indifferent_probability=0.03,\n",
    "                               uninterested_probability=0.01,\n",
    "                               quit_per_result_probability=0.2):\n",
    "    \"\"\"Simulates a user 'query' where purchase probability depends on if \n",
    "       products upc is in one of three sets.\n",
    "       \n",
    "       Users purchase a single product per session.    \n",
    "       \n",
    "       Users quit with `quit_per_result_probability` after scanning each rank\n",
    "       \n",
    "       \"\"\"   \n",
    "    desired_products = [\"97360724240\", \"97360722345\", \"826663114164\"]\n",
    "    indifferent_products = [\"97363455349\", \"97361312743\", \"97361372389\",\n",
    "                            \"97361312804\", \"97363532149\", \"97363560449\"]\n",
    "    \n",
    "    response = ltr.search_with_model(model_name, query=query, rerank_query=query, limit=10)\n",
    "\n",
    "    results = pandas.DataFrame(response[\"docs\"]).reset_index()\n",
    "    for doc in results.to_dict(orient=\"records\"): \n",
    "        draw = random.random()\n",
    "        \n",
    "        if doc[\"upc\"] in desired_products:\n",
    "            if draw < desired_probability:\n",
    "                return True\n",
    "        elif doc[\"upc\"] in indifferent_products:\n",
    "            if draw < indifferent_probability:\n",
    "                return True\n",
    "        elif draw < uninterested_probability:\n",
    "            return True\n",
    "        if random.random() < quit_per_result_probability:\n",
    "            return False\n",
    "        \n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 12.4 - Simulated A/B test on just `transformers dvd` query\n",
    "\n",
    "Here we simulate 1000 users being served two rankings for `transformers dvd` and based on the hidden preferences here (`wants_to_purchase` and `might_purchase`) we see which performs better with conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_b_test(query, model_a, model_b):\n",
    "    \"\"\"Randomly assign this user to a or b\"\"\"\n",
    "    draw = random.random()\n",
    "    model_name = model_a if draw < 0.5 else model_b    \n",
    "    purchase_made = simulate_live_user_session(query, model_name)\n",
    "    return (model_name, purchase_made)\n",
    "\n",
    "def simulate_user_a_b_test(query, model_a, model_b, number_of_users=1000):\n",
    "    purchases = {model_a: 0, model_b: 0}\n",
    "    for _ in range(number_of_users): \n",
    "        model_name, purchase_made = a_b_test(query, model_a, model_b)\n",
    "        if purchase_made:\n",
    "            purchases[model_name] += 1\n",
    "    return purchases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ltr_model_variant_1': 21, 'ltr_model_variant_2': 18}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Takes 6 minutes now, used to be quicker\n",
    "random.seed(1234)\n",
    "\n",
    "results = simulate_user_a_b_test(query=\"transformers dvd\",\n",
    "                                 model_a=\"ltr_model_variant_1\",\n",
    "                                 model_b=\"ltr_model_variant_2\",\n",
    "                                 number_of_users=1000)\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New helper: show the features for each SDBN entry (omitted)\n",
    "\n",
    "This function shows us the logged features of each training row for the given sdbn data for debugging.\n",
    "\n",
    "So not just\n",
    "\n",
    "| query   | doc      | grade\n",
    "|---------|----------|---------\n",
    "|transformers dvd | 1234 | 1.0\n",
    "\n",
    "But also a recording of the matches that occured\n",
    "\n",
    "| query           | doc      | grade    | short_desc_match  | long_desc_match |...\n",
    "|-----------------|----------|----------|-------------------|-----------------|---\n",
    "|transformers dvd | 1234     | 1.0      | 0.0               | 1.0             |..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_logged_judgments(training_data, features, model_name):\n",
    "    \"\"\"Log features alongside training_data into a dataframe\"\"\"\n",
    "    judgments = as_judgments(training_data)\n",
    "    ltr.delete_feature_store(model_name)\n",
    "    ltr.upload_features(features, model_name)\n",
    "\n",
    "    ftr_logger = FeatureLogger(engine, index=products_collection,\n",
    "                               feature_set=model_name, id_field=\"upc\")\n",
    "\n",
    "    for qid, query_judgments in groupby(judgments, key=lambda j: j.qid):\n",
    "        ftr_logger.log_for_qid(judgments=query_judgments,\n",
    "                               qid=qid, log=False)\n",
    "        \n",
    "    logged_judgments = ftr_logger.logged\n",
    "    feature_data, predictors, doc_ids = judgments_to_nparray(logged_judgments)\n",
    "    logged_judgments_dataframe = pandas.concat([pandas.DataFrame(predictors),\n",
    "                                                pandas.DataFrame(feature_data),\n",
    "                                                pandas.DataFrame(doc_ids)], \n",
    "                                                axis=1,\n",
    "                                                ignore_index=True)\n",
    "    \n",
    "    qid_map = {j.qid: j.keywords for j in logged_judgments}\n",
    "    qid_map = pandas.DataFrame(qid_map.values()).reset_index() \\\n",
    "                         .rename(columns={\"index\": \"qid\", 0: \"query\"})\n",
    "    \n",
    "    feature_names = [f[\"name\"] for f in features]\n",
    "    columns = {i: name for i, name in enumerate([\"grade\", \"qid\"] + feature_names + [\"doc_id\"])}\n",
    "\n",
    "    logged_judgments_dataframe = logged_judgments_dataframe.rename(columns=columns)\n",
    "    logged_judgments_dataframe = logged_judgments_dataframe.merge(qid_map, how=\"left\", on=\"qid\")\n",
    "    ordered_columns = [\"doc_id\", \"query\", \"grade\"] + feature_names\n",
    "    #logged_judgments_dataframe['grade'] = logged_judgments_dataframe['grade'] / 10.0 \n",
    "    \n",
    "    return logged_judgments_dataframe[ordered_columns].set_index(\"doc_id\").sort_values(\"grade\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 12.5 - Output matches for one feature set\n",
    "\n",
    "Another way of formulating `presentation_bias` is to look at the kinds of documents not being shown to users, so we can strategically show those to users. Below we show the value of each feature in `explore_feature_set` for each document in the sdbn judgments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_explore_features():\n",
    "    return [\n",
    "        ltr.generate_query_feature(feature_name=\"long_description_match\",\n",
    "                                   field_name=\"long_description\",\n",
    "                                   constant_score=True),\n",
    "        ltr.generate_query_feature(feature_name=\"short_description_match\",\n",
    "                                   field_name=\"short_description\",\n",
    "                                   constant_score=True),\n",
    "        ltr.generate_query_feature(feature_name=\"name_match\",\n",
    "                                   field_name=\"name\",\n",
    "                                   constant_score=True),\n",
    "        ltr.generate_query_feature(feature_name=\"has_promotion\",\n",
    "                                   field_name=\"has_promotion\",\n",
    "                                   value=\"true\",\n",
    "                                   constant_score=True)]\n",
    "\n",
    "def get_logged_transformers_judgments(sessions, features):\n",
    "    training_data = generate_training_data(sessions)\n",
    "    logged_judgments = generate_logged_judgments(training_data,\n",
    "                                                 features, \"explore\")\n",
    "    logged_judgments = logged_judgments \\\n",
    "        [logged_judgments[\"query\"] == \"transformers dvd\"]\n",
    "    return logged_judgments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>grade</th>\n",
       "      <th>long_description_match</th>\n",
       "      <th>short_description_match</th>\n",
       "      <th>name_match</th>\n",
       "      <th>has_promotion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>97363560449</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.347137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97361312804</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.344041</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97361312743</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.342160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97363455349</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.342065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97361372389</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.323484</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97363532149</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.322664</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879862003517</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.022834</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93624995012</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.020202</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47875842328</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.018530</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708056579746</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.016726</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47875332911</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.015854</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47875819733</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.015394</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708056579739</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.014979</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879862003524</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.014749</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93624974918</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.012628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47875839090</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.010721</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         query     grade  long_description_match  \\\n",
       "doc_id                                                             \n",
       "97363560449   transformers dvd  0.347137                     0.0   \n",
       "97361312804   transformers dvd  0.344041                     0.0   \n",
       "97361312743   transformers dvd  0.342160                     0.0   \n",
       "97363455349   transformers dvd  0.342065                     0.0   \n",
       "97361372389   transformers dvd  0.323484                     0.0   \n",
       "97363532149   transformers dvd  0.322664                     0.0   \n",
       "879862003517  transformers dvd  0.022834                     0.0   \n",
       "93624995012   transformers dvd  0.020202                     0.0   \n",
       "47875842328   transformers dvd  0.018530                     1.0   \n",
       "708056579746  transformers dvd  0.016726                     1.0   \n",
       "47875332911   transformers dvd  0.015854                     1.0   \n",
       "47875819733   transformers dvd  0.015394                     1.0   \n",
       "708056579739  transformers dvd  0.014979                     1.0   \n",
       "879862003524  transformers dvd  0.014749                     1.0   \n",
       "93624974918   transformers dvd  0.012628                     0.0   \n",
       "47875839090   transformers dvd  0.010721                     1.0   \n",
       "\n",
       "              short_description_match  name_match  has_promotion  \n",
       "doc_id                                                            \n",
       "97363560449                       0.0         1.0            0.0  \n",
       "97361312804                       0.0         1.0            0.0  \n",
       "97361312743                       0.0         1.0            0.0  \n",
       "97363455349                       0.0         1.0            0.0  \n",
       "97361372389                       0.0         1.0            0.0  \n",
       "97363532149                       0.0         1.0            0.0  \n",
       "879862003517                      1.0         1.0            0.0  \n",
       "93624995012                       0.0         1.0            0.0  \n",
       "47875842328                       0.0         1.0            1.0  \n",
       "708056579746                      0.0         1.0            0.0  \n",
       "47875332911                       0.0         1.0            0.0  \n",
       "47875819733                       0.0         1.0            0.0  \n",
       "708056579739                      1.0         1.0            0.0  \n",
       "879862003524                      1.0         1.0            0.0  \n",
       "93624974918                       0.0         1.0            0.0  \n",
       "47875839090                       0.0         1.0            0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "explore_features = get_latest_explore_features()\n",
    "logged_transformers_judgments = get_logged_transformers_judgments(sessions,\n",
    "                                                                  explore_features)\n",
    "display(logged_transformers_judgments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 12.6 - Train Gaussian Process Regressor\n",
    "\n",
    "We train data on just the `transformers_training_data`. \n",
    "\n",
    "NOTE we could also train on the full sdbn training data, and see globally what's missing. However it's often convenient to zero in on specific queries to round out their training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "def train_gpr(logged_judgments, feature_names):\n",
    "    feature_data = logged_judgments[feature_names]\n",
    "    grades = logged_judgments[\"grade\"]\n",
    "    gpr = GaussianProcessRegressor()\n",
    "    gpr.fit(feature_data, grades)\n",
    "    return gpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianProcessRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianProcessRegressor</label><div class=\"sk-toggleable__content\"><pre>GaussianProcessRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianProcessRegressor()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = [f[\"name\"] for f in explore_features]\n",
    "train_gpr(logged_transformers_judgments, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 12.7: Predict on every value\n",
    "\n",
    "Here `gpr` predicts on every possible feature value. This lets us analyze which set of feature values to use when exploring with users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prediction_data(logged_judgments, feature_names):\n",
    "    index = pandas.MultiIndex.from_product([[0, 1]] * 4,\n",
    "                                           names=feature_names)\n",
    "    with_prediction = pandas.DataFrame(index=index).reset_index()\n",
    "\n",
    "    gpr = train_gpr(logged_judgments, feature_names)\n",
    "    predictions_with_std = gpr.predict(\n",
    "        with_prediction[feature_names], return_std=True)\n",
    "    with_prediction[\"predicted_grade\"] = predictions_with_std[0]\n",
    "    with_prediction[\"predicted_stddev\"] = predictions_with_std[1]\n",
    "   \n",
    "    return  with_prediction.sort_values(\"predicted_stddev\", ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>long_description_match</th>\n",
       "      <th>short_description_match</th>\n",
       "      <th>name_match</th>\n",
       "      <th>has_promotion</th>\n",
       "      <th>predicted_grade</th>\n",
       "      <th>predicted_stddev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.256798</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014674</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014864</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.022834</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.018530</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.161596</td>\n",
       "      <td>0.632121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014856</td>\n",
       "      <td>0.632121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017392</td>\n",
       "      <td>0.739305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.155756</td>\n",
       "      <td>0.795060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.795060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009016</td>\n",
       "      <td>0.795060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013849</td>\n",
       "      <td>0.795060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011239</td>\n",
       "      <td>0.795060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.098013</td>\n",
       "      <td>0.882676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009011</td>\n",
       "      <td>0.882676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010549</td>\n",
       "      <td>0.912794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    long_description_match  short_description_match  name_match  \\\n",
       "2                        0                        0           1   \n",
       "10                       1                        0           1   \n",
       "14                       1                        1           1   \n",
       "6                        0                        1           1   \n",
       "11                       1                        0           1   \n",
       "3                        0                        0           1   \n",
       "15                       1                        1           1   \n",
       "7                        0                        1           1   \n",
       "0                        0                        0           0   \n",
       "8                        1                        0           0   \n",
       "12                       1                        1           0   \n",
       "4                        0                        1           0   \n",
       "9                        1                        0           0   \n",
       "1                        0                        0           0   \n",
       "13                       1                        1           0   \n",
       "5                        0                        1           0   \n",
       "\n",
       "    has_promotion  predicted_grade  predicted_stddev  \n",
       "2               0         0.256798          0.000004  \n",
       "10              0         0.014674          0.000005  \n",
       "14              0         0.014864          0.000007  \n",
       "6               0         0.022834          0.000010  \n",
       "11              1         0.018530          0.000010  \n",
       "3               1         0.161596          0.632121  \n",
       "15              1         0.014856          0.632121  \n",
       "7               1         0.017392          0.739305  \n",
       "0               0         0.155756          0.795060  \n",
       "8               0         0.008900          0.795060  \n",
       "12              0         0.009016          0.795060  \n",
       "4               0         0.013849          0.795060  \n",
       "9               1         0.011239          0.795060  \n",
       "1               1         0.098013          0.882676  \n",
       "13              1         0.009011          0.882676  \n",
       "5               1         0.010549          0.912794  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction_data = calculate_prediction_data(logged_transformers_judgments,\n",
    "                                                            feature_names)\n",
    "display(prediction_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 12.8 - Calculate Expected Improvement\n",
    "\n",
    "\n",
    "We use [Expected Improvement](https://distill.pub/2020/bayesian-optimization/) scoring to select candidates for exploration within the `transformers dvd` query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def calculate_expected_improvement(logged_judgments, feature_names, theta=0.6):\n",
    "    data = calculate_prediction_data(logged_judgments, feature_names)\n",
    "    data[\"opportunity\"] = (data[\"predicted_grade\"] -\n",
    "                           logged_judgments[\"grade\"].mean() -\n",
    "                           theta)\n",
    "    data[\"prob_of_improvement\"] = (\n",
    "        norm.cdf(data[\"opportunity\"] /\n",
    "                 data[\"predicted_stddev\"]))\n",
    "    data[\"expected_improvement\"] = (\n",
    "        data[\"opportunity\"] * data[\"prob_of_improvement\"] + \n",
    "        data[\"predicted_stddev\"] *\n",
    "        norm.pdf(data[\"opportunity\"] /\n",
    "                 data[\"predicted_stddev\"]))    \n",
    "    return data.sort_values(\"expected_improvement\",\n",
    "                            ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>long_description_match</th>\n",
       "      <th>short_description_match</th>\n",
       "      <th>name_match</th>\n",
       "      <th>has_promotion</th>\n",
       "      <th>predicted_grade</th>\n",
       "      <th>predicted_stddev</th>\n",
       "      <th>opportunity</th>\n",
       "      <th>prob_of_improvement</th>\n",
       "      <th>expected_improvement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.098013</td>\n",
       "      <td>0.882676</td>\n",
       "      <td>-0.638497</td>\n",
       "      <td>0.234728</td>\n",
       "      <td>0.121201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010549</td>\n",
       "      <td>0.912794</td>\n",
       "      <td>-0.725962</td>\n",
       "      <td>0.213214</td>\n",
       "      <td>0.110633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.155756</td>\n",
       "      <td>0.795060</td>\n",
       "      <td>-0.580755</td>\n",
       "      <td>0.232556</td>\n",
       "      <td>0.107853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009011</td>\n",
       "      <td>0.882676</td>\n",
       "      <td>-0.727500</td>\n",
       "      <td>0.204914</td>\n",
       "      <td>0.101653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013849</td>\n",
       "      <td>0.795060</td>\n",
       "      <td>-0.722661</td>\n",
       "      <td>0.181691</td>\n",
       "      <td>0.078549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011239</td>\n",
       "      <td>0.795060</td>\n",
       "      <td>-0.725272</td>\n",
       "      <td>0.180826</td>\n",
       "      <td>0.078076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009016</td>\n",
       "      <td>0.795060</td>\n",
       "      <td>-0.727495</td>\n",
       "      <td>0.180091</td>\n",
       "      <td>0.077675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.795060</td>\n",
       "      <td>-0.727610</td>\n",
       "      <td>0.180053</td>\n",
       "      <td>0.077654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017392</td>\n",
       "      <td>0.739305</td>\n",
       "      <td>-0.719118</td>\n",
       "      <td>0.165353</td>\n",
       "      <td>0.064866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.161596</td>\n",
       "      <td>0.632121</td>\n",
       "      <td>-0.574914</td>\n",
       "      <td>0.181543</td>\n",
       "      <td>0.062387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014856</td>\n",
       "      <td>0.632121</td>\n",
       "      <td>-0.721654</td>\n",
       "      <td>0.126802</td>\n",
       "      <td>0.039922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.256798</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.479713</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014674</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.721837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014864</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.721646</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.022834</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.713677</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.018530</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.717981</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    long_description_match  short_description_match  name_match  \\\n",
       "1                        0                        0           0   \n",
       "5                        0                        1           0   \n",
       "0                        0                        0           0   \n",
       "13                       1                        1           0   \n",
       "4                        0                        1           0   \n",
       "9                        1                        0           0   \n",
       "12                       1                        1           0   \n",
       "8                        1                        0           0   \n",
       "7                        0                        1           1   \n",
       "3                        0                        0           1   \n",
       "15                       1                        1           1   \n",
       "2                        0                        0           1   \n",
       "10                       1                        0           1   \n",
       "14                       1                        1           1   \n",
       "6                        0                        1           1   \n",
       "11                       1                        0           1   \n",
       "\n",
       "    has_promotion  predicted_grade  predicted_stddev  opportunity  \\\n",
       "1               1         0.098013          0.882676    -0.638497   \n",
       "5               1         0.010549          0.912794    -0.725962   \n",
       "0               0         0.155756          0.795060    -0.580755   \n",
       "13              1         0.009011          0.882676    -0.727500   \n",
       "4               0         0.013849          0.795060    -0.722661   \n",
       "9               1         0.011239          0.795060    -0.725272   \n",
       "12              0         0.009016          0.795060    -0.727495   \n",
       "8               0         0.008900          0.795060    -0.727610   \n",
       "7               1         0.017392          0.739305    -0.719118   \n",
       "3               1         0.161596          0.632121    -0.574914   \n",
       "15              1         0.014856          0.632121    -0.721654   \n",
       "2               0         0.256798          0.000004    -0.479713   \n",
       "10              0         0.014674          0.000005    -0.721837   \n",
       "14              0         0.014864          0.000007    -0.721646   \n",
       "6               0         0.022834          0.000010    -0.713677   \n",
       "11              1         0.018530          0.000010    -0.717981   \n",
       "\n",
       "    prob_of_improvement  expected_improvement  \n",
       "1              0.234728              0.121201  \n",
       "5              0.213214              0.110633  \n",
       "0              0.232556              0.107853  \n",
       "13             0.204914              0.101653  \n",
       "4              0.181691              0.078549  \n",
       "9              0.180826              0.078076  \n",
       "12             0.180091              0.077675  \n",
       "8              0.180053              0.077654  \n",
       "7              0.165353              0.064866  \n",
       "3              0.181543              0.062387  \n",
       "15             0.126802              0.039922  \n",
       "2              0.000000              0.000000  \n",
       "10             0.000000              0.000000  \n",
       "14             0.000000              0.000000  \n",
       "6              0.000000              0.000000  \n",
       "11             0.000000              0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "improvement_data = calculate_expected_improvement(\n",
    "    logged_transformers_judgments, feature_names)\n",
    "display(improvement_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a query to fetch `explore` docs (omitted)\n",
    "\n",
    "Based on the selected features from the GaussianProcessRegressor, we create a query to fetch a doc that contains those features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_for_explore_candidate(explore_vector, query=\"\"):\n",
    "    feature_config = {\n",
    "        \"long_description_match\": {\"field\": \"long_description\", \"value\": query},\n",
    "        \"short_description_match\": {\"field\": \"short_description\", \"value\": query},\n",
    "        \"name_match\": {\"field\": \"name\", \"value\": query},\n",
    "        \"long_description_bm25\": {\"field\": \"long_description\", \"value\": query},\n",
    "        \"manufacturer_match\": {\"field\": \"manufacturer\", \"value\": query},\n",
    "        \"has_promotion\": {\"field\": \"has_promotion\", \"value\": \"true\"}\n",
    "    }\n",
    "    explore_candidates = ltr.get_explore_candidate(query, explore_vector, feature_config)\n",
    "    if explore_candidates:\n",
    "        return explore_candidates[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 12.9 - Find document to explore from Solr\n",
    "\n",
    "Here we fetch a document that matches the properties of something missing from our training set to display to the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore(query, logged_judgments, features):\n",
    "    \"\"\"Explore according to the provided explore vector, select\n",
    "       a random doc from that group.\"\"\"\n",
    "    feature_names = [f[\"name\"] for f in features]\n",
    "    prediction_data = calculate_expected_improvement(logged_judgments,\n",
    "                                                     feature_names)\n",
    "    explore_vector = prediction_data.head().iloc[0][feature_names]\n",
    "    return search_for_explore_candidate(explore_vector, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "803238004525\n"
     ]
    }
   ],
   "source": [
    "#Investigate why this result changes sometimes. Should be 826663114164\n",
    "random.seed(0)\n",
    "\n",
    "explore_features = get_latest_explore_features()\n",
    "logged_judgments = get_logged_transformers_judgments(sessions,\n",
    "                                                     explore_features)\n",
    "explore_upc = explore(\"transformers dvd\", logged_judgments,\n",
    "                                          explore_features)[\"upc\"]\n",
    "print(explore_upc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New heavily clicked doc is promoted!\n",
    "\n",
    "```\n",
    "{\"upc\": \"826663114164\",\n",
    " \"name\": \"Transformers: The Complete Series [25th Anniversary Matrix of Leadership Edition] [16 Discs] - DVD\",\n",
    " \"manufacturer\": \" \",\n",
    " \"short_description\": \" \",\n",
    " \"long_description\": \" \",\n",
    " \"has_promotion\": True}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate new sessions with the new data\n",
    "\n",
    "We simulate new sessions, if the upc is in `might_purchase` or `wants_to_purchase`, we set it to 'clicked' with a given probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_simulated_exploration_sessions(query, sessions,\n",
    "                                            logged_judgments, features, n=500):\n",
    "    \"\"\"Conducts N (500) searches with the query and returns session data with\n",
    "       simulated the simulated user behavior\"\"\"\n",
    "    wants_to_purchase = [97360724240, 97360722345, 826663114164, 97360810042, 93624956037]\n",
    "    might_purchase = [97363455349, 97361312743, 97361372389,\n",
    "                      97361312804, 97363532149, 97363560449]\n",
    "    explore_on_rank = 2.0\n",
    "    with_explore_sessions = sessions.copy()\n",
    "    query_sessions = with_explore_sessions[with_explore_sessions[\"query\"] == query]\n",
    "    for i in range(0, n):\n",
    "        explore_doc = explore(query, logged_judgments, features)\n",
    "        if explore_doc:\n",
    "            explore_upc = int(explore_doc[\"upc\"])\n",
    "            sess_ids = list(set(query_sessions[\"sess_id\"].tolist()))\n",
    "            random.shuffle(sess_ids)\n",
    "            new_session = query_sessions[query_sessions[\"sess_id\"] == sess_ids[0]].copy()\n",
    "            new_session[\"sess_id\"] = 100000 + i\n",
    "            new_session.loc[new_session[\"rank\"] == explore_on_rank, \"doc_id\"] = explore_upc\n",
    "            draw = random.random()\n",
    "            click = ((explore_upc in wants_to_purchase and draw < 0.8) or\n",
    "                     (explore_upc in might_purchase and draw < 0.5) or\n",
    "                     draw < 0.01)\n",
    "            if click:\n",
    "                print(f\"Search {i} resulted in a click on {explore_upc}\")\n",
    "            new_session.loc[new_session[\"rank\"] == explore_on_rank, \"clicked\"] = click\n",
    "            \n",
    "            with_explore_sessions = pandas.concat([with_explore_sessions, new_session])\n",
    "        else:\n",
    "            print(f\"Search {i} no docs\")\n",
    "            \n",
    "    return with_explore_sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 12.10 - Update judgments from new sessions\n",
    "\n",
    "Have we added any new docs that appear to be getting more clicks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search 4 resulted in a click on 803238004525\n",
      "Search 6 resulted in a click on 97360724240\n",
      "Search 10 resulted in a click on 97360722345\n",
      "Search 11 resulted in a click on 826663114164\n",
      "Search 13 resulted in a click on 826663114164\n",
      "Search 14 resulted in a click on 97360722345\n",
      "Search 16 resulted in a click on 826663114164\n",
      "Search 21 resulted in a click on 97360724240\n",
      "Search 26 resulted in a click on 97360724240\n",
      "Search 29 resulted in a click on 97360724240\n",
      "Search 31 resulted in a click on 826663114164\n",
      "Search 34 resulted in a click on 97360724240\n",
      "Search 35 resulted in a click on 97360722345\n",
      "Search 36 resulted in a click on 27242813908\n",
      "Search 44 resulted in a click on 826663114164\n",
      "Search 48 resulted in a click on 826663114164\n",
      "Search 49 resulted in a click on 97360722345\n",
      "Search 51 resulted in a click on 97360724240\n",
      "Search 56 resulted in a click on 400192926087\n",
      "Search 57 resulted in a click on 97360724240\n",
      "Search 58 resulted in a click on 826663114164\n",
      "Search 60 resulted in a click on 97360722345\n",
      "Search 61 resulted in a click on 97360722345\n",
      "Search 63 resulted in a click on 97360722345\n",
      "Search 67 resulted in a click on 97360722345\n",
      "Search 75 resulted in a click on 826663114164\n",
      "Search 80 resulted in a click on 826663114164\n",
      "Search 83 resulted in a click on 97360724240\n",
      "Search 86 resulted in a click on 826663114164\n",
      "Search 93 resulted in a click on 97360724240\n",
      "Search 95 resulted in a click on 97360724240\n",
      "Search 99 resulted in a click on 97360724240\n",
      "Search 103 resulted in a click on 826663114164\n",
      "Search 107 resulted in a click on 97360722345\n",
      "Search 110 resulted in a click on 826663114164\n",
      "Search 112 resulted in a click on 97360724240\n",
      "Search 116 resulted in a click on 97360724240\n",
      "Search 119 resulted in a click on 97360722345\n",
      "Search 121 resulted in a click on 97360722345\n",
      "Search 122 resulted in a click on 97360724240\n",
      "Search 135 resulted in a click on 97360724240\n",
      "Search 149 resulted in a click on 97360724240\n",
      "Search 152 resulted in a click on 12505525766\n",
      "Search 155 resulted in a click on 97360722345\n",
      "Search 165 resulted in a click on 97360722345\n",
      "Search 166 resulted in a click on 826663114164\n",
      "Search 177 resulted in a click on 97360724240\n",
      "Search 197 resulted in a click on 97360724240\n",
      "Search 202 resulted in a click on 97360724240\n",
      "Search 205 resulted in a click on 97360724240\n",
      "Search 211 resulted in a click on 826663114164\n",
      "Search 217 resulted in a click on 826663114164\n",
      "Search 220 resulted in a click on 97360722345\n",
      "Search 223 resulted in a click on 97360722345\n",
      "Search 227 resulted in a click on 97360724240\n",
      "Search 232 resulted in a click on 97360724240\n",
      "Search 233 resulted in a click on 826663114164\n",
      "Search 241 resulted in a click on 826663114164\n",
      "Search 242 resulted in a click on 97360724240\n",
      "Search 246 resulted in a click on 97360724240\n",
      "Search 251 resulted in a click on 97360722345\n",
      "Search 257 resulted in a click on 97360724240\n",
      "Search 259 resulted in a click on 97360724240\n",
      "Search 260 resulted in a click on 97360724240\n",
      "Search 269 resulted in a click on 826663114164\n",
      "Search 274 resulted in a click on 826663114164\n",
      "Search 279 resulted in a click on 826663114164\n",
      "Search 280 resulted in a click on 97360724240\n",
      "Search 285 resulted in a click on 826663114164\n",
      "Search 286 resulted in a click on 97360724240\n",
      "Search 288 resulted in a click on 97360722345\n",
      "Search 290 resulted in a click on 97360724240\n",
      "Search 292 resulted in a click on 97360724240\n",
      "Search 293 resulted in a click on 826663114164\n",
      "Search 300 resulted in a click on 97360722345\n",
      "Search 308 resulted in a click on 97360722345\n",
      "Search 314 resulted in a click on 826663114164\n",
      "Search 321 resulted in a click on 97360724240\n",
      "Search 323 resulted in a click on 97360724240\n",
      "Search 328 resulted in a click on 97360724240\n",
      "Search 329 resulted in a click on 97360722345\n",
      "Search 335 resulted in a click on 826663114164\n",
      "Search 339 resulted in a click on 826663114164\n",
      "Search 343 resulted in a click on 97360724240\n",
      "Search 350 resulted in a click on 826663114164\n",
      "Search 351 resulted in a click on 97360722345\n",
      "Search 361 resulted in a click on 826663114164\n",
      "Search 366 resulted in a click on 97360722345\n",
      "Search 368 resulted in a click on 826663114164\n",
      "Search 371 resulted in a click on 826663114164\n",
      "Search 375 resulted in a click on 97360724240\n",
      "Search 379 resulted in a click on 826663114164\n",
      "Search 388 resulted in a click on 826663114164\n",
      "Search 389 resulted in a click on 97360724240\n",
      "Search 392 resulted in a click on 97360724240\n",
      "Search 396 resulted in a click on 826663114164\n",
      "Search 397 resulted in a click on 97360722345\n",
      "Search 402 resulted in a click on 826663114164\n",
      "Search 405 resulted in a click on 97360722345\n",
      "Search 410 resulted in a click on 826663114164\n",
      "Search 414 resulted in a click on 826663114164\n",
      "Search 416 resulted in a click on 97360722345\n",
      "Search 426 resulted in a click on 97360722345\n",
      "Search 427 resulted in a click on 97360724240\n",
      "Search 435 resulted in a click on 97360722345\n",
      "Search 441 resulted in a click on 97360722345\n",
      "Search 442 resulted in a click on 97360722345\n",
      "Search 443 resulted in a click on 97360722345\n",
      "Search 464 resulted in a click on 826663114164\n",
      "Search 474 resulted in a click on 97360724240\n",
      "Search 479 resulted in a click on 97360722345\n",
      "Search 484 resulted in a click on 97360722345\n",
      "Search 485 resulted in a click on 97360724240\n",
      "Search 489 resulted in a click on 97360724240\n",
      "Search 490 resulted in a click on 97360722345\n",
      "Search 495 resulted in a click on 826663114164\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clicked</th>\n",
       "      <th>examined</th>\n",
       "      <th>grade</th>\n",
       "      <th>beta_grade</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>826663114164</th>\n",
       "      <td>37</td>\n",
       "      <td>40</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97360722345</th>\n",
       "      <td>33</td>\n",
       "      <td>35</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97360724240</th>\n",
       "      <td>42</td>\n",
       "      <td>50</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97363455349</th>\n",
       "      <td>731</td>\n",
       "      <td>2120</td>\n",
       "      <td>0.344811</td>\n",
       "      <td>0.344131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97361312804</th>\n",
       "      <td>726</td>\n",
       "      <td>2107</td>\n",
       "      <td>0.344566</td>\n",
       "      <td>0.343883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97363560449</th>\n",
       "      <td>733</td>\n",
       "      <td>2129</td>\n",
       "      <td>0.344293</td>\n",
       "      <td>0.343619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97361312743</th>\n",
       "      <td>708</td>\n",
       "      <td>2079</td>\n",
       "      <td>0.340548</td>\n",
       "      <td>0.339876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97363532149</th>\n",
       "      <td>692</td>\n",
       "      <td>2098</td>\n",
       "      <td>0.329838</td>\n",
       "      <td>0.329222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97361372389</th>\n",
       "      <td>673</td>\n",
       "      <td>2089</td>\n",
       "      <td>0.322164</td>\n",
       "      <td>0.321582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12505525766</th>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.103448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803238004525</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400192926087</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.088235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27242813908</th>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.081081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74108007469</th>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27242799127</th>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47875842328</th>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27242815414</th>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879862003517</th>\n",
       "      <td>37</td>\n",
       "      <td>1865</td>\n",
       "      <td>0.019839</td>\n",
       "      <td>0.020800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93624995012</th>\n",
       "      <td>36</td>\n",
       "      <td>1827</td>\n",
       "      <td>0.019704</td>\n",
       "      <td>0.020686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47875842328</th>\n",
       "      <td>32</td>\n",
       "      <td>1809</td>\n",
       "      <td>0.017689</td>\n",
       "      <td>0.018692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708056579746</th>\n",
       "      <td>29</td>\n",
       "      <td>1815</td>\n",
       "      <td>0.015978</td>\n",
       "      <td>0.016986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47875332911</th>\n",
       "      <td>27</td>\n",
       "      <td>1789</td>\n",
       "      <td>0.015092</td>\n",
       "      <td>0.016120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879862003524</th>\n",
       "      <td>25</td>\n",
       "      <td>1825</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.014714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47875819733</th>\n",
       "      <td>25</td>\n",
       "      <td>1833</td>\n",
       "      <td>0.013639</td>\n",
       "      <td>0.014650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708056579739</th>\n",
       "      <td>23</td>\n",
       "      <td>1820</td>\n",
       "      <td>0.012637</td>\n",
       "      <td>0.013661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93624974918</th>\n",
       "      <td>20</td>\n",
       "      <td>1784</td>\n",
       "      <td>0.011211</td>\n",
       "      <td>0.012263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47875839090</th>\n",
       "      <td>17</td>\n",
       "      <td>1819</td>\n",
       "      <td>0.009346</td>\n",
       "      <td>0.010388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              clicked  examined     grade  beta_grade\n",
       "doc_id                                               \n",
       "826663114164       37        40  0.925000    0.780000\n",
       "97360722345        33        35  0.942857    0.777778\n",
       "97360724240        42        50  0.840000    0.733333\n",
       "97363455349       731      2120  0.344811    0.344131\n",
       "97361312804       726      2107  0.344566    0.343883\n",
       "97363560449       733      2129  0.344293    0.343619\n",
       "97361312743       708      2079  0.340548    0.339876\n",
       "97363532149       692      2098  0.329838    0.329222\n",
       "97361372389       673      2089  0.322164    0.321582\n",
       "12505525766         1        19  0.052632    0.103448\n",
       "803238004525        1        23  0.043478    0.090909\n",
       "400192926087        1        24  0.041667    0.088235\n",
       "27242813908         1        27  0.037037    0.081081\n",
       "74108007469         0        18  0.000000    0.071429\n",
       "27242799127         0        23  0.000000    0.060606\n",
       "47875842328         0        24  0.000000    0.058824\n",
       "27242815414         0        24  0.000000    0.058824\n",
       "879862003517       37      1865  0.019839    0.020800\n",
       "93624995012        36      1827  0.019704    0.020686\n",
       "47875842328        32      1809  0.017689    0.018692\n",
       "708056579746       29      1815  0.015978    0.016986\n",
       "47875332911        27      1789  0.015092    0.016120\n",
       "879862003524       25      1825  0.013699    0.014714\n",
       "47875819733        25      1833  0.013639    0.014650\n",
       "708056579739       23      1820  0.012637    0.013661\n",
       "93624974918        20      1784  0.011211    0.012263\n",
       "47875839090        17      1819  0.009346    0.010388"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random.seed(1234)\n",
    "\n",
    "query = \"transformers dvd\"\n",
    "sessions_with_exploration = generate_simulated_exploration_sessions(\n",
    "    query, sessions, logged_transformers_judgments, explore_features)\n",
    "training_data_with_exploration = \\\n",
    "    generate_training_data(sessions_with_exploration)\n",
    "display(training_data_with_exploration.loc[\"transformers dvd\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 12.11 - Rebuild model using updated judgments\n",
    "\n",
    "After showing the new document to users, we can rebuild the model using judgments that cover this feature blindspot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dryer': 0.12737002598513025,\n",
       " 'blue ray': 0.08461538461538462,\n",
       " 'headphones': 0.12110565745285455,\n",
       " 'dark of moon': 0.1492224251599605,\n",
       " 'transformers dvd': 0.2596766133918415}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random.seed(1234)\n",
    "\n",
    "promotion_feature_set = [\n",
    "    ltr.generate_fuzzy_query_feature(feature_name=\"name_fuzzy\",\n",
    "                                     field_name=\"name\"),\n",
    "    ltr.generate_bigram_query_feature(feature_name=\"name_bigram\",\n",
    "                                      field_name=\"name\"),\n",
    "    ltr.generate_bigram_query_feature(feature_name=\"short_description_bigram\",\n",
    "                                      field_name=\"short_description\"),\n",
    "    ltr.generate_query_feature(feature_name=\"has_promotion\",\n",
    "                               field_name=\"has_promotion\",\n",
    "                               value=\"true\",\n",
    "                               constant_score=True)]\n",
    "\n",
    "evaluation = train_and_evaluate_model(sessions_with_exploration,\n",
    "                                      \"ltr_model_variant_3\",\n",
    "                                      promotion_feature_set)\n",
    "display(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'name_fuzzy',\n",
       "  'class': 'org.apache.solr.ltr.feature.SolrFeature',\n",
       "  'params': {'q': 'name_ngram:(${keywords})'},\n",
       "  'store': 'ltr_model_variant_3'},\n",
       " {'name': 'name_bigram',\n",
       "  'class': 'org.apache.solr.ltr.feature.SolrFeature',\n",
       "  'params': {'q': '{!edismax qf=name pf2=name}(${keywords})'},\n",
       "  'store': 'ltr_model_variant_3'},\n",
       " {'name': 'short_description_bigram',\n",
       "  'class': 'org.apache.solr.ltr.feature.SolrFeature',\n",
       "  'params': {'q': '{!edismax qf=short_description pf2=short_description}(${keywords})'},\n",
       "  'store': 'ltr_model_variant_3'},\n",
       " {'name': 'has_promotion',\n",
       "  'class': 'org.apache.solr.ltr.feature.SolrFeature',\n",
       "  'params': {'q': 'has_promotion:true^=1'},\n",
       "  'store': 'ltr_model_variant_3'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(promotion_feature_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 12.12 - Searching with the latest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Transformers/Transformers: Revenge of the Fallen: Two-Movie Mega Collection [2 Discs] - Widescreen - DVD',\n",
       " 'Transformers: Revenge of the Fallen - Widescreen - DVD',\n",
       " 'Transformers: Dark of the Moon - Original Soundtrack - CD',\n",
       " 'Transformers: The Complete Series [25th Anniversary Matrix of Leadership Edition] [16 Discs] - DVD',\n",
       " 'Transformers: Dark of the Moon Stealth Force Edition - Nintendo Wii']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = ltr.search_with_model(\"ltr_model_variant_3\",\n",
    "                                query=\"transformers dvd\",\n",
    "                                rerank_query=\"transformers dvd\",\n",
    "                                limit=5)[\"docs\"]\n",
    "display([doc[\"name\"] for doc in results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 12.13 - Rerun A/B test on new `promotion` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ltr_model_variant_1': 21, 'ltr_model_variant_3': 145}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random.seed(1234)\n",
    "\n",
    "results = simulate_user_a_b_test(query=\"transformers dvd\",\n",
    "                                 model_a=\"ltr_model_variant_1\",\n",
    "                                 model_b=\"ltr_model_variant_3\",\n",
    "                                 number_of_users=1000)\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 12.14 - Fully Automated LTR Loop\n",
    "\n",
    "These lines expand Listing 12.13 from the book (the book content is a truncated form of what's below). You could put this in a loop and constantly try new features to try to get closer at a generalized ranking solution of what users actually want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1234)\n",
    "ltr.delete_feature_store(\"aips_feature_store\")\n",
    "\n",
    "def get_exploit_features():\n",
    "    return [\n",
    "        ltr.generate_fuzzy_query_feature(\"name_fuzzy\", \"name\"),\n",
    "        ltr.generate_query_feature(\"long_description_bm25\", \"long_description\"),\n",
    "        ltr.generate_query_feature(\"short_description_match\", \"short_description\", True)]\n",
    "\n",
    "def gather_latest_sessions(query, sessions, model_name, features):\n",
    "    \"\"\"For the sake of the examples, returns a static list of session data.\n",
    "       In a production environment, this would the most up to date user interactions\"\"\"\n",
    "    training_data = generate_training_data(sessions)\n",
    "    logged_judgments = generate_logged_judgments(training_data, features, model_name)\n",
    "    latest_sessions = generate_simulated_exploration_sessions(query,\n",
    "                                                              sessions,\n",
    "                                                              logged_judgments,\n",
    "                                                              features)\n",
    "    return latest_sessions\n",
    "\n",
    "def is_improvement(evaluation1, evaluation2):\n",
    "    #Model comparison is stubbed out\n",
    "    return True\n",
    "    \n",
    "def wait_for_more_sessions(t):\n",
    "    time.sleep(t)\n",
    "\n",
    "def ltr_retraining_loop(latest_sessions, iterations=sys.maxsize,\n",
    "                        retrain_frequency=60 * 60 * 24):\n",
    "    for i in range(0, iterations):\n",
    "        training_data = generate_training_data(latest_sessions)\n",
    "        train, test = split_training_data(training_data)\n",
    "        if i == 0:\n",
    "            exploit_features = get_exploit_features()\n",
    "            train_and_upload_model(train,\n",
    "                                   \"exploit\",\n",
    "                                   exploit_features)\n",
    "        else:\n",
    "            previous_explore_model_name = f\"explore_variant_{i-1}\"\n",
    "            exploit_model_evaluation = evaluate_model(test, \"exploit\", training_data, log=True)\n",
    "            explore_model_evaluation = evaluate_model(test, previous_explore_model_name, training_data, log=True)\n",
    "            print(f\"Exploit evaluation: {exploit_model_evaluation}\")\n",
    "            print(f\"Explore evaluation: {explore_model_evaluation}\")\n",
    "            if is_improvement(explore_model_evaluation, exploit_model_evaluation):\n",
    "                print(\"Promoting previous explore model\")\n",
    "                train_and_upload_model(train,\n",
    "                                      \"exploit\",\n",
    "                                       explore_features)\n",
    "                \n",
    "        explore_features = get_latest_explore_features()\n",
    "        train_and_upload_model(train,\n",
    "                               f\"explore_variant_{i}\",\n",
    "                               explore_features)\n",
    "        \n",
    "        wait_for_more_sessions(retrain_frequency)\n",
    "        latest_sessions = gather_latest_sessions(\"transformers dvd\", latest_sessions,\n",
    "                                                 f\"explore_variant_{i}\", explore_features)\n",
    "\n",
    "ltr_retraining_loop(sessions, 5, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up next: [Chapter 13: Semantic Search with Dense Vectors](../ch13/1.setting-up-the-outdoors-dataset.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
