{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ Chapter 12 - Overcoming Bias in Learned Relevance Models ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B Testing Simulation to Active Learning\n",
    "\n",
    "In this notebook, users have a hidden preference for a single query. We use this to explore A/B testing to see whether a given LTR model actually gives the users what they want.\n",
    "\n",
    "Then we ask, much like in real life, how can we learn what the user _actually_ wants? We employe active learning to try to escape the 'echo chamber' of presentation bias we learned about at the end of chapter 11. After all users can't click on results that never show up in their search results!\n",
    "\n",
    "## ðŸš¨ We're putting it all together in this chapter\n",
    "\n",
    "As this chapter puts together everything from chapters 10 and 11, much of the setup code below wraps up a lot of chapter 11 and 10 into a 'single function' so we can very easily run through the steps in 'one liners'\n",
    "\n",
    "### Getting training data (Ch 11)\n",
    "\n",
    "Chapter 11 is all about turning raw clickstream data into search training data (aka judgments). This involves overcoming biases in how users percieve search. But here we put that in one function call `calculate_sdbn`.\n",
    "\n",
    "### Train a model (Ch 10)\n",
    "\n",
    "Chapter 10 is about training an LTR model, including the extraction of features from the search engine, how a ranking model works, how to train a model, and how to perform a good test/train split for search. But here we similarly wrap that up into a handful of function calls, `split_training_data`, and `evaluate_model`.\n",
    "\n",
    "*long story short, if you see a reference to chapter 10 and 11, it's probably omited from chapter 12* - don't expect it to be covered in chapter 12 extensively.\n",
    "\n",
    "\n",
    "## Setup - gather some sessions (omitted)\n",
    "\n",
    "To get started, we first load a set of simulated search sessions for all queries. \n",
    "\n",
    "Much of this setup is omitted from the chapter. This first part is just loading and synthesizing a bunch of clickstream sessions, like we used in chapter 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from aips import get_engine, get_ltr_engine\n",
    "import aips.indexer\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import numpy\n",
    "import pandas   \n",
    "\n",
    "import random; random.seed(0)\n",
    "\n",
    "engine = get_engine()\n",
    "products_collection = aips.indexer.build_collection(engine, \"products_with_promotions\")\n",
    "aips.indexer.download_data_files(\"signals\")\n",
    "ltr = get_ltr_engine(products_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sess_id</th>\n",
       "      <th>query</th>\n",
       "      <th>rank</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>clicked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50002</td>\n",
       "      <td>blue ray</td>\n",
       "      <td>1.0</td>\n",
       "      <td>827396513927</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50002</td>\n",
       "      <td>blue ray</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24543672067</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50002</td>\n",
       "      <td>blue ray</td>\n",
       "      <td>3.0</td>\n",
       "      <td>719192580374</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50002</td>\n",
       "      <td>blue ray</td>\n",
       "      <td>4.0</td>\n",
       "      <td>885170033412</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50002</td>\n",
       "      <td>blue ray</td>\n",
       "      <td>5.0</td>\n",
       "      <td>58231300826</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74995</th>\n",
       "      <td>5001</td>\n",
       "      <td>transformers dark of the moon</td>\n",
       "      <td>10.0</td>\n",
       "      <td>47875841369</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74996</th>\n",
       "      <td>5001</td>\n",
       "      <td>transformers dark of the moon</td>\n",
       "      <td>11.0</td>\n",
       "      <td>97363560449</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74997</th>\n",
       "      <td>5001</td>\n",
       "      <td>transformers dark of the moon</td>\n",
       "      <td>12.0</td>\n",
       "      <td>93624956037</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74998</th>\n",
       "      <td>5001</td>\n",
       "      <td>transformers dark of the moon</td>\n",
       "      <td>13.0</td>\n",
       "      <td>97363532149</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74999</th>\n",
       "      <td>5001</td>\n",
       "      <td>transformers dark of the moon</td>\n",
       "      <td>14.0</td>\n",
       "      <td>400192926087</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1710000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sess_id                          query  rank        doc_id  clicked\n",
       "1        50002                       blue ray   1.0  827396513927    False\n",
       "2        50002                       blue ray   2.0   24543672067    False\n",
       "3        50002                       blue ray   3.0  719192580374    False\n",
       "4        50002                       blue ray   4.0  885170033412     True\n",
       "5        50002                       blue ray   5.0   58231300826    False\n",
       "...        ...                            ...   ...           ...      ...\n",
       "74995     5001  transformers dark of the moon  10.0   47875841369    False\n",
       "74996     5001  transformers dark of the moon  11.0   97363560449    False\n",
       "74997     5001  transformers dark of the moon  12.0   93624956037    False\n",
       "74998     5001  transformers dark of the moon  13.0   97363532149    False\n",
       "74999     5001  transformers dark of the moon  14.0  400192926087    False\n",
       "\n",
       "[1710000 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signals_upcs_to_omit = [600603132872, 600603125065, 600603141003, 600603139758,\n",
    "                        600603133237, 600603123061, 600603140631, 600603124570,\n",
    "                        600603132827, 600603135101]\n",
    "\n",
    "def all_sessions():\n",
    "    sessions = pandas.concat([pandas.read_csv(f, compression='gzip')\n",
    "                          for f in glob.glob('data/repositories/retrotech/sessions/*_sessions.gz')])\n",
    "    sessions = sessions.sort_values(['query', 'sess_id', 'rank'])\n",
    "    sessions = sessions.rename(columns={'clicked_doc_id': 'doc_id'})\n",
    "    return sessions[~sessions[\"doc_id\"].isin(signals_upcs_to_omit)]\n",
    "    \n",
    "sessions = all_sessions()\n",
    "sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['blue ray', 'bluray', 'dryer', 'headphones', 'ipad', 'iphone',\n",
       "       'kindle', 'lcd tv', 'macbook', 'nook', 'star trek', 'star wars',\n",
       "       'transformers dark of the moon'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sessions[\"query\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Part 2 - Add some more query sessions (omitted)\n",
    "\n",
    "Here we duplicate the simulated queries from above, but we flip a handful of the clicks. This just fills out our data a bit more, gives a bit more data to work with.\n",
    "\n",
    "## Setup Part 3 - Our test query, `transformers dvd`, with hidden, 'true' preferences\n",
    "\n",
    "We add a new query to our set of queries `transformers dvd` and we note the users' hidden preferences in the variables `desired_movies` as well as what they consider mediocre `meh_transformers_movies` and not at all relevant `irrelevant_transformers_products`. Each holds the UPC of the associated product.\n",
    "\n",
    "This simulates biased sessions in the data, as if the user never actually sees (and hence never clicks) their actual desired item. If the users desired results are shown, those results get a higher probability of click. Otherwise there is a lower probability of clicks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        sess_id     query  rank        doc_id  clicked\n",
      "1         50002  blue ray   1.0  827396513927    False\n",
      "2         50002  blue ray   2.0   24543672067    False\n",
      "3         50002  blue ray   3.0  719192580374    False\n",
      "4         50002  blue ray   4.0  885170033412     True\n",
      "5         50002  blue ray   5.0   58231300826    False\n",
      "...         ...       ...   ...           ...      ...\n",
      "149994    55001   blueray  24.0   36725617605    False\n",
      "149995    55001   blueray  25.0   22265004517    False\n",
      "149996    55001   blueray  26.0  885170038875    False\n",
      "149997    55001   blueray  27.0  786936817232    False\n",
      "149999    55001   blueray  29.0   27242815414    False\n",
      "\n",
      "[3085000 rows x 5 columns]\n",
      "Click num 4158\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sess_id</th>\n",
       "      <th>query</th>\n",
       "      <th>rank</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>clicked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50002</td>\n",
       "      <td>blue ray</td>\n",
       "      <td>1.0</td>\n",
       "      <td>827396513927</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50002</td>\n",
       "      <td>blue ray</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24543672067</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50002</td>\n",
       "      <td>blue ray</td>\n",
       "      <td>3.0</td>\n",
       "      <td>719192580374</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50002</td>\n",
       "      <td>blue ray</td>\n",
       "      <td>4.0</td>\n",
       "      <td>885170033412</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50002</td>\n",
       "      <td>blue ray</td>\n",
       "      <td>5.0</td>\n",
       "      <td>58231300826</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79995</th>\n",
       "      <td>65000</td>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>11.0</td>\n",
       "      <td>47875842328</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79996</th>\n",
       "      <td>65000</td>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>12.0</td>\n",
       "      <td>879862003517</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79997</th>\n",
       "      <td>65000</td>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>13.0</td>\n",
       "      <td>97361372389</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79998</th>\n",
       "      <td>65000</td>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>14.0</td>\n",
       "      <td>93624995012</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79999</th>\n",
       "      <td>65000</td>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>15.0</td>\n",
       "      <td>47875839090</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3165000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sess_id             query  rank        doc_id  clicked\n",
       "1        50002          blue ray   1.0  827396513927    False\n",
       "2        50002          blue ray   2.0   24543672067    False\n",
       "3        50002          blue ray   3.0  719192580374    False\n",
       "4        50002          blue ray   4.0  885170033412     True\n",
       "5        50002          blue ray   5.0   58231300826    False\n",
       "...        ...               ...   ...           ...      ...\n",
       "79995    65000  transformers dvd  11.0   47875842328    False\n",
       "79996    65000  transformers dvd  12.0  879862003517    False\n",
       "79997    65000  transformers dvd  13.0   97361372389    False\n",
       "79998    65000  transformers dvd  14.0   93624995012    False\n",
       "79999    65000  transformers dvd  15.0   47875839090    False\n",
       "\n",
       "[3165000 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(0)\n",
    "numpy.random.seed(0)\n",
    "\n",
    "def copy_query_sessions(sessions, src_query, dest_query, flip=False):\n",
    "    new_sessions = sessions[sessions[\"query\"] == src_query].copy()  \n",
    "    new_sessions[\"draw\"] = numpy.random.rand(len(new_sessions), 1)\n",
    "    new_sessions.loc[new_sessions[\"clicked\"] & (new_sessions[\"draw\"] < 0.04), \"clicked\"] = False\n",
    "    new_sessions[\"query\"] = dest_query\n",
    "    return pandas.concat([sessions, new_sessions.drop(\"draw\", axis=1)])\n",
    "\n",
    "sessions = all_sessions()\n",
    "sessions = copy_query_sessions(sessions, \"transformers dark of the moon\", \"transformers dark of moon\")\n",
    "sessions = copy_query_sessions(sessions, \"transformers dark of the moon\", \"dark of moon\")\n",
    "sessions = copy_query_sessions(sessions, \"transformers dark of the moon\", \"dark of the moon\")\n",
    "sessions = copy_query_sessions(sessions, \"headphones\", \"head phones\")\n",
    "sessions = copy_query_sessions(sessions, \"lcd tv\", \"lcd television\")\n",
    "sessions = copy_query_sessions(sessions, \"lcd tv\", \"television, lcd\")\n",
    "sessions = copy_query_sessions(sessions, \"macbook\", \"apple laptop\")\n",
    "sessions = copy_query_sessions(sessions, \"iphone\", \"apple iphone\")\n",
    "sessions = copy_query_sessions(sessions, \"kindle\", \"amazon kindle\")\n",
    "sessions = copy_query_sessions(sessions, \"kindle\", \"amazon ereader\")\n",
    "sessions = copy_query_sessions(sessions, \"blue ray\", \"blueray\")\n",
    "\n",
    "print(sessions)\n",
    "\n",
    "next_sess_id = sessions[\"sess_id\"].max()\n",
    "\n",
    "# For some reason, the sessions only capture examines on the 'dubbed' transformers movies\n",
    "# ie the Japanese shows brought to an English-speaking market. But we'll see this is not what the \n",
    "# user wants (ie presentation bias). These are 'meh' mildly interesting. There are also many many\n",
    "# completely irrelevant movies.\n",
    "\n",
    "# What the user wants, but never visible! Never gets clicked!\n",
    "\n",
    "# These are the widescreen transformers dvds of the hollywood movies\n",
    "desired_transformers_movies = [\"97360724240\", \"97360722345\", \"826663114164\"]\n",
    "# Other transformer movies\n",
    "meh_transformers_movies = [\"97363455349\", \"97361312743\", \"97361372389\",\n",
    "                           \"97361312804\", \"97363532149\", \"97363560449\"]\n",
    "# Bunch of random merchandise\n",
    "irrelevant_transformers_products = [\"708056579739\", \"93624995012\", \"47875819733\", \"47875839090\", \"708056579746\",\n",
    "                                    \"47875332911\", \"47875842328\", \"879862003524\", \"879862003517\", \"93624974918\"] \n",
    "\n",
    "\n",
    "displayed_transformer_products = meh_transformers_movies + irrelevant_transformers_products\n",
    "new_sessions = []\n",
    "click = 0\n",
    "for i in range(0, 5000):\n",
    "    random.shuffle(displayed_transformer_products)\n",
    "\n",
    "    # shuffle each session\n",
    "    for rank, upc in enumerate(displayed_transformer_products):\n",
    "        draw = random.random()        \n",
    "        clicked = ((upc in meh_transformers_movies and draw < 0.13) or\n",
    "                   (upc in irrelevant_transformers_products and draw < 0.005))\n",
    "        click += (1 if clicked else 0)\n",
    "        new_sessions.append({\"sess_id\": next_sess_id + i, \n",
    "                             \"query\": \"transformers dvd\", \n",
    "                             \"rank\": rank,\n",
    "                             \"clicked\": clicked,\n",
    "                             \"doc_id\": upc})\n",
    "\n",
    "print(\"Click num \" + str(click))\n",
    "sessions = pandas.concat([sessions, pandas.DataFrame(new_sessions)])\n",
    "sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['blue ray', 'bluray', 'dryer', 'headphones', 'ipad', 'iphone',\n",
       "       'kindle', 'lcd tv', 'macbook', 'nook', 'star trek', 'star wars',\n",
       "       'transformers dark of the moon', 'transformers dark of moon',\n",
       "       'dark of moon', 'dark of the moon', 'head phones',\n",
       "       'lcd television', 'television, lcd', 'apple laptop',\n",
       "       'apple iphone', 'amazon kindle', 'amazon ereader', 'blueray',\n",
       "       'transformers dvd'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sessions[\"query\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup 4 - chapter 11 In One Function (omitted) \n",
    "\n",
    "Wrapping up Chapter 11 in a single function `generate_training_data`. \n",
    "\n",
    "This function computes a relevance grade out of raw clickstream data. Recall that the SDBN (Simplified Dynamic Bayesian Network) click model we learned about in chapter 11 helps overcome position bias. We also use a beta prior so that a single click doesn't count as much as an observation with hundreds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load -s calculate_ctr,calculate_average_rank,caclulate_examine_probability,calculate_clicked_examined,calculate_grade,calculate_prior,calculate_sdbn ../ltr/sdbn_functions.py\n",
    "def calculate_ctr(sessions):\n",
    "    click_counts = sessions.groupby(\"doc_id\")[\"clicked\"].sum()\n",
    "    sess_counts = sessions.groupby(\"doc_id\")[\"sess_id\"].nunique()\n",
    "    ctrs = click_counts / sess_counts\n",
    "    return ctrs.sort_values(ascending=False)\n",
    "\n",
    "def calculate_average_rank(sessions):\n",
    "    avg_rank = sessions.groupby(\"doc_id\")[\"rank\"].mean()\n",
    "    return avg_rank.sort_values(ascending=True)\n",
    "\n",
    "def caclulate_examine_probability(sessions):\n",
    "    last_click_per_session = sessions.groupby([\"clicked\", \"sess_id\"])[\"rank\"].max()[True]\n",
    "    sessions[\"last_click_rank\"] = last_click_per_session\n",
    "    sessions[\"examined\"] = sessions[\"rank\"] <= sessions[\"last_click_rank\"]\n",
    "    return sessions\n",
    "\n",
    "def calculate_clicked_examined(sessions):\n",
    "    sessions = caclulate_examine_probability(sessions)\n",
    "    return sessions[sessions[\"examined\"]] \\\n",
    "        .groupby(\"doc_id\")[[\"clicked\", \"examined\"]].sum()\n",
    "\n",
    "def calculate_grade(sessions):\n",
    "    sessions = calculate_clicked_examined(sessions)\n",
    "    sessions[\"grade\"] = sessions[\"clicked\"] / sessions[\"examined\"]\n",
    "    return sessions.sort_values(\"grade\", ascending=False)\n",
    "\n",
    "def calculate_prior(sessions, prior_grade, prior_weight):\n",
    "    sessions = calculate_grade(sessions)\n",
    "    sessions[\"prior_a\"] = prior_grade * prior_weight\n",
    "    sessions[\"prior_b\"] = (1 - prior_grade) * prior_weight\n",
    "    return sessions\n",
    "\n",
    "def calculate_sdbn(sessions, prior_grade, prior_weight):\n",
    "    sessions = calculate_prior(sessions, prior_grade, prior_weight)\n",
    "    sessions[\"posterior_a\"] = (sessions[\"prior_a\"] + \n",
    "                               sessions[\"clicked\"])\n",
    "    sessions[\"posterior_b\"] = (sessions[\"prior_b\"] + \n",
    "                               sessions[\"examined\"] - sessions[\"clicked\"])\n",
    "    sessions[\"beta_grade\"] = (sessions[\"posterior_a\"] /\n",
    "      (sessions[\"posterior_a\"] + sessions[\"posterior_b\"]))\n",
    "    return sessions.sort_values(\"beta_grade\", ascending=False)\n",
    "\n",
    "def generate_training_data(sessions, prior_grade=0.2, prior_weight=10):\n",
    "    all_sdbn = pandas.DataFrame()\n",
    "    for query in sessions[\"query\"].unique():        \n",
    "        query_sessions = sessions[sessions[\"query\"] == query].copy().set_index(\"sess_id\")\n",
    "        query_sessions = calculate_sdbn(query_sessions, prior_grade, prior_weight)\n",
    "        query_sessions[\"query\"] = query\n",
    "        all_sdbn = pandas.concat([all_sdbn, query_sessions])\n",
    "    return all_sdbn[[\"query\", \"clicked\", \"examined\", \"grade\", \"beta_grade\"]].reset_index().set_index([\"query\", \"doc_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 10 Functions (omitted from book)\n",
    "\n",
    "Now with the chapter 11 setup out of the way, we'll need to give Chapter 10's code a similar treatment, wrapping that LTR system into a black box.\n",
    "\n",
    "All of the following are support functions for the chapter:\n",
    "\n",
    "1. Convert the sdbn dataframe into individual `Judgment` objects needed for training the model from chapter 10\n",
    "2. Pairwise transformation of the data\n",
    "3. Normalization of the data\n",
    "4. Training the model\n",
    "5. Uploading the model to Solr\n",
    "\n",
    "All of these steps are covered in Chapter 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy\n",
    "from ltr.judgments import judgments_to_nparray\n",
    "from sklearn import svm\n",
    "from itertools import groupby\n",
    "from ltr.log import FeatureLogger\n",
    "from itertools import groupby\n",
    "from ltr.judgments import judgments_writer\n",
    "\n",
    "from ltr.judgments import Judgment\n",
    "\n",
    "def as_judgments(training_data):\n",
    "    \"\"\"Turn pandas dataframe into ltr judgments objects.\"\"\"        \n",
    "    qid_map = {}\n",
    "    judgments = []\n",
    "    next_qid = 0\n",
    "    for datum in training_data.reset_index().to_dict(orient=\"records\"):       \n",
    "        if datum[\"query\"] not in qid_map:\n",
    "            qid_map[datum[\"query\"]] = next_qid\n",
    "            next_qid += 1\n",
    "        qid = qid_map[datum[\"query\"]]\n",
    "\n",
    "        judgments.append(Judgment(doc_id=datum[\"doc_id\"],\n",
    "                        keywords=datum[\"query\"],\n",
    "                        qid=qid,\n",
    "                        grade=datum[\"beta_grade\"]))\n",
    "        \n",
    "    return judgments\n",
    "\n",
    "def normalize_features(logged_judgments):\n",
    "    num_features = len(logged_judgments[0].features)\n",
    "    means = [numpy.mean([j.features[i] for j in logged_judgments])\n",
    "             for i in range(0, num_features)]    \n",
    "    \n",
    "    std_devs = [numpy.std([j.features[i] for j in logged_judgments])\n",
    "                for i in range(0, num_features)]\n",
    "    \n",
    "    normed_judgments = copy.deepcopy(logged_judgments)\n",
    "    for j in normed_judgments:\n",
    "        for i, score in enumerate(j.features):\n",
    "            j.features[i] = (score - means[i]) / std_devs[i]\n",
    "\n",
    "    return means, std_devs, normed_judgments\n",
    "\n",
    "def pairwise_transform(normed_judgments):        \n",
    "    predictor_deltas = []\n",
    "    feature_deltas = []\n",
    "    for qid, grouped_judgments in groupby(normed_judgments, key=lambda j: j.qid):\n",
    "        query_judgments = list(grouped_judgments)\n",
    "        for judgment1 in query_judgments:\n",
    "            for judgment2 in query_judgments:\n",
    "                j1_features = numpy.array(judgment1.features)\n",
    "                j2_features = numpy.array(judgment2.features)\n",
    "                \n",
    "                if judgment1.grade > judgment2.grade:\n",
    "                    predictor_deltas.append(1)\n",
    "                    feature_deltas.append(j1_features - j2_features)\n",
    "                elif judgment1.grade < judgment2.grade:\n",
    "                    predictor_deltas.append(-1)\n",
    "                    feature_deltas.append(j1_features - j2_features)\n",
    "\n",
    "    return numpy.array(feature_deltas), numpy.array(predictor_deltas)\n",
    "\n",
    "def write_judgments(judgments, dest=\"retrotech_judgments.txt\"):\n",
    "    with judgments_writer(open(dest, \"wt\")) as writer:\n",
    "        for judgment in judgments:\n",
    "            writer.write(judgment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Also Chapter 10 - Perform a test / train split on the SDBN data (omitted)\n",
    "\n",
    "This function is broken out from the model training. It lets us train a model on one set of data (reusing the chapter 10 training code), reserving test queries for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "\n",
    "def split_training_data(training_data, train_proportion=0.8):\n",
    "    \"\"\"Split queries in training_data into train / test split with `train` proportion going to training set.\"\"\"\n",
    "    queries = training_data.index.get_level_values('query').unique().copy().tolist()\n",
    "    random.shuffle(queries)\n",
    "    num_queries = len(queries)\n",
    "    split_point = floor(num_queries * train_proportion)\n",
    "    \n",
    "    train_queries = queries[:split_point]\n",
    "    test_queries = queries[split_point:]\n",
    "    return training_data.loc[train_queries, :], training_data.loc[test_queries]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 10 - Evaluate the model on the test set (omitted)\n",
    "\n",
    "This function computes the model's performance on a set of test queries. The `test_data` is the control set not used to train the model. We compute the precision of these queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm_model(model_name, features, logged_judgments):\n",
    "    means, std_devs, normed_judgments = normalize_features(logged_judgments)\n",
    "    feature_deltas, predictor_deltas = pairwise_transform(normed_judgments)\n",
    "\n",
    "    model = svm.LinearSVC(max_iter=10000, verbose=1)\n",
    "    model.fit(feature_deltas, predictor_deltas) \n",
    "\n",
    "    feature_names = [ftr[\"name\"] for ftr in features]\n",
    "    linear_model = ltr.generate_model(model_name, feature_names,\n",
    "                                      means, std_devs, model.coef_[0])\n",
    "\n",
    "    return linear_model\n",
    "\n",
    "def train_and_upload_model(training_data, model_name, features, log=False):\n",
    "    \"\"\"Train a RankSVM model with features from the search engine and upload the model.\"\"\"\n",
    "    judgments = as_judgments(training_data)\n",
    "    ltr.delete_feature_store(model_name, log=log)\n",
    "    ltr.delete_model(model_name)\n",
    "    ltr.upload_features(features, model_name, log=log)\n",
    "    ftr_logger = FeatureLogger(products_collection, feature_set=model_name,\n",
    "                               id_field=\"upc\")\n",
    "            \n",
    "    for qid, query_judgments in groupby(judgments, key=lambda j: j.qid):\n",
    "        ftr_logger.log_for_qid(judgments=query_judgments, \n",
    "                               qid=qid, log=log)\n",
    "\n",
    "    linear_model = train_svm_model(model_name, features, ftr_logger.logged)\n",
    "    ltr.upload_model(linear_model, log=log)\n",
    "    return linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(test_data, model_name, training_data, limit=10, log=False):\n",
    "    queries = test_data.index.get_level_values(\"query\").unique()\n",
    "    query_results = {}\n",
    "    \n",
    "    for query in queries:\n",
    "        response = ltr.search_with_model(model_name, query=query,\n",
    "                                         limit=limit, rerank_query=query, log=log)\n",
    "    \n",
    "        results = pandas.DataFrame(response[\"docs\"]).reset_index()\n",
    "        judgments = training_data.loc[query, :].copy().reset_index()\n",
    "        judgments[\"doc_id\"] = judgments[\"doc_id\"].astype(str)\n",
    "        if len(results) == 0:\n",
    "            print(f\"No Results for {query}\")\n",
    "            query_results[query] = 0\n",
    "        else:\n",
    "            graded_results = results.merge(judgments, left_on=\"upc\",\n",
    "                                           right_on=\"doc_id\", how=\"left\")\n",
    "            graded_results[[\"clicked\", \"examined\", \"grade\", \"beta_grade\"]] = graded_results[[\"clicked\", \"examined\", \"grade\", \"beta_grade\"]].fillna(0)\n",
    "            graded_results = graded_results.drop(\"doc_id\", axis=1)\n",
    "            if log:\n",
    "                print(graded_results.drop([\"index\", \"manufacturer\", \"short_description\",\n",
    "                                           \"long_description\", \"grade\", \"name\"], axis=1))\n",
    "\n",
    "            query_results[query] = float(graded_results[\"beta_grade\"].sum() / limit)\n",
    "    return query_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(sessions, model_name, features, log=False):\n",
    "    training_data = generate_training_data(sessions)\n",
    "    train, test = split_training_data(training_data, 0.8)\n",
    "    train_and_upload_model(train, model_name, features=features, log=log)\n",
    "    evaluation = evaluate_model(test, model_name, training_data, log=log)\n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dryer': 0.03753076750950996,\n",
       " 'blue ray': 0.0,\n",
       " 'headphones': 0.06540945492120899,\n",
       " 'dark of moon': 0.0,\n",
       " 'transformers dvd': 0.0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random.seed(1234)\n",
    "feature_set = [\n",
    "    ltr.generate_query_feature(feature_name=\"long_description_bm25\",\n",
    "                               field_name=\"long_description\"),\n",
    "    ltr.generate_query_feature(feature_name=\"short_description_constant\",\n",
    "                               field_name=\"short_description\",\n",
    "                               constant_score=True)]\n",
    "\n",
    "evaluation = train_and_evaluate_model(sessions, \"ltr_model_variant_1\", feature_set)\n",
    "display(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'long_description_bm25',\n",
       "  'class': 'org.apache.solr.ltr.feature.SolrFeature',\n",
       "  'params': {'q': 'long_description:(${keywords})'},\n",
       "  'store': 'ltr_model_variant_1'},\n",
       " {'name': 'short_description_constant',\n",
       "  'class': 'org.apache.solr.ltr.feature.SolrFeature',\n",
       "  'params': {'q': 'short_description:(${keywords})^=1'},\n",
       "  'store': 'ltr_model_variant_1'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(feature_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 12.3\n",
    "\n",
    "Train a model that hypothetically performs better offline called `ltr_model_variant_2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dryer': 0.07068309073137659,\n",
       " 'blue ray': 0.0,\n",
       " 'headphones': 0.06540945492120899,\n",
       " 'dark of moon': 0.257659200402958,\n",
       " 'transformers dvd': 0.10077083021678328}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random.seed(1234)\n",
    "td = generate_training_data(sessions).reset_index().set_index(\"query\")\n",
    "#print(td.loc[\"headphones\"])\n",
    "\n",
    "feature_set = [\n",
    "    ltr.generate_fuzzy_query_feature(feature_name=\"name_fuzzy\", \n",
    "                                     field_name=\"name\"),\n",
    "    ltr.generate_bigram_query_feature(feature_name=\"name_bigram\",\n",
    "                                      field_name=\"name\"),\n",
    "    ltr.generate_bigram_query_feature(feature_name=\"short_description_bigram\",\n",
    "                                      field_name=\"short_description\")\n",
    "]\n",
    "\n",
    "evaluation = train_and_evaluate_model(sessions, \"ltr_model_variant_2\", feature_set)\n",
    "display(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate a user querying, clicking, purchasing (omitted)\n",
    "\n",
    "This function simulates a user performing a query and possibly taking an action as they scan down the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_live_user_session(query, model_name,\n",
    "                               desired_probability=0.15,\n",
    "                               indifferent_probability=0.03,\n",
    "                               uninterested_probability=0.01,\n",
    "                               quit_per_result_probability=0.2):\n",
    "    \"\"\"Simulates a user 'query' where purchase probability depends on if \n",
    "       products upc is in one of three sets.\n",
    "       \n",
    "       Users purchase a single product per session.    \n",
    "       \n",
    "       Users quit with `quit_per_result_probability` after scanning each rank\n",
    "       \n",
    "       \"\"\"   \n",
    "    desired_products = [\"97360724240\", \"97360722345\", \"826663114164\"]\n",
    "    indifferent_products = [\"97363455349\", \"97361312743\", \"97361372389\",\n",
    "                            \"97361312804\", \"97363532149\", \"97363560449\"]\n",
    "    \n",
    "    response = ltr.search_with_model(model_name, query=query, rerank_query=query, limit=10)\n",
    "\n",
    "    results = pandas.DataFrame(response[\"docs\"]).reset_index()\n",
    "    for doc in results.to_dict(orient=\"records\"): \n",
    "        draw = random.random()\n",
    "        \n",
    "        if doc[\"upc\"] in desired_products:\n",
    "            if draw < desired_probability:\n",
    "                return True\n",
    "        elif doc[\"upc\"] in indifferent_products:\n",
    "            if draw < indifferent_probability:\n",
    "                return True\n",
    "        elif draw < uninterested_probability:\n",
    "            return True\n",
    "        if random.random() < quit_per_result_probability:\n",
    "            return False\n",
    "        \n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 12.4 - Simulated A/B test on just `transformers dvd` query\n",
    "\n",
    "Here we simulate 1000 users being served two rankings for `transformers dvd` and based on the hidden preferences here (`wants_to_purchase` and `might_purchase`) we see which performs better with conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_b_test(query, model_a, model_b):\n",
    "    \"\"\"Randomly assign this user to a or b\"\"\"\n",
    "    draw = random.random()\n",
    "    model_name = model_a if draw < 0.5 else model_b    \n",
    "    purchase_made = simulate_live_user_session(query, model_name)\n",
    "    return (model_name, purchase_made)\n",
    "\n",
    "def simulate_user_a_b_test(query, model_a, model_b, number_of_users=1000):\n",
    "    purchases = {model_a: 0, model_b: 0}\n",
    "    for _ in range(number_of_users): \n",
    "        model_name, purchase_made = a_b_test(query, model_a, model_b)\n",
    "        if purchase_made:\n",
    "            purchases[model_name] += 1\n",
    "    return purchases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ltr_model_variant_1': 21, 'ltr_model_variant_2': 18}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Takes about 10 minutes depending on which search engine is being used.\n",
    "random.seed(1234)\n",
    "\n",
    "results = simulate_user_a_b_test(query=\"transformers dvd\",\n",
    "                                 model_a=\"ltr_model_variant_1\",\n",
    "                                 model_b=\"ltr_model_variant_2\",\n",
    "                                 number_of_users=1000)\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New helper: show the features for each SDBN entry (omitted)\n",
    "\n",
    "This function shows us the logged features of each training row for the given sdbn data for debugging.\n",
    "\n",
    "So not just\n",
    "\n",
    "| query   | doc      | grade\n",
    "|---------|----------|---------\n",
    "|transformers dvd | 1234 | 1.0\n",
    "\n",
    "But also a recording of the matches that occured\n",
    "\n",
    "| query           | doc      | grade    | short_desc_match  | long_desc_match |...\n",
    "|-----------------|----------|----------|-------------------|-----------------|---\n",
    "|transformers dvd | 1234     | 1.0      | 0.0               | 1.0             |..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_logged_judgments(training_data, features, model_name):\n",
    "    \"\"\"Log features alongside training_data into a dataframe\"\"\"\n",
    "    judgments = as_judgments(training_data)\n",
    "    ltr.delete_feature_store(model_name)\n",
    "    ltr.upload_features(features, model_name)\n",
    "\n",
    "    ftr_logger = FeatureLogger(index=products_collection,\n",
    "                               feature_set=model_name, id_field=\"upc\")\n",
    "\n",
    "    for qid, query_judgments in groupby(judgments, key=lambda j: j.qid):\n",
    "        ftr_logger.log_for_qid(judgments=query_judgments,\n",
    "                               qid=qid, log=False)\n",
    "        \n",
    "    logged_judgments = ftr_logger.logged\n",
    "    feature_data, predictors, doc_ids = judgments_to_nparray(logged_judgments)\n",
    "    logged_judgments_dataframe = pandas.concat([pandas.DataFrame(predictors),\n",
    "                                                pandas.DataFrame(feature_data),\n",
    "                                                pandas.DataFrame(doc_ids)], \n",
    "                                                axis=1,\n",
    "                                                ignore_index=True)\n",
    "    \n",
    "    qid_map = {j.qid: j.keywords for j in logged_judgments}\n",
    "    qid_map = pandas.DataFrame(qid_map.values()).reset_index() \\\n",
    "                         .rename(columns={\"index\": \"qid\", 0: \"query\"})\n",
    "    \n",
    "    feature_names = [f[\"name\"] for f in features]\n",
    "    columns = {i: name for i, name in enumerate([\"grade\", \"qid\"] + feature_names + [\"doc_id\"])}\n",
    "\n",
    "    logged_judgments_dataframe = logged_judgments_dataframe.rename(columns=columns)\n",
    "    logged_judgments_dataframe = logged_judgments_dataframe.merge(qid_map, how=\"left\", on=\"qid\")\n",
    "    ordered_columns = [\"doc_id\", \"query\", \"grade\"] + feature_names\n",
    "    #logged_judgments_dataframe['grade'] = logged_judgments_dataframe['grade'] / 10.0 \n",
    "    \n",
    "    return logged_judgments_dataframe[ordered_columns].set_index(\"doc_id\").sort_values(\"grade\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 12.5 - Output matches for one feature set\n",
    "\n",
    "Another way of formulating `presentation_bias` is to look at the kinds of documents not being shown to users, so we can strategically show those to users. Below we show the value of each feature in `explore_feature_set` for each document in the sdbn judgments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_explore_features():\n",
    "    return [\n",
    "        ltr.generate_query_feature(feature_name=\"long_description_match\",\n",
    "                                   field_name=\"long_description\",\n",
    "                                   constant_score=True),\n",
    "        ltr.generate_query_feature(feature_name=\"short_description_match\",\n",
    "                                   field_name=\"short_description\",\n",
    "                                   constant_score=True),\n",
    "        ltr.generate_query_feature(feature_name=\"name_match\",\n",
    "                                   field_name=\"name\",\n",
    "                                   constant_score=True),\n",
    "        ltr.generate_query_feature(feature_name=\"has_promotion\",\n",
    "                                   field_name=\"has_promotion\",\n",
    "                                   value=\"true\",\n",
    "                                   constant_score=True)]\n",
    "\n",
    "def get_logged_transformers_judgments(sessions, features):\n",
    "    training_data = generate_training_data(sessions)\n",
    "    logged_judgments = generate_logged_judgments(training_data,\n",
    "                                                 features, \"explore\")\n",
    "    logged_judgments = logged_judgments \\\n",
    "        [logged_judgments[\"query\"] == \"transformers dvd\"]\n",
    "    return logged_judgments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>grade</th>\n",
       "      <th>long_description_match</th>\n",
       "      <th>short_description_match</th>\n",
       "      <th>name_match</th>\n",
       "      <th>has_promotion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>97363560449</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.347137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97361312804</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.344041</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97361312743</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.342160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97363455349</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.342065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97361372389</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.323484</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97363532149</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.322664</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879862003517</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.022834</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93624995012</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.020202</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47875842328</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.018530</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708056579746</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.016726</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47875332911</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.015854</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47875819733</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.015394</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708056579739</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.014979</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879862003524</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.014749</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93624974918</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.012628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47875839090</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.010721</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         query     grade  long_description_match  \\\n",
       "doc_id                                                             \n",
       "97363560449   transformers dvd  0.347137                     0.0   \n",
       "97361312804   transformers dvd  0.344041                     0.0   \n",
       "97361312743   transformers dvd  0.342160                     0.0   \n",
       "97363455349   transformers dvd  0.342065                     0.0   \n",
       "97361372389   transformers dvd  0.323484                     0.0   \n",
       "97363532149   transformers dvd  0.322664                     0.0   \n",
       "879862003517  transformers dvd  0.022834                     0.0   \n",
       "93624995012   transformers dvd  0.020202                     0.0   \n",
       "47875842328   transformers dvd  0.018530                     1.0   \n",
       "708056579746  transformers dvd  0.016726                     1.0   \n",
       "47875332911   transformers dvd  0.015854                     1.0   \n",
       "47875819733   transformers dvd  0.015394                     1.0   \n",
       "708056579739  transformers dvd  0.014979                     1.0   \n",
       "879862003524  transformers dvd  0.014749                     1.0   \n",
       "93624974918   transformers dvd  0.012628                     0.0   \n",
       "47875839090   transformers dvd  0.010721                     1.0   \n",
       "\n",
       "              short_description_match  name_match  has_promotion  \n",
       "doc_id                                                            \n",
       "97363560449                       0.0         1.0            0.0  \n",
       "97361312804                       0.0         1.0            0.0  \n",
       "97361312743                       0.0         1.0            0.0  \n",
       "97363455349                       0.0         1.0            0.0  \n",
       "97361372389                       0.0         1.0            0.0  \n",
       "97363532149                       0.0         1.0            0.0  \n",
       "879862003517                      1.0         1.0            0.0  \n",
       "93624995012                       0.0         1.0            0.0  \n",
       "47875842328                       0.0         1.0            1.0  \n",
       "708056579746                      0.0         1.0            0.0  \n",
       "47875332911                       0.0         1.0            0.0  \n",
       "47875819733                       0.0         1.0            0.0  \n",
       "708056579739                      1.0         1.0            0.0  \n",
       "879862003524                      1.0         1.0            0.0  \n",
       "93624974918                       0.0         1.0            0.0  \n",
       "47875839090                       0.0         1.0            0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "explore_features = get_latest_explore_features()\n",
    "logged_transformers_judgments = get_logged_transformers_judgments(sessions,\n",
    "                                                                  explore_features)\n",
    "display(logged_transformers_judgments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 12.6 - Train Gaussian Process Regressor\n",
    "\n",
    "We train data on just the `transformers_training_data`. \n",
    "\n",
    "NOTE we could also train on the full sdbn training data, and see globally what's missing. However it's often convenient to zero in on specific queries to round out their training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "def train_gpr(logged_judgments, feature_names):\n",
    "    feature_data = logged_judgments[feature_names]\n",
    "    grades = logged_judgments[\"grade\"]\n",
    "    gpr = GaussianProcessRegressor()\n",
    "    gpr.fit(feature_data, grades)\n",
    "    return gpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianProcessRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianProcessRegressor</label><div class=\"sk-toggleable__content\"><pre>GaussianProcessRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianProcessRegressor()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = [f[\"name\"] for f in explore_features]\n",
    "train_gpr(logged_transformers_judgments, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 12.7: Predict on every value\n",
    "\n",
    "Here `gpr` predicts on every possible feature value. This lets us analyze which set of feature values to use when exploring with users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prediction_data(logged_judgments, feature_names):\n",
    "    index = pandas.MultiIndex.from_product([[0, 1]] * 4,\n",
    "                                           names=feature_names)\n",
    "    with_prediction = pandas.DataFrame(index=index).reset_index()\n",
    "\n",
    "    gpr = train_gpr(logged_judgments, feature_names)\n",
    "    predictions_with_std = gpr.predict(\n",
    "        with_prediction[feature_names], return_std=True)\n",
    "    with_prediction[\"predicted_grade\"] = predictions_with_std[0]\n",
    "    with_prediction[\"predicted_stddev\"] = predictions_with_std[1]\n",
    "   \n",
    "    return  with_prediction.sort_values(\"predicted_stddev\", ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>long_description_match</th>\n",
       "      <th>short_description_match</th>\n",
       "      <th>name_match</th>\n",
       "      <th>has_promotion</th>\n",
       "      <th>predicted_grade</th>\n",
       "      <th>predicted_stddev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.256798</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014674</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014864</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.022834</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.018530</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.161596</td>\n",
       "      <td>0.632121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014856</td>\n",
       "      <td>0.632121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017392</td>\n",
       "      <td>0.739305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.155756</td>\n",
       "      <td>0.795060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.795060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009016</td>\n",
       "      <td>0.795060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013849</td>\n",
       "      <td>0.795060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011239</td>\n",
       "      <td>0.795060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.098013</td>\n",
       "      <td>0.882676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009011</td>\n",
       "      <td>0.882676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010549</td>\n",
       "      <td>0.912794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    long_description_match  short_description_match  name_match  \\\n",
       "2                        0                        0           1   \n",
       "10                       1                        0           1   \n",
       "14                       1                        1           1   \n",
       "6                        0                        1           1   \n",
       "11                       1                        0           1   \n",
       "3                        0                        0           1   \n",
       "15                       1                        1           1   \n",
       "7                        0                        1           1   \n",
       "0                        0                        0           0   \n",
       "8                        1                        0           0   \n",
       "12                       1                        1           0   \n",
       "4                        0                        1           0   \n",
       "9                        1                        0           0   \n",
       "1                        0                        0           0   \n",
       "13                       1                        1           0   \n",
       "5                        0                        1           0   \n",
       "\n",
       "    has_promotion  predicted_grade  predicted_stddev  \n",
       "2               0         0.256798          0.000004  \n",
       "10              0         0.014674          0.000005  \n",
       "14              0         0.014864          0.000007  \n",
       "6               0         0.022834          0.000010  \n",
       "11              1         0.018530          0.000010  \n",
       "3               1         0.161596          0.632121  \n",
       "15              1         0.014856          0.632121  \n",
       "7               1         0.017392          0.739305  \n",
       "0               0         0.155756          0.795060  \n",
       "8               0         0.008900          0.795060  \n",
       "12              0         0.009016          0.795060  \n",
       "4               0         0.013849          0.795060  \n",
       "9               1         0.011239          0.795060  \n",
       "1               1         0.098013          0.882676  \n",
       "13              1         0.009011          0.882676  \n",
       "5               1         0.010549          0.912794  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction_data = calculate_prediction_data(logged_transformers_judgments,\n",
    "                                                            feature_names)\n",
    "display(prediction_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 12.8 - Calculate Expected Improvement\n",
    "\n",
    "\n",
    "We use [Expected Improvement](https://distill.pub/2020/bayesian-optimization/) scoring to select candidates for exploration within the `transformers dvd` query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def calculate_expected_improvement(logged_judgments, feature_names, theta=0.6):\n",
    "    data = calculate_prediction_data(logged_judgments, feature_names)\n",
    "    data[\"opportunity\"] = (data[\"predicted_grade\"] -\n",
    "                           logged_judgments[\"grade\"].mean() -\n",
    "                           theta)\n",
    "    data[\"prob_of_improvement\"] = (\n",
    "        norm.cdf(data[\"opportunity\"] /\n",
    "                 data[\"predicted_stddev\"]))\n",
    "    data[\"expected_improvement\"] = (\n",
    "        data[\"opportunity\"] * data[\"prob_of_improvement\"] + \n",
    "        data[\"predicted_stddev\"] *\n",
    "        norm.pdf(data[\"opportunity\"] /\n",
    "                 data[\"predicted_stddev\"]))    \n",
    "    return data.sort_values(\"expected_improvement\",\n",
    "                            ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>long_description_match</th>\n",
       "      <th>short_description_match</th>\n",
       "      <th>name_match</th>\n",
       "      <th>has_promotion</th>\n",
       "      <th>predicted_grade</th>\n",
       "      <th>predicted_stddev</th>\n",
       "      <th>opportunity</th>\n",
       "      <th>prob_of_improvement</th>\n",
       "      <th>expected_improvement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.098013</td>\n",
       "      <td>0.882676</td>\n",
       "      <td>-0.638497</td>\n",
       "      <td>0.234728</td>\n",
       "      <td>0.121201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010549</td>\n",
       "      <td>0.912794</td>\n",
       "      <td>-0.725962</td>\n",
       "      <td>0.213214</td>\n",
       "      <td>0.110633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.155756</td>\n",
       "      <td>0.795060</td>\n",
       "      <td>-0.580755</td>\n",
       "      <td>0.232556</td>\n",
       "      <td>0.107853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009011</td>\n",
       "      <td>0.882676</td>\n",
       "      <td>-0.727500</td>\n",
       "      <td>0.204914</td>\n",
       "      <td>0.101653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013849</td>\n",
       "      <td>0.795060</td>\n",
       "      <td>-0.722661</td>\n",
       "      <td>0.181691</td>\n",
       "      <td>0.078549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011239</td>\n",
       "      <td>0.795060</td>\n",
       "      <td>-0.725272</td>\n",
       "      <td>0.180826</td>\n",
       "      <td>0.078076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009016</td>\n",
       "      <td>0.795060</td>\n",
       "      <td>-0.727495</td>\n",
       "      <td>0.180091</td>\n",
       "      <td>0.077675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.795060</td>\n",
       "      <td>-0.727610</td>\n",
       "      <td>0.180053</td>\n",
       "      <td>0.077654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017392</td>\n",
       "      <td>0.739305</td>\n",
       "      <td>-0.719118</td>\n",
       "      <td>0.165353</td>\n",
       "      <td>0.064866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.161596</td>\n",
       "      <td>0.632121</td>\n",
       "      <td>-0.574914</td>\n",
       "      <td>0.181543</td>\n",
       "      <td>0.062387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014856</td>\n",
       "      <td>0.632121</td>\n",
       "      <td>-0.721654</td>\n",
       "      <td>0.126802</td>\n",
       "      <td>0.039922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.256798</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.479713</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014674</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.721837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014864</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.721646</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.022834</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.713676</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.018530</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.717981</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    long_description_match  short_description_match  name_match  \\\n",
       "1                        0                        0           0   \n",
       "5                        0                        1           0   \n",
       "0                        0                        0           0   \n",
       "13                       1                        1           0   \n",
       "4                        0                        1           0   \n",
       "9                        1                        0           0   \n",
       "12                       1                        1           0   \n",
       "8                        1                        0           0   \n",
       "7                        0                        1           1   \n",
       "3                        0                        0           1   \n",
       "15                       1                        1           1   \n",
       "2                        0                        0           1   \n",
       "10                       1                        0           1   \n",
       "14                       1                        1           1   \n",
       "6                        0                        1           1   \n",
       "11                       1                        0           1   \n",
       "\n",
       "    has_promotion  predicted_grade  predicted_stddev  opportunity  \\\n",
       "1               1         0.098013          0.882676    -0.638497   \n",
       "5               1         0.010549          0.912794    -0.725962   \n",
       "0               0         0.155756          0.795060    -0.580755   \n",
       "13              1         0.009011          0.882676    -0.727500   \n",
       "4               0         0.013849          0.795060    -0.722661   \n",
       "9               1         0.011239          0.795060    -0.725272   \n",
       "12              0         0.009016          0.795060    -0.727495   \n",
       "8               0         0.008900          0.795060    -0.727610   \n",
       "7               1         0.017392          0.739305    -0.719118   \n",
       "3               1         0.161596          0.632121    -0.574914   \n",
       "15              1         0.014856          0.632121    -0.721654   \n",
       "2               0         0.256798          0.000004    -0.479713   \n",
       "10              0         0.014674          0.000005    -0.721837   \n",
       "14              0         0.014864          0.000007    -0.721646   \n",
       "6               0         0.022834          0.000010    -0.713676   \n",
       "11              1         0.018530          0.000010    -0.717981   \n",
       "\n",
       "    prob_of_improvement  expected_improvement  \n",
       "1              0.234728              0.121201  \n",
       "5              0.213214              0.110633  \n",
       "0              0.232556              0.107853  \n",
       "13             0.204914              0.101653  \n",
       "4              0.181691              0.078549  \n",
       "9              0.180826              0.078076  \n",
       "12             0.180091              0.077675  \n",
       "8              0.180053              0.077654  \n",
       "7              0.165353              0.064866  \n",
       "3              0.181543              0.062387  \n",
       "15             0.126802              0.039922  \n",
       "2              0.000000              0.000000  \n",
       "10             0.000000              0.000000  \n",
       "14             0.000000              0.000000  \n",
       "6              0.000000              0.000000  \n",
       "11             0.000000              0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "improvement_data = calculate_expected_improvement(\n",
    "    logged_transformers_judgments, feature_names)\n",
    "display(improvement_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a query to fetch `explore` docs (omitted)\n",
    "\n",
    "Based on the selected features from the GaussianProcessRegressor, we create a query to fetch a doc that contains those features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_for_explore_candidate(explore_vector, query=\"\"):\n",
    "    feature_config = {\n",
    "        \"long_description_match\": {\"field\": \"long_description\", \"value\": query},\n",
    "        \"short_description_match\": {\"field\": \"short_description\", \"value\": query},\n",
    "        \"name_match\": {\"field\": \"name\", \"value\": query},\n",
    "        \"long_description_bm25\": {\"field\": \"long_description\", \"value\": query},\n",
    "        \"manufacturer_match\": {\"field\": \"manufacturer\", \"value\": query},\n",
    "        \"has_promotion\": {\"field\": \"has_promotion\", \"value\": \"true\"}\n",
    "    }\n",
    "    explore_candidates = ltr.get_explore_candidate(query, explore_vector, feature_config)\n",
    "    if explore_candidates:\n",
    "        return explore_candidates[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 12.9 - Find document to explore\n",
    "\n",
    "Here we fetch a document that matches the properties of something missing from our training set to display to the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore(query, logged_judgments, features):\n",
    "    \"\"\"Explore according to the provided explore vector, select\n",
    "       a random doc from that group.\"\"\"\n",
    "    feature_names = [f[\"name\"] for f in features]\n",
    "    prediction_data = calculate_expected_improvement(logged_judgments,\n",
    "                                                     feature_names)\n",
    "    explore_vector = prediction_data.head().iloc[0][feature_names]\n",
    "    return search_for_explore_candidate(explore_vector, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27242813908\n"
     ]
    }
   ],
   "source": [
    "#Investigate why this result changes sometimes. Should be 826663114164\n",
    "random.seed(0)\n",
    "\n",
    "explore_features = get_latest_explore_features()\n",
    "logged_judgments = get_logged_transformers_judgments(sessions,\n",
    "                                                     explore_features)\n",
    "explore_upc = explore(\"transformers dvd\", logged_judgments,\n",
    "                                          explore_features)[\"upc\"]\n",
    "print(explore_upc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New heavily clicked doc is promoted!\n",
    "\n",
    "```\n",
    "{\"upc\": \"826663114164\",\n",
    " \"name\": \"Transformers: The Complete Series [25th Anniversary Matrix of Leadership Edition] [16 Discs] - DVD\",\n",
    " \"manufacturer\": \" \",\n",
    " \"short_description\": \" \",\n",
    " \"long_description\": \" \",\n",
    " \"has_promotion\": True}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate new sessions with the new data\n",
    "\n",
    "We simulate new sessions, if the upc is in `might_purchase` or `wants_to_purchase`, we set it to 'clicked' with a given probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_simulated_exploration_sessions(query, sessions,\n",
    "                                            logged_judgments, features, n=500):\n",
    "    \"\"\"Conducts N (500) searches with the query and returns session data with\n",
    "       simulated the simulated user behavior\"\"\"\n",
    "    wants_to_purchase = [97360724240, 97360722345, 826663114164, 97360810042, 93624956037]\n",
    "    might_purchase = [97363455349, 97361312743, 97361372389,\n",
    "                      97361312804, 97363532149, 97363560449]\n",
    "    explore_on_rank = 2.0\n",
    "    with_explore_sessions = sessions.copy()\n",
    "    query_sessions = with_explore_sessions[with_explore_sessions[\"query\"] == query]\n",
    "    for i in range(0, n):\n",
    "        explore_doc = explore(query, logged_judgments, features)\n",
    "        if explore_doc:\n",
    "            explore_upc = int(explore_doc[\"upc\"])\n",
    "            sess_ids = list(set(query_sessions[\"sess_id\"].tolist()))\n",
    "            random.shuffle(sess_ids)\n",
    "            new_session = query_sessions[query_sessions[\"sess_id\"] == sess_ids[0]].copy()\n",
    "            new_session[\"sess_id\"] = 100000 + i\n",
    "            new_session.loc[new_session[\"rank\"] == explore_on_rank, \"doc_id\"] = explore_upc\n",
    "            draw = random.random()\n",
    "            click = ((explore_upc in wants_to_purchase and draw < 0.8) or\n",
    "                     (explore_upc in might_purchase and draw < 0.5) or\n",
    "                     draw < 0.01)\n",
    "            if click:\n",
    "                print(f\"Search {i} resulted in a click on {explore_upc}\")\n",
    "            new_session.loc[new_session[\"rank\"] == explore_on_rank, \"clicked\"] = click\n",
    "            \n",
    "            with_explore_sessions = pandas.concat([with_explore_sessions, new_session])\n",
    "        else:\n",
    "            print(f\"Search {i} no docs\")\n",
    "            \n",
    "    return with_explore_sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 12.10 - Update judgments from new sessions\n",
    "\n",
    "Have we added any new docs that appear to be getting more clicks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search 4 resulted in a click on 74108007469\n",
      "Search 5 resulted in a click on 97360724240\n",
      "Search 6 resulted in a click on 97360724240\n",
      "Search 12 resulted in a click on 97360724240\n",
      "Search 19 resulted in a click on 97360724240\n",
      "Search 20 resulted in a click on 826663114164\n",
      "Search 22 resulted in a click on 97360724240\n",
      "Search 24 resulted in a click on 826663114164\n",
      "Search 25 resulted in a click on 826663114164\n",
      "Search 29 resulted in a click on 97360724240\n",
      "Search 34 resulted in a click on 826663114164\n",
      "Search 36 resulted in a click on 12505525766\n",
      "Search 41 resulted in a click on 97360722345\n",
      "Search 42 resulted in a click on 97360722345\n",
      "Search 44 resulted in a click on 97360724240\n",
      "Search 47 resulted in a click on 97360724240\n",
      "Search 50 resulted in a click on 826663114164\n",
      "Search 56 resulted in a click on 27242813908\n",
      "Search 61 resulted in a click on 97360722345\n",
      "Search 62 resulted in a click on 97360724240\n",
      "Search 71 resulted in a click on 97360722345\n",
      "Search 72 resulted in a click on 97360722345\n",
      "Search 83 resulted in a click on 826663114164\n",
      "Search 89 resulted in a click on 826663114164\n",
      "Search 90 resulted in a click on 97360724240\n",
      "Search 101 resulted in a click on 826663114164\n",
      "Search 103 resulted in a click on 97360722345\n",
      "Search 107 resulted in a click on 97360722345\n",
      "Search 109 resulted in a click on 97360722345\n",
      "Search 110 resulted in a click on 826663114164\n",
      "Search 113 resulted in a click on 97360722345\n",
      "Search 116 resulted in a click on 97360722345\n",
      "Search 117 resulted in a click on 826663114164\n",
      "Search 119 resulted in a click on 97360724240\n",
      "Search 122 resulted in a click on 97360722345\n",
      "Search 129 resulted in a click on 826663114164\n",
      "Search 132 resulted in a click on 97360722345\n",
      "Search 136 resulted in a click on 97360722345\n",
      "Search 138 resulted in a click on 97360724240\n",
      "Search 139 resulted in a click on 826663114164\n",
      "Search 142 resulted in a click on 97360722345\n",
      "Search 144 resulted in a click on 97360722345\n",
      "Search 146 resulted in a click on 826663114164\n",
      "Search 152 resulted in a click on 27242815414\n",
      "Search 153 resulted in a click on 97360724240\n",
      "Search 158 resulted in a click on 97360724240\n",
      "Search 165 resulted in a click on 826663114164\n",
      "Search 175 resulted in a click on 826663114164\n",
      "Search 176 resulted in a click on 826663114164\n",
      "Search 180 resulted in a click on 97360722345\n",
      "Search 184 resulted in a click on 97360724240\n",
      "Search 188 resulted in a click on 826663114164\n",
      "Search 189 resulted in a click on 97360722345\n",
      "Search 192 resulted in a click on 97360722345\n",
      "Search 193 resulted in a click on 826663114164\n",
      "Search 197 resulted in a click on 97360722345\n",
      "Search 200 resulted in a click on 826663114164\n",
      "Search 208 resulted in a click on 826663114164\n",
      "Search 217 resulted in a click on 826663114164\n",
      "Search 219 resulted in a click on 97360724240\n",
      "Search 223 resulted in a click on 97360722345\n",
      "Search 228 resulted in a click on 97360724240\n",
      "Search 229 resulted in a click on 97360724240\n",
      "Search 232 resulted in a click on 826663114164\n",
      "Search 238 resulted in a click on 826663114164\n",
      "Search 240 resulted in a click on 826663114164\n",
      "Search 244 resulted in a click on 97360724240\n",
      "Search 249 resulted in a click on 97360724240\n",
      "Search 250 resulted in a click on 97360722345\n",
      "Search 253 resulted in a click on 97360722345\n",
      "Search 255 resulted in a click on 97360724240\n",
      "Search 261 resulted in a click on 826663114164\n",
      "Search 272 resulted in a click on 826663114164\n",
      "Search 275 resulted in a click on 97360722345\n",
      "Search 278 resulted in a click on 97360724240\n",
      "Search 285 resulted in a click on 826663114164\n",
      "Search 287 resulted in a click on 826663114164\n",
      "Search 288 resulted in a click on 826663114164\n",
      "Search 290 resulted in a click on 826663114164\n",
      "Search 292 resulted in a click on 97360724240\n",
      "Search 293 resulted in a click on 97360722345\n",
      "Search 294 resulted in a click on 97360724240\n",
      "Search 307 resulted in a click on 826663114164\n",
      "Search 308 resulted in a click on 97360724240\n",
      "Search 310 resulted in a click on 826663114164\n",
      "Search 313 resulted in a click on 97360722345\n",
      "Search 317 resulted in a click on 826663114164\n",
      "Search 318 resulted in a click on 826663114164\n",
      "Search 320 resulted in a click on 97360722345\n",
      "Search 321 resulted in a click on 97360722345\n",
      "Search 322 resulted in a click on 97360724240\n",
      "Search 327 resulted in a click on 97360722345\n",
      "Search 329 resulted in a click on 826663114164\n",
      "Search 332 resulted in a click on 97360722345\n",
      "Search 336 resulted in a click on 826663114164\n",
      "Search 337 resulted in a click on 97360724240\n",
      "Search 342 resulted in a click on 826663114164\n",
      "Search 345 resulted in a click on 826663114164\n",
      "Search 350 resulted in a click on 97360722345\n",
      "Search 356 resulted in a click on 826663114164\n",
      "Search 363 resulted in a click on 97360724240\n",
      "Search 365 resulted in a click on 97360722345\n",
      "Search 367 resulted in a click on 826663114164\n",
      "Search 370 resulted in a click on 826663114164\n",
      "Search 371 resulted in a click on 826663114164\n",
      "Search 372 resulted in a click on 826663114164\n",
      "Search 375 resulted in a click on 97360722345\n",
      "Search 388 resulted in a click on 826663114164\n",
      "Search 389 resulted in a click on 97360722345\n",
      "Search 390 resulted in a click on 826663114164\n",
      "Search 394 resulted in a click on 97360724240\n",
      "Search 397 resulted in a click on 826663114164\n",
      "Search 398 resulted in a click on 97360722345\n",
      "Search 410 resulted in a click on 826663114164\n",
      "Search 424 resulted in a click on 97360722345\n",
      "Search 425 resulted in a click on 826663114164\n",
      "Search 426 resulted in a click on 97360722345\n",
      "Search 441 resulted in a click on 826663114164\n",
      "Search 446 resulted in a click on 826663114164\n",
      "Search 450 resulted in a click on 97360724240\n",
      "Search 459 resulted in a click on 97360724240\n",
      "Search 470 resulted in a click on 97360724240\n",
      "Search 471 resulted in a click on 97360722345\n",
      "Search 473 resulted in a click on 97360722345\n",
      "Search 478 resulted in a click on 826663114164\n",
      "Search 481 resulted in a click on 97360722345\n",
      "Search 482 resulted in a click on 826663114164\n",
      "Search 483 resulted in a click on 97360724240\n",
      "Search 486 resulted in a click on 826663114164\n",
      "Search 492 resulted in a click on 97360722345\n",
      "Search 493 resulted in a click on 97360724240\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clicked</th>\n",
       "      <th>examined</th>\n",
       "      <th>grade</th>\n",
       "      <th>beta_grade</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>826663114164</th>\n",
       "      <td>53</td>\n",
       "      <td>57</td>\n",
       "      <td>0.929825</td>\n",
       "      <td>0.820896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97360722345</th>\n",
       "      <td>40</td>\n",
       "      <td>45</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.763636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97360724240</th>\n",
       "      <td>34</td>\n",
       "      <td>39</td>\n",
       "      <td>0.871795</td>\n",
       "      <td>0.734694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97363455349</th>\n",
       "      <td>731</td>\n",
       "      <td>2115</td>\n",
       "      <td>0.345626</td>\n",
       "      <td>0.344941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97361312804</th>\n",
       "      <td>726</td>\n",
       "      <td>2110</td>\n",
       "      <td>0.344076</td>\n",
       "      <td>0.343396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97363560449</th>\n",
       "      <td>733</td>\n",
       "      <td>2133</td>\n",
       "      <td>0.343647</td>\n",
       "      <td>0.342977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97361312743</th>\n",
       "      <td>708</td>\n",
       "      <td>2074</td>\n",
       "      <td>0.341369</td>\n",
       "      <td>0.340691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97363532149</th>\n",
       "      <td>692</td>\n",
       "      <td>2099</td>\n",
       "      <td>0.329681</td>\n",
       "      <td>0.329066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97361372389</th>\n",
       "      <td>673</td>\n",
       "      <td>2095</td>\n",
       "      <td>0.321241</td>\n",
       "      <td>0.320665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12505525766</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27242815414</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74108007469</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27242813908</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.085714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400192926087</th>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47875842328</th>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803238004525</th>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27242799127</th>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879862003517</th>\n",
       "      <td>37</td>\n",
       "      <td>1865</td>\n",
       "      <td>0.019839</td>\n",
       "      <td>0.020800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93624995012</th>\n",
       "      <td>36</td>\n",
       "      <td>1830</td>\n",
       "      <td>0.019672</td>\n",
       "      <td>0.020652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47875842328</th>\n",
       "      <td>32</td>\n",
       "      <td>1808</td>\n",
       "      <td>0.017699</td>\n",
       "      <td>0.018702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708056579746</th>\n",
       "      <td>29</td>\n",
       "      <td>1817</td>\n",
       "      <td>0.015960</td>\n",
       "      <td>0.016968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47875332911</th>\n",
       "      <td>27</td>\n",
       "      <td>1788</td>\n",
       "      <td>0.015101</td>\n",
       "      <td>0.016129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879862003524</th>\n",
       "      <td>25</td>\n",
       "      <td>1831</td>\n",
       "      <td>0.013654</td>\n",
       "      <td>0.014666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47875819733</th>\n",
       "      <td>25</td>\n",
       "      <td>1834</td>\n",
       "      <td>0.013631</td>\n",
       "      <td>0.014642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708056579739</th>\n",
       "      <td>23</td>\n",
       "      <td>1820</td>\n",
       "      <td>0.012637</td>\n",
       "      <td>0.013661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93624974918</th>\n",
       "      <td>20</td>\n",
       "      <td>1786</td>\n",
       "      <td>0.011198</td>\n",
       "      <td>0.012249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47875839090</th>\n",
       "      <td>17</td>\n",
       "      <td>1820</td>\n",
       "      <td>0.009341</td>\n",
       "      <td>0.010383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              clicked  examined     grade  beta_grade\n",
       "doc_id                                               \n",
       "826663114164       53        57  0.929825    0.820896\n",
       "97360722345        40        45  0.888889    0.763636\n",
       "97360724240        34        39  0.871795    0.734694\n",
       "97363455349       731      2115  0.345626    0.344941\n",
       "97361312804       726      2110  0.344076    0.343396\n",
       "97363560449       733      2133  0.343647    0.342977\n",
       "97361312743       708      2074  0.341369    0.340691\n",
       "97363532149       692      2099  0.329681    0.329066\n",
       "97361372389       673      2095  0.321241    0.320665\n",
       "12505525766         1        17  0.058824    0.111111\n",
       "27242815414         1        17  0.058824    0.111111\n",
       "74108007469         1        23  0.043478    0.090909\n",
       "27242813908         1        25  0.040000    0.085714\n",
       "400192926087        0        20  0.000000    0.066667\n",
       "47875842328         0        24  0.000000    0.058824\n",
       "803238004525        0        24  0.000000    0.058824\n",
       "27242799127         0        27  0.000000    0.054054\n",
       "879862003517       37      1865  0.019839    0.020800\n",
       "93624995012        36      1830  0.019672    0.020652\n",
       "47875842328        32      1808  0.017699    0.018702\n",
       "708056579746       29      1817  0.015960    0.016968\n",
       "47875332911        27      1788  0.015101    0.016129\n",
       "879862003524       25      1831  0.013654    0.014666\n",
       "47875819733        25      1834  0.013631    0.014642\n",
       "708056579739       23      1820  0.012637    0.013661\n",
       "93624974918        20      1786  0.011198    0.012249\n",
       "47875839090        17      1820  0.009341    0.010383"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random.seed(1234)\n",
    "\n",
    "query = \"transformers dvd\"\n",
    "sessions_with_exploration = generate_simulated_exploration_sessions(\n",
    "    query, sessions, logged_transformers_judgments, explore_features)\n",
    "training_data_with_exploration = \\\n",
    "    generate_training_data(sessions_with_exploration)\n",
    "display(training_data_with_exploration.loc[\"transformers dvd\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 12.11 - Rebuild model using updated judgments\n",
    "\n",
    "After showing the new document to users, we can rebuild the model using judgments that cover this feature blindspot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dryer': 0.12737002598513025,\n",
       " 'blue ray': 0.08461538461538462,\n",
       " 'headphones': 0.12110565745285455,\n",
       " 'dark of moon': 0.1492224251599605,\n",
       " 'transformers dvd': 0.26602432266662907}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random.seed(1234)\n",
    "\n",
    "promotion_feature_set = [\n",
    "    ltr.generate_fuzzy_query_feature(feature_name=\"name_fuzzy\",\n",
    "                                     field_name=\"name\"),\n",
    "    ltr.generate_bigram_query_feature(feature_name=\"name_bigram\",\n",
    "                                      field_name=\"name\"),\n",
    "    ltr.generate_bigram_query_feature(feature_name=\"short_description_bigram\",\n",
    "                                      field_name=\"short_description\"),\n",
    "    ltr.generate_query_feature(feature_name=\"has_promotion\",\n",
    "                               field_name=\"has_promotion\",\n",
    "                               value=\"true\",\n",
    "                               constant_score=True)]\n",
    "\n",
    "evaluation = train_and_evaluate_model(sessions_with_exploration,\n",
    "                                      \"ltr_model_variant_3\",\n",
    "                                      promotion_feature_set)\n",
    "display(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'name_fuzzy',\n",
       "  'class': 'org.apache.solr.ltr.feature.SolrFeature',\n",
       "  'params': {'q': 'name_ngram:(${keywords})'},\n",
       "  'store': 'ltr_model_variant_3'},\n",
       " {'name': 'name_bigram',\n",
       "  'class': 'org.apache.solr.ltr.feature.SolrFeature',\n",
       "  'params': {'q': '{!edismax qf=name pf2=name}(${keywords})'},\n",
       "  'store': 'ltr_model_variant_3'},\n",
       " {'name': 'short_description_bigram',\n",
       "  'class': 'org.apache.solr.ltr.feature.SolrFeature',\n",
       "  'params': {'q': '{!edismax qf=short_description pf2=short_description}(${keywords})'},\n",
       "  'store': 'ltr_model_variant_3'},\n",
       " {'name': 'has_promotion',\n",
       "  'class': 'org.apache.solr.ltr.feature.SolrFeature',\n",
       "  'params': {'q': 'has_promotion:true^=1'},\n",
       "  'store': 'ltr_model_variant_3'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(promotion_feature_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 12.12 - Searching with the latest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Transformers/Transformers: Revenge of the Fallen: Two-Movie Mega Collection [2 Discs] - Widescreen - DVD',\n",
       " 'Transformers: Revenge of the Fallen - Widescreen - DVD',\n",
       " 'Transformers: Dark of the Moon - Original Soundtrack - CD',\n",
       " 'Transformers: The Complete Series [25th Anniversary Matrix of Leadership Edition] [16 Discs] - DVD',\n",
       " 'Transformers: Dark of the Moon Stealth Force Edition - Nintendo Wii']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = ltr.search_with_model(\"ltr_model_variant_3\",\n",
    "                                query=\"transformers dvd\",\n",
    "                                rerank_query=\"transformers dvd\",\n",
    "                                limit=5)[\"docs\"]\n",
    "display([doc[\"name\"] for doc in results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 12.13 - Rerun A/B test on new `promotion` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ltr_model_variant_1': 21, 'ltr_model_variant_3': 145}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Takes ~25 minutes\n",
    "random.seed(1234)\n",
    "\n",
    "results = simulate_user_a_b_test(query=\"transformers dvd\",\n",
    "                                 model_a=\"ltr_model_variant_1\",\n",
    "                                 model_b=\"ltr_model_variant_3\",\n",
    "                                 number_of_users=1000)\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 12.14 - Fully Automated LTR Loop\n",
    "\n",
    "These lines expand Listing 12.13 from the book (the book content is a truncated form of what's below). You could put this in a loop and constantly try new features to try to get closer at a generalized ranking solution of what users actually want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1234)\n",
    "ltr.delete_feature_store(\"aips_feature_store\")\n",
    "\n",
    "def get_exploit_features():\n",
    "    return [\n",
    "        ltr.generate_fuzzy_query_feature(\"name_fuzzy\", \"name\"),\n",
    "        ltr.generate_query_feature(\"long_description_bm25\", \"long_description\"),\n",
    "        ltr.generate_query_feature(\"short_description_match\", \"short_description\", True)]\n",
    "\n",
    "def gather_latest_sessions(query, sessions, model_name, features):\n",
    "    \"\"\"For the sake of the examples, returns a static list of session data.\n",
    "       In a production environment, this would the most up to date user interactions\"\"\"\n",
    "    training_data = generate_training_data(sessions)\n",
    "    logged_judgments = generate_logged_judgments(training_data, features, model_name)\n",
    "    latest_sessions = generate_simulated_exploration_sessions(query,\n",
    "                                                              sessions,\n",
    "                                                              logged_judgments,\n",
    "                                                              features)\n",
    "    return latest_sessions\n",
    "\n",
    "def is_improvement(evaluation1, evaluation2):\n",
    "    #Model comparison is stubbed out\n",
    "    return True\n",
    "    \n",
    "def wait_for_more_sessions(t):\n",
    "    time.sleep(t)\n",
    "\n",
    "def ltr_retraining_loop(latest_sessions, iterations=sys.maxsize,\n",
    "                        retrain_frequency=60 * 60 * 24):\n",
    "    for i in range(0, iterations):\n",
    "        training_data = generate_training_data(latest_sessions)\n",
    "        train, test = split_training_data(training_data)\n",
    "        if i == 0:\n",
    "            exploit_features = get_exploit_features()\n",
    "            train_and_upload_model(train,\n",
    "                                   \"exploit\",\n",
    "                                   exploit_features)\n",
    "        else:\n",
    "            previous_explore_model_name = f\"explore_variant_{i-1}\"\n",
    "            exploit_model_evaluation = evaluate_model(test, \"exploit\", training_data)\n",
    "            explore_model_evaluation = evaluate_model(test, previous_explore_model_name, training_data)\n",
    "            print(f\"Exploit evaluation: {exploit_model_evaluation}\")\n",
    "            print(f\"Explore evaluation: {explore_model_evaluation}\")\n",
    "            if is_improvement(explore_model_evaluation, exploit_model_evaluation):\n",
    "                print(\"Promoting previous explore model\")\n",
    "                train_and_upload_model(train,\n",
    "                                      \"exploit\",\n",
    "                                       explore_features)\n",
    "                \n",
    "        explore_features = get_latest_explore_features()\n",
    "        train_and_upload_model(train,\n",
    "                               f\"explore_variant_{i}\",\n",
    "                               explore_features)\n",
    "        \n",
    "        wait_for_more_sessions(retrain_frequency)\n",
    "        latest_sessions = gather_latest_sessions(\"transformers dvd\", latest_sessions,\n",
    "                                                 f\"explore_variant_{i}\", explore_features)\n",
    "        \n",
    "# This is an example of production LTR, however this functionality is mostly simulated and does not produce results.\n",
    "# Set iterations above 0 to actually run. Currently set to 0 to allow tests to pass\n",
    "\n",
    "ltr_retraining_loop(sessions, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up next: [Chapter 13: Semantic Search with Dense Vectors](../ch13/1.setting-up-the-outdoors-dataset.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
