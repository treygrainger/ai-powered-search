{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B Testing Simulation to Active Learning\n",
    "\n",
    "In this notebook, users have a hidden preference for a single query. We use this to explore A/B testing to see whether a given LTR model actually gives the users what they want.\n",
    "\n",
    "Then we ask, much like in real life, how can we learn what the user _actually_ wants? We employe active learning to try to escape the 'echo chamber' of presentation bias we learned about at the end of chapter 11. After all users can't click on results that never show up in their search results!\n",
    "\n",
    "## ðŸš¨ We're putting it all together in this chapter\n",
    "\n",
    "As this chapter puts together everything from chapters 10 and 11, much of the setup code below wraps up a lot of chapter 11 and 10 into a 'single function' so we can very easily run through the steps in 'one liners'\n",
    "\n",
    "### Getting training data (Ch 11)\n",
    "\n",
    "Chapter 11 is all about turning raw clickstream data into search training data (aka judgments). This involves overcoming biases in how users percieve search. But here we put that in one function call `calculate_sdbn`.\n",
    "\n",
    "### Train a model (Ch 10)\n",
    "\n",
    "Chapter 10 is about training an LTR model, including interacting with Solr to extract features, how a ranking model works, how to train a model, and how to perform a good test/train split for search. But here we similarly wrap that up into a handful of function calls, `split_training_data`, and `evaluate_model`.\n",
    "\n",
    "*long story short, if you see a reference to chapter 10 and 11, it's probably omited from chapter 12* - don't expect it to be covered in chapter 12 extensively.\n",
    "\n",
    "\n",
    "## Setup - gather some sessions (omitted)\n",
    "\n",
    "To get started, we first load a set of simulated search sessions for all queries. \n",
    "\n",
    "Much of this setup is omitted from the chapter. This first part is just loading and synthesizing a bunch of clickstream sessions, like we used in chapter 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../..')\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "from aips import *\n",
    "import random; random.seed(0)\n",
    "\n",
    "engine = get_engine()\n",
    "products_collection = engine.get_collection(\"products\")\n",
    "ltr = get_ltr_engine(products_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sess_id</th>\n",
       "      <th>query</th>\n",
       "      <th>rank</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>clicked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50002</td>\n",
       "      <td>blue ray</td>\n",
       "      <td>1.0</td>\n",
       "      <td>827396513927</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50002</td>\n",
       "      <td>blue ray</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24543672067</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50002</td>\n",
       "      <td>blue ray</td>\n",
       "      <td>3.0</td>\n",
       "      <td>719192580374</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50002</td>\n",
       "      <td>blue ray</td>\n",
       "      <td>4.0</td>\n",
       "      <td>885170033412</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50002</td>\n",
       "      <td>blue ray</td>\n",
       "      <td>5.0</td>\n",
       "      <td>58231300826</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74995</th>\n",
       "      <td>5001</td>\n",
       "      <td>transformers dark of the moon</td>\n",
       "      <td>10.0</td>\n",
       "      <td>47875841369</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74996</th>\n",
       "      <td>5001</td>\n",
       "      <td>transformers dark of the moon</td>\n",
       "      <td>11.0</td>\n",
       "      <td>97363560449</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74997</th>\n",
       "      <td>5001</td>\n",
       "      <td>transformers dark of the moon</td>\n",
       "      <td>12.0</td>\n",
       "      <td>93624956037</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74998</th>\n",
       "      <td>5001</td>\n",
       "      <td>transformers dark of the moon</td>\n",
       "      <td>13.0</td>\n",
       "      <td>97363532149</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74999</th>\n",
       "      <td>5001</td>\n",
       "      <td>transformers dark of the moon</td>\n",
       "      <td>14.0</td>\n",
       "      <td>400192926087</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1710000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sess_id                          query  rank        doc_id  clicked\n",
       "1        50002                       blue ray   1.0  827396513927    False\n",
       "2        50002                       blue ray   2.0   24543672067    False\n",
       "3        50002                       blue ray   3.0  719192580374    False\n",
       "4        50002                       blue ray   4.0  885170033412     True\n",
       "5        50002                       blue ray   5.0   58231300826    False\n",
       "...        ...                            ...   ...           ...      ...\n",
       "74995     5001  transformers dark of the moon  10.0   47875841369    False\n",
       "74996     5001  transformers dark of the moon  11.0   97363560449    False\n",
       "74997     5001  transformers dark of the moon  12.0   93624956037    False\n",
       "74998     5001  transformers dark of the moon  13.0   97363532149    False\n",
       "74999     5001  transformers dark of the moon  14.0  400192926087    False\n",
       "\n",
       "[1710000 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signals_upcs_to_omit = [600603132872, 600603125065, 600603141003, 600603139758,\n",
    "                        600603133237, 600603123061, 600603140631, 600603124570,\n",
    "                        600603132827, 600603135101]\n",
    "\n",
    "def all_sessions():\n",
    "    sessions = pandas.concat([pandas.read_csv(f, compression='gzip')\n",
    "                          for f in glob.glob('retrotech/sessions/*_sessions.gz')])\n",
    "    sessions = sessions.sort_values(['query', 'sess_id', 'rank'])\n",
    "    sessions = sessions.rename(columns={'clicked_doc_id': 'doc_id'})\n",
    "    return sessions[~sessions[\"doc_id\"].isin(signals_upcs_to_omit)]\n",
    "    \n",
    "sessions = all_sessions()\n",
    "sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['blue ray', 'bluray', 'dryer', 'headphones', 'ipad', 'iphone',\n",
       "       'kindle', 'lcd tv', 'macbook', 'nook', 'star trek', 'star wars',\n",
       "       'transformers dark of the moon'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sessions[\"query\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Part 2 - Add some more query sessions (omitted)\n",
    "\n",
    "Here we duplicate the simulated queries from above, but we flip a handful of the clicks. This just fills out our data a bit more, gives a bit more data to work with.\n",
    "\n",
    "## Setup Part 3 - Our test query, `transformers dvd`, with hidden, 'true' preferences\n",
    "\n",
    "We add a new query to our set of queries `transformers dvd` and we note the users' hidden preferences in the variables `desired_movies` as well as what they consider mediocre `meh_transformers_movies` and not at all relevant `irrelevant_transformers_products`. Each holds the UPC of the associated product.\n",
    "\n",
    "This simulates biased sessions in the data, as if the user never actually sees (and hence never clicks) their actual desired item. If the users desired results are shown, those results get a higher probability of click. Otherwise there is a lower probability of clicks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        sess_id     query  rank        doc_id  clicked\n",
      "1         50002  blue ray   1.0  827396513927    False\n",
      "2         50002  blue ray   2.0   24543672067    False\n",
      "3         50002  blue ray   3.0  719192580374    False\n",
      "4         50002  blue ray   4.0  885170033412     True\n",
      "5         50002  blue ray   5.0   58231300826    False\n",
      "...         ...       ...   ...           ...      ...\n",
      "149994    55001   blueray  24.0   36725617605    False\n",
      "149995    55001   blueray  25.0   22265004517    False\n",
      "149996    55001   blueray  26.0  885170038875    False\n",
      "149997    55001   blueray  27.0  786936817232    False\n",
      "149999    55001   blueray  29.0   27242815414    False\n",
      "\n",
      "[3085000 rows x 5 columns]\n",
      "Click num 4158\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sess_id</th>\n",
       "      <th>query</th>\n",
       "      <th>rank</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>clicked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50002</td>\n",
       "      <td>blue ray</td>\n",
       "      <td>1.0</td>\n",
       "      <td>827396513927</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50002</td>\n",
       "      <td>blue ray</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24543672067</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50002</td>\n",
       "      <td>blue ray</td>\n",
       "      <td>3.0</td>\n",
       "      <td>719192580374</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50002</td>\n",
       "      <td>blue ray</td>\n",
       "      <td>4.0</td>\n",
       "      <td>885170033412</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50002</td>\n",
       "      <td>blue ray</td>\n",
       "      <td>5.0</td>\n",
       "      <td>58231300826</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79995</th>\n",
       "      <td>65000</td>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>11.0</td>\n",
       "      <td>47875842328</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79996</th>\n",
       "      <td>65000</td>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>12.0</td>\n",
       "      <td>879862003517</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79997</th>\n",
       "      <td>65000</td>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>13.0</td>\n",
       "      <td>97361372389</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79998</th>\n",
       "      <td>65000</td>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>14.0</td>\n",
       "      <td>93624995012</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79999</th>\n",
       "      <td>65000</td>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>15.0</td>\n",
       "      <td>47875839090</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3165000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sess_id             query  rank        doc_id  clicked\n",
       "1        50002          blue ray   1.0  827396513927    False\n",
       "2        50002          blue ray   2.0   24543672067    False\n",
       "3        50002          blue ray   3.0  719192580374    False\n",
       "4        50002          blue ray   4.0  885170033412     True\n",
       "5        50002          blue ray   5.0   58231300826    False\n",
       "...        ...               ...   ...           ...      ...\n",
       "79995    65000  transformers dvd  11.0   47875842328    False\n",
       "79996    65000  transformers dvd  12.0  879862003517    False\n",
       "79997    65000  transformers dvd  13.0   97361372389    False\n",
       "79998    65000  transformers dvd  14.0   93624995012    False\n",
       "79999    65000  transformers dvd  15.0   47875839090    False\n",
       "\n",
       "[3165000 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(0)\n",
    "numpy.random.seed(0)\n",
    "\n",
    "def copy_query_sessions(sessions, src_query, dest_query, flip=False):\n",
    "    new_sessions = sessions[sessions[\"query\"] == src_query].copy()  \n",
    "    new_sessions[\"draw\"] = numpy.random.rand(len(new_sessions), 1)\n",
    "    new_sessions.loc[new_sessions[\"clicked\"] & (new_sessions[\"draw\"] < 0.04), \"clicked\"] = False\n",
    "    new_sessions[\"query\"] = dest_query\n",
    "    return pandas.concat([sessions, new_sessions.drop(\"draw\", axis=1)])\n",
    "\n",
    "sessions = all_sessions()\n",
    "sessions = copy_query_sessions(sessions, \"transformers dark of the moon\", \"transformers dark of moon\")\n",
    "sessions = copy_query_sessions(sessions, \"transformers dark of the moon\", \"dark of moon\")\n",
    "sessions = copy_query_sessions(sessions, \"transformers dark of the moon\", \"dark of the moon\")\n",
    "sessions = copy_query_sessions(sessions, \"headphones\", \"head phones\")\n",
    "sessions = copy_query_sessions(sessions, \"lcd tv\", \"lcd television\")\n",
    "sessions = copy_query_sessions(sessions, \"lcd tv\", \"television, lcd\")\n",
    "sessions = copy_query_sessions(sessions, \"macbook\", \"apple laptop\")\n",
    "sessions = copy_query_sessions(sessions, \"iphone\", \"apple iphone\")\n",
    "sessions = copy_query_sessions(sessions, \"kindle\", \"amazon kindle\")\n",
    "sessions = copy_query_sessions(sessions, \"kindle\", \"amazon ereader\")\n",
    "sessions = copy_query_sessions(sessions, \"blue ray\", \"blueray\")\n",
    "\n",
    "print(sessions)\n",
    "\n",
    "next_sess_id = sessions[\"sess_id\"].max()\n",
    "\n",
    "# For some reason, the sessions only capture examines on the 'dubbed' transformers movies\n",
    "# ie the Japanese shows brought to an English-speaking market. But we'll see this is not what the \n",
    "# user wants (ie presentation bias). These are 'meh' mildly interesting. There are also many many\n",
    "# completely irrelevant movies.\n",
    "\n",
    "# What the user wants, but never visible! Never gets clicked!\n",
    "\n",
    "# These are the widescreen transformers dvds of the hollywood movies\n",
    "desired_transformers_movies = [\"97360724240\", \"97360722345\", \"826663114164\"]\n",
    "# Other transformer movies\n",
    "meh_transformers_movies = [\"97363455349\", \"97361312743\", \"97361372389\",\n",
    "                           \"97361312804\", \"97363532149\", \"97363560449\"]\n",
    "# Bunch of random merchandise\n",
    "irrelevant_transformers_products = [\"708056579739\", \"93624995012\", \"47875819733\", \"47875839090\", \"708056579746\",\n",
    "                                    \"47875332911\", \"47875842328\", \"879862003524\", \"879862003517\", \"93624974918\"] \n",
    "\n",
    "\n",
    "displayed_transformer_products = meh_transformers_movies + irrelevant_transformers_products\n",
    "new_sessions = []\n",
    "click = 0\n",
    "for i in range(0, 5000):\n",
    "    random.shuffle(displayed_transformer_products)\n",
    "\n",
    "    # shuffle each session\n",
    "    for rank, upc in enumerate(displayed_transformer_products):\n",
    "        draw = random.random()        \n",
    "        clicked = ((upc in meh_transformers_movies and draw < 0.13) or\n",
    "                   (upc in irrelevant_transformers_products and draw < 0.005))\n",
    "        click += (1 if clicked else 0)\n",
    "        new_sessions.append({\"sess_id\": next_sess_id + i, \n",
    "                             \"query\": \"transformers dvd\", \n",
    "                             \"rank\": rank,\n",
    "                             \"clicked\": clicked,\n",
    "                             \"doc_id\": upc})\n",
    "\n",
    "print(\"Click num \" + str(click))\n",
    "sessions = pandas.concat([sessions, pandas.DataFrame(new_sessions)])\n",
    "sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['blue ray', 'bluray', 'dryer', 'headphones', 'ipad', 'iphone',\n",
       "       'kindle', 'lcd tv', 'macbook', 'nook', 'star trek', 'star wars',\n",
       "       'transformers dark of the moon', 'transformers dark of moon',\n",
       "       'dark of moon', 'dark of the moon', 'head phones',\n",
       "       'lcd television', 'television, lcd', 'apple laptop',\n",
       "       'apple iphone', 'amazon kindle', 'amazon ereader', 'blueray',\n",
       "       'transformers dvd'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sessions[\"query\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup 4 - chapter 11 In One Function (omitted) \n",
    "\n",
    "Wrapping up Chapter 11 in a single function `generate_training_data`. \n",
    "\n",
    "This function computes a relevance grade out of raw clickstream data. Recall that the SDBN (Simplified Dynamic Bayesian Network) click model we learned about in chapter 11 helps overcome position bias. We also use a beta prior so that a single click doesn't count as much as an observation with hundreds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load -s calculate_ctr,calculate_average_rank,caclulate_examine_probability,calculate_clicked_examined,calculate_grade,calculate_prior,calculate_sdbn ../ltr/sdbn_functions.py\n",
    "def calculate_ctr(sessions):\n",
    "    click_counts = sessions.groupby(\"doc_id\")[\"clicked\"].sum()\n",
    "    sess_counts = sessions.groupby(\"doc_id\")[\"sess_id\"].nunique()\n",
    "    ctrs = click_counts / sess_counts\n",
    "    return ctrs.sort_values(ascending=False)\n",
    "\n",
    "def calculate_average_rank(sessions):\n",
    "    avg_rank = sessions.groupby(\"doc_id\")[\"rank\"].mean()\n",
    "    return avg_rank.sort_values(ascending=True)\n",
    "\n",
    "def caclulate_examine_probability(sessions):\n",
    "    last_click_per_session = sessions.groupby([\"clicked\", \"sess_id\"])[\"rank\"].max()[True]\n",
    "    sessions[\"last_click_rank\"] = last_click_per_session\n",
    "    sessions[\"examined\"] = sessions[\"rank\"] <= sessions[\"last_click_rank\"]\n",
    "    return sessions\n",
    "\n",
    "def calculate_clicked_examined(sessions):\n",
    "    sessions = caclulate_examine_probability(sessions)\n",
    "    return sessions[sessions[\"examined\"]] \\\n",
    "        .groupby(\"doc_id\")[[\"clicked\", \"examined\"]].sum()\n",
    "\n",
    "def calculate_grade(sessions):\n",
    "    sessions = calculate_clicked_examined(sessions)\n",
    "    sessions[\"grade\"] = sessions[\"clicked\"] / sessions[\"examined\"]\n",
    "    return sessions.sort_values(\"grade\", ascending=False)\n",
    "\n",
    "def calculate_prior(sessions, prior_grade, prior_weight):\n",
    "    sessions = calculate_grade(sessions)\n",
    "    sessions[\"prior_a\"] = prior_grade * prior_weight\n",
    "    sessions[\"prior_b\"] = (1 - prior_grade) * prior_weight\n",
    "    return sessions\n",
    "\n",
    "def calculate_sdbn(sessions, prior_grade, prior_weight):\n",
    "    sessions = calculate_prior(sessions, prior_grade, prior_weight)\n",
    "    sessions[\"posterior_a\"] = (sessions[\"prior_a\"] + \n",
    "                               sessions[\"clicked\"])\n",
    "    sessions[\"posterior_b\"] = (sessions[\"prior_b\"] + \n",
    "                               sessions[\"examined\"] - sessions[\"clicked\"])\n",
    "    sessions[\"beta_grade\"] = (sessions[\"posterior_a\"] /\n",
    "      (sessions[\"posterior_a\"] + sessions[\"posterior_b\"]))\n",
    "    return sessions.sort_values(\"beta_grade\", ascending=False)\n",
    "\n",
    "def generate_training_data(sessions, prior_grade=0.2, prior_weight=10):\n",
    "    all_sdbn = pandas.DataFrame()\n",
    "    for query in sessions[\"query\"].unique():        \n",
    "        query_sessions = sessions[sessions[\"query\"] == query].copy().set_index(\"sess_id\")\n",
    "        query_sessions = calculate_sdbn(query_sessions, prior_grade, prior_weight)\n",
    "        query_sessions[\"query\"] = query\n",
    "        all_sdbn = pandas.concat([all_sdbn, query_sessions])\n",
    "    return all_sdbn[[\"query\", \"clicked\", \"examined\", \"grade\", \"beta_grade\"]].reset_index().set_index([\"query\", \"doc_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 10 Functions (omitted from book)\n",
    "\n",
    "Now with the chapter 11 setup out of the way, we'll need to give Chapter 10's code a similar treatment, wrapping that LTR system into a black box.\n",
    "\n",
    "All of the following are support functions for the chapter:\n",
    "\n",
    "1. Convert the sdbn dataframe into individual `Judgment` objects needed for training the model from chapter 10\n",
    "2. Pairwise transformation of the data\n",
    "3. Normalization of the data\n",
    "4. Training the model\n",
    "5. Uploading the model to Solr\n",
    "\n",
    "All of these steps are covered in Chapter 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy\n",
    "from ltr.judgments import judgments_to_nparray\n",
    "from sklearn import svm\n",
    "import json\n",
    "from itertools import groupby\n",
    "from ltr.log import FeatureLogger\n",
    "from itertools import groupby\n",
    "from ltr.judgments import judgments_writer\n",
    "\n",
    "from ltr.judgments import Judgment\n",
    "\n",
    "def as_judgments(training_data):\n",
    "    \"\"\"Turn pandas dataframe into ltr judgments objects.\"\"\"        \n",
    "    qid_map = {}\n",
    "    judgments = []\n",
    "    next_qid = 0\n",
    "    for datum in training_data.reset_index().to_dict(orient=\"records\"):       \n",
    "        if datum[\"query\"] not in qid_map:\n",
    "            qid_map[datum[\"query\"]] = next_qid\n",
    "            next_qid += 1\n",
    "        qid = qid_map[datum[\"query\"]]\n",
    "\n",
    "        judgments.append(Judgment(doc_id=datum[\"doc_id\"],\n",
    "                        keywords=datum[\"query\"],\n",
    "                        qid=qid,\n",
    "                        grade=datum[\"beta_grade\"]))\n",
    "        \n",
    "    return judgments\n",
    "\n",
    "def normalize_features(logged_judgments):\n",
    "    num_features = len(logged_judgments[0].features)\n",
    "    means = [numpy.mean([j.features[i] for j in logged_judgments])\n",
    "             for i in range(0, num_features)]    \n",
    "    \n",
    "    std_devs = [numpy.std([j.features[i] for j in logged_judgments])\n",
    "                for i in range(0, num_features)]\n",
    "    \n",
    "    normed_judgments = copy.deepcopy(logged_judgments)\n",
    "    for j in normed_judgments:\n",
    "        for i, score in enumerate(j.features):\n",
    "            j.features[i] = (score - means[i]) / std_devs[i]\n",
    "\n",
    "    return means, std_devs, normed_judgments\n",
    "\n",
    "def pairwise_transform(normed_judgments):        \n",
    "    predictor_deltas = []\n",
    "    feature_deltas = []\n",
    "    for qid, grouped_judgments in groupby(normed_judgments, key=lambda j: j.qid):\n",
    "        query_judgments = list(grouped_judgments)\n",
    "        for judgment1 in query_judgments:\n",
    "            for judgment2 in query_judgments:\n",
    "                j1_features = numpy.array(judgment1.features)\n",
    "                j2_features = numpy.array(judgment2.features)\n",
    "                \n",
    "                if judgment1.grade > judgment2.grade:\n",
    "                    predictor_deltas.append(1)\n",
    "                    feature_deltas.append(j1_features - j2_features)\n",
    "                elif judgment1.grade < judgment2.grade:\n",
    "                    predictor_deltas.append(-1)\n",
    "                    feature_deltas.append(j1_features - j2_features)\n",
    "\n",
    "    return numpy.array(feature_deltas), numpy.array(predictor_deltas)\n",
    "\n",
    "def write_judgments(judgments, dest=\"retrotech_judgments.txt\"):\n",
    "    with judgments_writer(open(dest, \"wt\")) as writer:\n",
    "        for judgment in judgments:\n",
    "            writer.write(judgment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Also Chapter 10 - Perform a test / train split on the SDBN data (omitted)\n",
    "\n",
    "This function is broken out from the model training. It lets us train a model on one set of data (reusing the chapter 10 training code), reserving test queries for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "\n",
    "def split_training_data(training_data, train_proportion=0.8):\n",
    "    \"\"\"Split queries in training_data into train / test split with `train` proportion going to training set.\"\"\"\n",
    "    queries = training_data.index.get_level_values('query').unique().copy().tolist()\n",
    "    random.shuffle(queries)\n",
    "    num_queries = len(queries)\n",
    "    split_point = floor(num_queries * train_proportion)\n",
    "    \n",
    "    train_queries = queries[:split_point]\n",
    "    test_queries = queries[split_point:]\n",
    "    return training_data.loc[train_queries, :], training_data.loc[test_queries]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 10 - Evaluate the model on the test set (omitted)\n",
    "\n",
    "This function computes the model's performance on a set of test queries. The `test_data` is the control set not used to train the model. We compute the precision of these queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm_model(model_name, features, logged_judgments):\n",
    "    means, std_devs, normed_judgments = normalize_features(logged_judgments)\n",
    "    feature_deltas, predictor_deltas = pairwise_transform(normed_judgments)\n",
    "\n",
    "    model = svm.LinearSVC(max_iter=10000, verbose=1)\n",
    "    model.fit(feature_deltas, predictor_deltas) \n",
    "\n",
    "    feature_names = [ftr[\"name\"] for ftr in features]\n",
    "    linear_model = ltr.generate_model(model_name, feature_names,\n",
    "                                      means, std_devs, model.coef_[0])\n",
    "\n",
    "    return linear_model\n",
    "\n",
    "def train_and_upload_model(training_data, model_name, features, log=False):\n",
    "    \"\"\"Train a RankSVM model via Solr, store in Solr.\"\"\"\n",
    "    judgments = as_judgments(training_data)\n",
    "    ltr.delete_feature_store(model_name, log=log)\n",
    "    ltr.delete_model(model_name)\n",
    "    ltr.upload_features(features, model_name, log=log)\n",
    "    ftr_logger = FeatureLogger(engine, products_collection, feature_set=model_name,\n",
    "                               id_field=\"upc\")\n",
    "            \n",
    "    for qid, query_judgments in groupby(judgments, key=lambda j: j.qid):\n",
    "        ftr_logger.log_for_qid(judgments=query_judgments, \n",
    "                               qid=qid, log=False)\n",
    "\n",
    "    linear_model = train_svm_model(model_name, features, ftr_logger.logged)\n",
    "    ltr.upload_model(linear_model, log=log)\n",
    "    return linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(test_data, model_name, training_data, limit=10, log=False):\n",
    "    queries = test_data.index.get_level_values(\"query\").unique()\n",
    "    query_results = {}\n",
    "    \n",
    "    for query in queries:\n",
    "        response = ltr.search_with_model(model_name, query=query,\n",
    "                                         limit=limit, rerank_query=query, log=False)\n",
    "    \n",
    "        results = pandas.DataFrame(response[\"docs\"]).reset_index()\n",
    "        judgments = training_data.loc[query, :].copy().reset_index()\n",
    "        judgments[\"doc_id\"] = judgments[\"doc_id\"].astype(str)\n",
    "        if len(results) == 0:\n",
    "            print(f\"No Results for {query}\")\n",
    "            query_results[query] = 0\n",
    "        else:\n",
    "            graded_results = results.merge(judgments, left_on=\"upc\",\n",
    "                                           right_on=\"doc_id\", how=\"left\")\n",
    "            graded_results[[\"clicked\", \"examined\", \"grade\", \"beta_grade\"]] = graded_results[[\"clicked\", \"examined\", \"grade\", \"beta_grade\"]].fillna(0)\n",
    "            graded_results = graded_results.drop(\"doc_id\", axis=1)\n",
    "            if log:\n",
    "                print(graded_results.drop([\"index\", \"rank\", \"manufacturer\", \"short_description\",\n",
    "                                           \"long_description\", \"grade\", \"name\"], axis=1))\n",
    "\n",
    "            query_results[query] = (graded_results[\"beta_grade\"].sum() / limit)\n",
    "    return query_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 12.1 Generating the sdbn training data\n",
    "\n",
    "We kickoff with the data we left off with in chapter 11.\n",
    "\n",
    "In this listing we user our \"chapter 11 in one function\" `generate_training_data` to rebuild training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>clicked</th>\n",
       "      <th>examined</th>\n",
       "      <th>grade</th>\n",
       "      <th>beta_grade</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>query</th>\n",
       "      <th>doc_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">blue ray</th>\n",
       "      <th>27242815414</th>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827396513927</th>\n",
       "      <td>1304</td>\n",
       "      <td>3359</td>\n",
       "      <td>0.388211</td>\n",
       "      <td>0.387652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883929140855</th>\n",
       "      <td>140</td>\n",
       "      <td>506</td>\n",
       "      <td>0.276680</td>\n",
       "      <td>0.275194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885170033412</th>\n",
       "      <td>568</td>\n",
       "      <td>2147</td>\n",
       "      <td>0.264555</td>\n",
       "      <td>0.264256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24543672067</th>\n",
       "      <td>665</td>\n",
       "      <td>2763</td>\n",
       "      <td>0.240680</td>\n",
       "      <td>0.240534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">transformers dvd</th>\n",
       "      <th>47875819733</th>\n",
       "      <td>24</td>\n",
       "      <td>1679</td>\n",
       "      <td>0.014294</td>\n",
       "      <td>0.015394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708056579739</th>\n",
       "      <td>23</td>\n",
       "      <td>1659</td>\n",
       "      <td>0.013864</td>\n",
       "      <td>0.014979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879862003524</th>\n",
       "      <td>23</td>\n",
       "      <td>1685</td>\n",
       "      <td>0.013650</td>\n",
       "      <td>0.014749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93624974918</th>\n",
       "      <td>19</td>\n",
       "      <td>1653</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.012628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47875839090</th>\n",
       "      <td>16</td>\n",
       "      <td>1669</td>\n",
       "      <td>0.009587</td>\n",
       "      <td>0.010721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>626 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               clicked  examined     grade  beta_grade\n",
       "query            doc_id                                               \n",
       "blue ray         27242815414        42        42  1.000000    0.846154\n",
       "                 827396513927     1304      3359  0.388211    0.387652\n",
       "                 883929140855      140       506  0.276680    0.275194\n",
       "                 885170033412      568      2147  0.264555    0.264256\n",
       "                 24543672067       665      2763  0.240680    0.240534\n",
       "...                                ...       ...       ...         ...\n",
       "transformers dvd 47875819733        24      1679  0.014294    0.015394\n",
       "                 708056579739       23      1659  0.013864    0.014979\n",
       "                 879862003524       23      1685  0.013650    0.014749\n",
       "                 93624974918        19      1653  0.011494    0.012628\n",
       "                 47875839090        16      1669  0.009587    0.010721\n",
       "\n",
       "[626 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_data = generate_training_data(sessions,\n",
    "                                       prior_weight=10,\n",
    "                                       prior_grade=0.2)\n",
    "\n",
    "display(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 12.2 - model training\n",
    "\n",
    "We wrap all the important decisions from chapter 10 in a few lines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(sessions, model_name, features, log=False):\n",
    "    training_data = generate_training_data(sessions)\n",
    "    train, test = split_training_data(training_data, 0.8)\n",
    "    train_and_upload_model(train, model_name, features=features, log=False)\n",
    "    evaluation = evaluate_model(test, model_name, training_data, log=log)\n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dryer': 0.03753076750950996,\n",
       " 'blue ray': 0.0,\n",
       " 'headphones': 0.019262295081967213,\n",
       " 'dark of moon': 0.0,\n",
       " 'transformers dvd': 0.0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random.seed(1234)\n",
    "feature_set = [\n",
    "    ltr.generate_query_feature(feature_name=\"long_description_bm25\",\n",
    "                               field_name=\"long_description\"),\n",
    "    ltr.generate_query_feature(feature_name=\"short_description_constant\",\n",
    "                               field_name=\"short_description\",\n",
    "                               constant_score=True)]\n",
    "\n",
    "evaluation = train_and_evaluate_model(sessions, \"ltr_model_variant_1\", feature_set)\n",
    "display(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'long_description_bm25',\n",
       "  'class': 'org.apache.solr.ltr.feature.SolrFeature',\n",
       "  'params': {'q': 'long_description:(${keywords})'},\n",
       "  'store': 'ltr_model_variant_1'},\n",
       " {'name': 'short_description_constant',\n",
       "  'class': 'org.apache.solr.ltr.feature.SolrFeature',\n",
       "  'params': {'q': 'short_description:(${keywords})^=1'},\n",
       "  'store': 'ltr_model_variant_1'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(feature_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 12.3\n",
    "\n",
    "Train a model that hypothetically performs better offline called `ltr_model_variant_2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dryer': 0.07068309073137659,\n",
       " 'blue ray': 0.0,\n",
       " 'headphones': 0.06540945492120899,\n",
       " 'dark of moon': 0.257659200402958,\n",
       " 'transformers dvd': 0.10077083021678328}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random.seed(1234)\n",
    "td = generate_training_data(sessions).reset_index().set_index(\"query\")\n",
    "#print(td.loc[\"headphones\"])\n",
    "\n",
    "feature_set = [\n",
    "    ltr.generate_fuzzy_query_feature(feature_name=\"name_fuzzy\", \n",
    "                                     field_name=\"name\"),\n",
    "    ltr.generate_bigram_query_feature(feature_name=\"name_bigram\",\n",
    "                                      field_name=\"name\"),\n",
    "    ltr.generate_bigram_query_feature(feature_name=\"short_description_bigram\",\n",
    "                                      field_name=\"short_description\")\n",
    "]\n",
    "\n",
    "evaluation = train_and_evaluate_model(sessions, \"ltr_model_variant_2\", feature_set)\n",
    "display(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate a user querying, clicking, purchasing (omitted)\n",
    "\n",
    "This function simulates a user performing a query and possibly taking an action as they scan down the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_live_user_session(query, model_name,\n",
    "                               desired_probability=0.15,\n",
    "                               indifferent_probability=0.03,\n",
    "                               uninterested_probability=0.01,\n",
    "                               quit_per_result_probability=0.2):\n",
    "    \"\"\"Simulates a user 'query' where purchase probability depends on if \n",
    "       products upc is in one of three sets.\n",
    "       \n",
    "       Users purchase a single product per session.    \n",
    "       \n",
    "       Users quit with `quit_per_result_probability` after scanning each rank\n",
    "       \n",
    "       \"\"\"   \n",
    "    desired_products = [\"97360724240\", \"97360722345\", \"826663114164\"]\n",
    "    indifferent_products = [\"97363455349\", \"97361312743\", \"97361372389\",\n",
    "                            \"97361312804\", \"97363532149\", \"97363560449\"]\n",
    "    \n",
    "    response = ltr.search_with_model(model_name, query=query, rerank_query=query, limit=10)\n",
    "\n",
    "    results = pandas.DataFrame(response[\"docs\"]).reset_index()\n",
    "    for doc in results.to_dict(orient=\"records\"): \n",
    "        draw = random.random()\n",
    "        \n",
    "        if doc[\"upc\"] in desired_products:\n",
    "            if draw < desired_probability:\n",
    "                return True\n",
    "        elif doc[\"upc\"] in indifferent_products:\n",
    "            if draw < indifferent_probability:\n",
    "                return True\n",
    "        elif draw < uninterested_probability:\n",
    "            return True\n",
    "        if random.random() < quit_per_result_probability:\n",
    "            return False\n",
    "        \n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 12.4 - Simulated A/B test on just `transformers dvd` query\n",
    "\n",
    "Here we simulate 1000 users being served two rankings for `transformers dvd` and based on the hidden preferences here (`wants_to_purchase` and `might_purchase`) we see which performs better with conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_b_test(query, model_a, model_b):\n",
    "    \"\"\"Randomly assign this user to a or b\"\"\"\n",
    "    draw = random.random()\n",
    "    model_name = model_a if draw < 0.5 else model_b    \n",
    "    purchase_made = simulate_live_user_session(query, model_name)\n",
    "    return (model_name, purchase_made)\n",
    "\n",
    "def simulate_user_a_b_test(query, model_a, model_b, number_of_users=1000):\n",
    "    purchases = {model_a: 0, model_b: 0}\n",
    "    for _ in range(number_of_users): \n",
    "        model_name, purchase_made = a_b_test(query, model_a, model_b)\n",
    "        if purchase_made:\n",
    "            purchases[model_name] += 1\n",
    "    return purchases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ltr_model_variant_1': 21, 'ltr_model_variant_2': 17}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Takes 6 minutes now, used to be quicker\n",
    "random.seed(1234)\n",
    "\n",
    "results = simulate_user_a_b_test(query=\"transformers dvd\",\n",
    "                                 model_a=\"ltr_model_variant_1\",\n",
    "                                 model_b=\"ltr_model_variant_2\",\n",
    "                                 number_of_users=1000)\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New helper: show the features for each SDBN entry (omitted)\n",
    "\n",
    "This function shows us the logged features of each training row for the given sdbn data for debugging.\n",
    "\n",
    "So not just\n",
    "\n",
    "| query   | doc      | grade\n",
    "|---------|----------|---------\n",
    "|transformers dvd | 1234 | 1.0\n",
    "\n",
    "But also a recording of the matches that occured\n",
    "\n",
    "| query           | doc      | grade    | short_desc_match  | long_desc_match |...\n",
    "|-----------------|----------|----------|-------------------|-----------------|---\n",
    "|transformers dvd | 1234     | 1.0      | 0.0               | 1.0             |..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_logged_judgments(training_data, features, model_name):\n",
    "    \"\"\"Log features alongside training_data into a dataframe\"\"\"\n",
    "    judgments = as_judgments(training_data)\n",
    "    ltr.delete_feature_store(model_name)\n",
    "    ltr.upload_features(features, model_name)\n",
    "\n",
    "    ftr_logger = FeatureLogger(engine, index=products_collection,\n",
    "                               feature_set=model_name, id_field=\"upc\")\n",
    "\n",
    "    for qid, query_judgments in groupby(judgments, key=lambda j: j.qid):\n",
    "        ftr_logger.log_for_qid(judgments=query_judgments,\n",
    "                               qid=qid, log=False)\n",
    "        \n",
    "    logged_judgments = ftr_logger.logged\n",
    "    feature_data, predictors, doc_ids = judgments_to_nparray(logged_judgments)\n",
    "    logged_judgments_dataframe = pandas.concat([pandas.DataFrame(predictors),\n",
    "                                                pandas.DataFrame(feature_data),\n",
    "                                                pandas.DataFrame(doc_ids)], \n",
    "                                                axis=1,\n",
    "                                                ignore_index=True)\n",
    "    \n",
    "    qid_map = {j.qid: j.keywords for j in logged_judgments}\n",
    "    qid_map = pandas.DataFrame(qid_map.values()).reset_index() \\\n",
    "                         .rename(columns={\"index\": \"qid\", 0: \"query\"})\n",
    "    \n",
    "    feature_names = [f[\"name\"] for f in features]\n",
    "    columns = {i: name for i, name in enumerate([\"grade\", \"qid\"] + feature_names + [\"doc_id\"])}\n",
    "\n",
    "    logged_judgments_dataframe = logged_judgments_dataframe.rename(columns=columns)\n",
    "    logged_judgments_dataframe = logged_judgments_dataframe.merge(qid_map, how=\"left\", on=\"qid\")\n",
    "    ordered_columns = [\"doc_id\", \"query\", \"grade\"] + feature_names\n",
    "    #logged_judgments_dataframe['grade'] = logged_judgments_dataframe['grade'] / 10.0 \n",
    "    \n",
    "    return logged_judgments_dataframe[ordered_columns].set_index(\"doc_id\").sort_values(\"grade\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 12.5 - Output matches for one feature set\n",
    "\n",
    "Another way of formulating `presentation_bias` is to look at the kinds of documents not being shown to users, so we can strategically show those to users. Below we show the value of each feature in `explore_feature_set` for each document in the sdbn judgments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_explore_features():\n",
    "    return [\n",
    "        ltr.generate_query_feature(feature_name=\"long_description_match\",\n",
    "                                   field_name=\"long_description\",\n",
    "                                   constant_score=True),\n",
    "        ltr.generate_query_feature(feature_name=\"short_description_match\",\n",
    "                                   field_name=\"short_description\",\n",
    "                                   constant_score=True),\n",
    "        ltr.generate_query_feature(feature_name=\"name_match\",\n",
    "                                   field_name=\"name\",\n",
    "                                   constant_score=True),\n",
    "        ltr.generate_query_feature(feature_name=\"has_promotion\",\n",
    "                                   field_name=\"has_promotion\",\n",
    "                                   value=\"true\",\n",
    "                                   constant_score=True)]\n",
    "\n",
    "def get_logged_transformers_judgments(sessions, features):\n",
    "    training_data = generate_training_data(sessions)\n",
    "    logged_judgments = generate_logged_judgments(training_data,\n",
    "                                                 features, \"explore\")\n",
    "    logged_judgments = logged_judgments \\\n",
    "        [logged_judgments[\"query\"] == \"transformers dvd\"]\n",
    "    return logged_judgments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>grade</th>\n",
       "      <th>long_description_match</th>\n",
       "      <th>short_description_match</th>\n",
       "      <th>name_match</th>\n",
       "      <th>has_promotion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>97363560449</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.347137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97361312804</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.344041</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97361312743</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.342160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97363455349</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.342065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97361372389</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.323484</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97363532149</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.322664</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879862003517</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.022834</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93624995012</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.020202</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47875842328</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.018530</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708056579746</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.016726</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47875332911</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.015854</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47875819733</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.015394</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708056579739</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.014979</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879862003524</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.014749</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93624974918</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.012628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47875839090</th>\n",
       "      <td>transformers dvd</td>\n",
       "      <td>0.010721</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         query     grade  long_description_match  \\\n",
       "doc_id                                                             \n",
       "97363560449   transformers dvd  0.347137                     0.0   \n",
       "97361312804   transformers dvd  0.344041                     0.0   \n",
       "97361312743   transformers dvd  0.342160                     0.0   \n",
       "97363455349   transformers dvd  0.342065                     0.0   \n",
       "97361372389   transformers dvd  0.323484                     0.0   \n",
       "97363532149   transformers dvd  0.322664                     0.0   \n",
       "879862003517  transformers dvd  0.022834                     0.0   \n",
       "93624995012   transformers dvd  0.020202                     0.0   \n",
       "47875842328   transformers dvd  0.018530                     1.0   \n",
       "708056579746  transformers dvd  0.016726                     1.0   \n",
       "47875332911   transformers dvd  0.015854                     1.0   \n",
       "47875819733   transformers dvd  0.015394                     1.0   \n",
       "708056579739  transformers dvd  0.014979                     1.0   \n",
       "879862003524  transformers dvd  0.014749                     1.0   \n",
       "93624974918   transformers dvd  0.012628                     0.0   \n",
       "47875839090   transformers dvd  0.010721                     1.0   \n",
       "\n",
       "              short_description_match  name_match  has_promotion  \n",
       "doc_id                                                            \n",
       "97363560449                       0.0         1.0            0.0  \n",
       "97361312804                       0.0         1.0            0.0  \n",
       "97361312743                       0.0         1.0            0.0  \n",
       "97363455349                       0.0         1.0            0.0  \n",
       "97361372389                       0.0         1.0            0.0  \n",
       "97363532149                       0.0         1.0            0.0  \n",
       "879862003517                      1.0         1.0            0.0  \n",
       "93624995012                       0.0         1.0            0.0  \n",
       "47875842328                       0.0         1.0            1.0  \n",
       "708056579746                      0.0         1.0            0.0  \n",
       "47875332911                       0.0         1.0            0.0  \n",
       "47875819733                       0.0         1.0            0.0  \n",
       "708056579739                      1.0         1.0            0.0  \n",
       "879862003524                      1.0         1.0            0.0  \n",
       "93624974918                       0.0         1.0            0.0  \n",
       "47875839090                       0.0         1.0            0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "explore_features = get_latest_explore_features()\n",
    "logged_transformers_judgments = get_logged_transformers_judgments(sessions,\n",
    "                                                                  explore_features)\n",
    "display(logged_transformers_judgments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 12.6 - Train Gaussian Process Regressor\n",
    "\n",
    "We train data on just the `transformers_training_data`. \n",
    "\n",
    "NOTE we could also train on the full sdbn training data, and see globally what's missing. However it's often convenient to zero in on specific queries to round out their training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "def train_gpr(logged_judgments, feature_names):\n",
    "    feature_data = logged_judgments[feature_names]\n",
    "    grades = logged_judgments[\"grade\"]\n",
    "    gpr = GaussianProcessRegressor()\n",
    "    gpr.fit(feature_data, grades)\n",
    "    return gpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianProcessRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianProcessRegressor</label><div class=\"sk-toggleable__content\"><pre>GaussianProcessRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianProcessRegressor()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = [f[\"name\"] for f in explore_features]\n",
    "train_gpr(logged_transformers_judgments, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 12.7: Predict on every value\n",
    "\n",
    "Here `gpr` predicts on every possible feature value. This lets us analyze which set of feature values to use when exploring with users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prediction_data(logged_judgments, feature_names):\n",
    "    index = pandas.MultiIndex.from_product([[0, 1]] * 4,\n",
    "                                           names=feature_names)\n",
    "    with_prediction = pandas.DataFrame(index=index).reset_index()\n",
    "\n",
    "    gpr = train_gpr(logged_judgments, feature_names)\n",
    "    predictions_with_std = gpr.predict(\n",
    "        with_prediction[feature_names], return_std=True)\n",
    "    with_prediction[\"predicted_grade\"] = predictions_with_std[0]\n",
    "    with_prediction[\"predicted_stddev\"] = predictions_with_std[1]\n",
    "   \n",
    "    return  with_prediction.sort_values(\"predicted_stddev\", ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>long_description_match</th>\n",
       "      <th>short_description_match</th>\n",
       "      <th>name_match</th>\n",
       "      <th>has_promotion</th>\n",
       "      <th>predicted_grade</th>\n",
       "      <th>predicted_stddev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.256798</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014674</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014864</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.022834</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.018530</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.161596</td>\n",
       "      <td>0.632121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014856</td>\n",
       "      <td>0.632121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017392</td>\n",
       "      <td>0.739305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.155756</td>\n",
       "      <td>0.795060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.795060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009016</td>\n",
       "      <td>0.795060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013849</td>\n",
       "      <td>0.795060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011239</td>\n",
       "      <td>0.795060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.098013</td>\n",
       "      <td>0.882676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009011</td>\n",
       "      <td>0.882676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010549</td>\n",
       "      <td>0.912794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    long_description_match  short_description_match  name_match  \\\n",
       "2                        0                        0           1   \n",
       "10                       1                        0           1   \n",
       "14                       1                        1           1   \n",
       "6                        0                        1           1   \n",
       "11                       1                        0           1   \n",
       "3                        0                        0           1   \n",
       "15                       1                        1           1   \n",
       "7                        0                        1           1   \n",
       "0                        0                        0           0   \n",
       "8                        1                        0           0   \n",
       "12                       1                        1           0   \n",
       "4                        0                        1           0   \n",
       "9                        1                        0           0   \n",
       "1                        0                        0           0   \n",
       "13                       1                        1           0   \n",
       "5                        0                        1           0   \n",
       "\n",
       "    has_promotion  predicted_grade  predicted_stddev  \n",
       "2               0         0.256798          0.000004  \n",
       "10              0         0.014674          0.000005  \n",
       "14              0         0.014864          0.000007  \n",
       "6               0         0.022834          0.000010  \n",
       "11              1         0.018530          0.000010  \n",
       "3               1         0.161596          0.632121  \n",
       "15              1         0.014856          0.632121  \n",
       "7               1         0.017392          0.739305  \n",
       "0               0         0.155756          0.795060  \n",
       "8               0         0.008900          0.795060  \n",
       "12              0         0.009016          0.795060  \n",
       "4               0         0.013849          0.795060  \n",
       "9               1         0.011239          0.795060  \n",
       "1               1         0.098013          0.882676  \n",
       "13              1         0.009011          0.882676  \n",
       "5               1         0.010549          0.912794  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction_data = calculate_prediction_data(logged_transformers_judgments,\n",
    "                                                            feature_names)\n",
    "display(prediction_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 12.8 - Calculate Expected Improvement\n",
    "\n",
    "\n",
    "We use [Expected Improvement](https://distill.pub/2020/bayesian-optimization/) scoring to select candidates for exploration within the `transformers dvd` query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def calculate_expected_improvement(logged_judgments, feature_names, theta=0.6):\n",
    "    data = calculate_prediction_data(logged_judgments, feature_names)\n",
    "    data[\"opportunity\"] = (data[\"predicted_grade\"] -\n",
    "                           logged_judgments[\"grade\"].mean() -\n",
    "                           theta)\n",
    "    data[\"prob_of_improvement\"] = (\n",
    "        norm.cdf(data[\"opportunity\"] /\n",
    "                 data[\"predicted_stddev\"]))\n",
    "    data[\"expected_improvement\"] = (\n",
    "        data[\"opportunity\"] * data[\"prob_of_improvement\"] + \n",
    "        data[\"predicted_stddev\"] *\n",
    "        norm.pdf(data[\"opportunity\"] /\n",
    "                 data[\"predicted_stddev\"]))    \n",
    "    return data.sort_values(\"expected_improvement\",\n",
    "                            ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>long_description_match</th>\n",
       "      <th>short_description_match</th>\n",
       "      <th>name_match</th>\n",
       "      <th>has_promotion</th>\n",
       "      <th>predicted_grade</th>\n",
       "      <th>predicted_stddev</th>\n",
       "      <th>opportunity</th>\n",
       "      <th>prob_of_improvement</th>\n",
       "      <th>expected_improvement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.098013</td>\n",
       "      <td>0.882676</td>\n",
       "      <td>-0.638497</td>\n",
       "      <td>0.234728</td>\n",
       "      <td>0.121201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010549</td>\n",
       "      <td>0.912794</td>\n",
       "      <td>-0.725962</td>\n",
       "      <td>0.213214</td>\n",
       "      <td>0.110633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.155756</td>\n",
       "      <td>0.795060</td>\n",
       "      <td>-0.580755</td>\n",
       "      <td>0.232556</td>\n",
       "      <td>0.107853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009011</td>\n",
       "      <td>0.882676</td>\n",
       "      <td>-0.727500</td>\n",
       "      <td>0.204914</td>\n",
       "      <td>0.101653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013849</td>\n",
       "      <td>0.795060</td>\n",
       "      <td>-0.722661</td>\n",
       "      <td>0.181691</td>\n",
       "      <td>0.078549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011239</td>\n",
       "      <td>0.795060</td>\n",
       "      <td>-0.725272</td>\n",
       "      <td>0.180826</td>\n",
       "      <td>0.078076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009016</td>\n",
       "      <td>0.795060</td>\n",
       "      <td>-0.727495</td>\n",
       "      <td>0.180091</td>\n",
       "      <td>0.077675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.795060</td>\n",
       "      <td>-0.727610</td>\n",
       "      <td>0.180053</td>\n",
       "      <td>0.077654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017392</td>\n",
       "      <td>0.739305</td>\n",
       "      <td>-0.719118</td>\n",
       "      <td>0.165353</td>\n",
       "      <td>0.064866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.161596</td>\n",
       "      <td>0.632121</td>\n",
       "      <td>-0.574914</td>\n",
       "      <td>0.181543</td>\n",
       "      <td>0.062387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014856</td>\n",
       "      <td>0.632121</td>\n",
       "      <td>-0.721654</td>\n",
       "      <td>0.126802</td>\n",
       "      <td>0.039922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.256798</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.479713</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014674</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.721837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014864</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.721646</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.022834</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.713677</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.018530</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.717981</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    long_description_match  short_description_match  name_match  \\\n",
       "1                        0                        0           0   \n",
       "5                        0                        1           0   \n",
       "0                        0                        0           0   \n",
       "13                       1                        1           0   \n",
       "4                        0                        1           0   \n",
       "9                        1                        0           0   \n",
       "12                       1                        1           0   \n",
       "8                        1                        0           0   \n",
       "7                        0                        1           1   \n",
       "3                        0                        0           1   \n",
       "15                       1                        1           1   \n",
       "2                        0                        0           1   \n",
       "10                       1                        0           1   \n",
       "14                       1                        1           1   \n",
       "6                        0                        1           1   \n",
       "11                       1                        0           1   \n",
       "\n",
       "    has_promotion  predicted_grade  predicted_stddev  opportunity  \\\n",
       "1               1         0.098013          0.882676    -0.638497   \n",
       "5               1         0.010549          0.912794    -0.725962   \n",
       "0               0         0.155756          0.795060    -0.580755   \n",
       "13              1         0.009011          0.882676    -0.727500   \n",
       "4               0         0.013849          0.795060    -0.722661   \n",
       "9               1         0.011239          0.795060    -0.725272   \n",
       "12              0         0.009016          0.795060    -0.727495   \n",
       "8               0         0.008900          0.795060    -0.727610   \n",
       "7               1         0.017392          0.739305    -0.719118   \n",
       "3               1         0.161596          0.632121    -0.574914   \n",
       "15              1         0.014856          0.632121    -0.721654   \n",
       "2               0         0.256798          0.000004    -0.479713   \n",
       "10              0         0.014674          0.000005    -0.721837   \n",
       "14              0         0.014864          0.000007    -0.721646   \n",
       "6               0         0.022834          0.000010    -0.713677   \n",
       "11              1         0.018530          0.000010    -0.717981   \n",
       "\n",
       "    prob_of_improvement  expected_improvement  \n",
       "1              0.234728              0.121201  \n",
       "5              0.213214              0.110633  \n",
       "0              0.232556              0.107853  \n",
       "13             0.204914              0.101653  \n",
       "4              0.181691              0.078549  \n",
       "9              0.180826              0.078076  \n",
       "12             0.180091              0.077675  \n",
       "8              0.180053              0.077654  \n",
       "7              0.165353              0.064866  \n",
       "3              0.181543              0.062387  \n",
       "15             0.126802              0.039922  \n",
       "2              0.000000              0.000000  \n",
       "10             0.000000              0.000000  \n",
       "14             0.000000              0.000000  \n",
       "6              0.000000              0.000000  \n",
       "11             0.000000              0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "improvement_data = calculate_expected_improvement(\n",
    "    logged_transformers_judgments, feature_names)\n",
    "display(improvement_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a query to fetch `explore` docs (omitted)\n",
    "\n",
    "Based on the selected features from the GaussianProcessRegressor, we create a query to fetch a doc that contains those features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_for_explore_candidate(explore_vector, query=\"\"):\n",
    "    feature_config = {\n",
    "        \"long_description_match\": {\"field\": \"long_description\", \"value\": query},\n",
    "        \"short_description_match\": {\"field\": \"short_description\", \"value\": query},\n",
    "        \"name_match\": {\"field\": \"name\", \"value\": query},\n",
    "        \"long_description_bm25\": {\"field\": \"long_description\", \"value\": query},\n",
    "        \"manufacturer_match\": {\"field\": \"manufacturer\", \"value\": query},\n",
    "        \"has_promotion\": {\"field\": \"has_promotion\", \"value\": \"true\"}\n",
    "    }\n",
    "    explore_candidates = ltr.get_explore_candidate(query, explore_vector, feature_config)\n",
    "    if explore_candidates:\n",
    "        return explore_candidates[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 12.9 - Find document to explore from Solr\n",
    "\n",
    "Here we fetch a document that matches the properties of something missing from our training set to display to the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore(query, logged_judgments, features):\n",
    "    \"\"\"Explore according to the provided explore vector, select\n",
    "       a random doc from that group.\"\"\"\n",
    "    feature_names = [f[\"name\"] for f in features]\n",
    "    prediction_data = calculate_expected_improvement(logged_judgments,\n",
    "                                                     feature_names)\n",
    "    explore_vector = prediction_data.head().iloc[0][feature_names]\n",
    "    return search_for_explore_candidate(explore_vector, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27242815414\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "\n",
    "explore_features = get_latest_explore_features()\n",
    "logged_judgments = get_logged_transformers_judgments(sessions,\n",
    "                                                     explore_features)\n",
    "explore_upc = explore(\"transformers dvd\", logged_judgments,\n",
    "                                          explore_features)[\"upc\"]\n",
    "print(explore_upc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New heavily clicked doc is promoted!\n",
    "\n",
    "```\n",
    "{\"upc\": \"826663114164\",\n",
    " \"name\": \"Transformers: The Complete Series [25th Anniversary Matrix of Leadership Edition] [16 Discs] - DVD\",\n",
    " \"manufacturer\": \" \",\n",
    " \"short_description\": \" \",\n",
    " \"long_description\": \" \",\n",
    " \"has_promotion\": True}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate new sessions with the new data\n",
    "\n",
    "We simulate new sessions, if the upc is in `might_purchase` or `wants_to_purchase`, we set it to 'clicked' with a given probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_simulated_exploration_sessions(query, sessions,\n",
    "                                            logged_judgments, features, n=500):\n",
    "    \"\"\"Conducts N (500) searches with the query and returns session data with\n",
    "       simulated the simulated user behavior\"\"\"\n",
    "    wants_to_purchase = [97360724240, 97360722345, 826663114164, 97360810042, 93624956037]\n",
    "    might_purchase = [97363455349, 97361312743, 97361372389,\n",
    "                      97361312804, 97363532149, 97363560449]\n",
    "    explore_on_rank = 2.0\n",
    "    with_explore_sessions = sessions.copy()\n",
    "    query_sessions = with_explore_sessions[with_explore_sessions[\"query\"] == query]\n",
    "    for i in range(0, n):\n",
    "        explore_doc = explore(query, logged_judgments, features)\n",
    "        if explore_doc:\n",
    "            explore_upc = int(explore_doc[\"upc\"])\n",
    "            sess_ids = list(set(query_sessions[\"sess_id\"].tolist()))\n",
    "            random.shuffle(sess_ids)\n",
    "            new_session = query_sessions[query_sessions[\"sess_id\"] == sess_ids[0]].copy()\n",
    "            new_session[\"sess_id\"] = 100000 + i\n",
    "            new_session.loc[new_session[\"rank\"] == explore_on_rank, \"doc_id\"] = explore_upc\n",
    "            draw = random.random()\n",
    "            click = ((explore_upc in wants_to_purchase and draw < 0.8) or\n",
    "                     (explore_upc in might_purchase and draw < 0.5) or\n",
    "                     draw < 0.01)\n",
    "            if click:\n",
    "                print(f\"Search {i} resulted in a click on {explore_upc}\")\n",
    "            new_session.loc[new_session[\"rank\"] == explore_on_rank, \"clicked\"] = click\n",
    "            \n",
    "            with_explore_sessions = pandas.concat([with_explore_sessions, new_session])\n",
    "        else:\n",
    "            print(f\"Search {i} no docs\")\n",
    "            \n",
    "    return with_explore_sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 12.10 - Update judgments from new sessions\n",
    "\n",
    "Have we added any new docs that appear to be getting more clicks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search 4 resulted in a click on 27242799127\n",
      "Search 8 resulted in a click on 97360724240\n",
      "Search 12 resulted in a click on 97360724240\n",
      "Search 13 resulted in a click on 97360722345\n",
      "Search 14 resulted in a click on 97360722345\n",
      "Search 21 resulted in a click on 97360722345\n",
      "Search 28 resulted in a click on 826663114164\n",
      "Search 31 resulted in a click on 97360724240\n",
      "Search 32 resulted in a click on 97360722345\n",
      "Search 34 resulted in a click on 826663114164\n",
      "Search 36 resulted in a click on 27242815414\n",
      "Search 38 resulted in a click on 97360722345\n",
      "Search 41 resulted in a click on 97360724240\n",
      "Search 45 resulted in a click on 826663114164\n",
      "Search 47 resulted in a click on 97360724240\n",
      "Search 50 resulted in a click on 826663114164\n",
      "Search 51 resulted in a click on 97360724240\n",
      "Search 56 resulted in a click on 74108007469\n",
      "Search 57 resulted in a click on 826663114164\n",
      "Search 61 resulted in a click on 97360724240\n",
      "Search 62 resulted in a click on 97360724240\n",
      "Search 67 resulted in a click on 97360722345\n",
      "Search 68 resulted in a click on 826663114164\n",
      "Search 71 resulted in a click on 826663114164\n",
      "Search 78 resulted in a click on 97360722345\n",
      "Search 83 resulted in a click on 826663114164\n",
      "Search 86 resulted in a click on 826663114164\n",
      "Search 90 resulted in a click on 826663114164\n",
      "Search 97 resulted in a click on 97360722345\n",
      "Search 98 resulted in a click on 826663114164\n",
      "Search 99 resulted in a click on 826663114164\n",
      "Search 102 resulted in a click on 97360722345\n",
      "Search 103 resulted in a click on 97360724240\n",
      "Search 107 resulted in a click on 97360724240\n",
      "Search 109 resulted in a click on 826663114164\n",
      "Search 117 resulted in a click on 97360722345\n",
      "Search 127 resulted in a click on 97360722345\n",
      "Search 130 resulted in a click on 826663114164\n",
      "Search 132 resulted in a click on 97360722345\n",
      "Search 135 resulted in a click on 826663114164\n",
      "Search 143 resulted in a click on 97360722345\n",
      "Search 144 resulted in a click on 826663114164\n",
      "Search 146 resulted in a click on 826663114164\n",
      "Search 152 resulted in a click on 74108007469\n",
      "Search 153 resulted in a click on 97360724240\n",
      "Search 155 resulted in a click on 826663114164\n",
      "Search 163 resulted in a click on 97360724240\n",
      "Search 164 resulted in a click on 826663114164\n",
      "Search 166 resulted in a click on 97360724240\n",
      "Search 186 resulted in a click on 97360724240\n",
      "Search 188 resulted in a click on 826663114164\n",
      "Search 189 resulted in a click on 97360722345\n",
      "Search 192 resulted in a click on 97360724240\n",
      "Search 205 resulted in a click on 97360724240\n",
      "Search 206 resulted in a click on 97360724240\n",
      "Search 209 resulted in a click on 97360724240\n",
      "Search 211 resulted in a click on 97360724240\n",
      "Search 213 resulted in a click on 826663114164\n",
      "Search 214 resulted in a click on 97360724240\n",
      "Search 219 resulted in a click on 97360724240\n",
      "Search 220 resulted in a click on 97360722345\n",
      "Search 221 resulted in a click on 97360722345\n",
      "Search 228 resulted in a click on 97360722345\n",
      "Search 232 resulted in a click on 97360724240\n",
      "Search 233 resulted in a click on 97360722345\n",
      "Search 239 resulted in a click on 97360724240\n",
      "Search 241 resulted in a click on 97360724240\n",
      "Search 242 resulted in a click on 826663114164\n",
      "Search 249 resulted in a click on 97360724240\n",
      "Search 254 resulted in a click on 97360724240\n",
      "Search 257 resulted in a click on 97360724240\n",
      "Search 265 resulted in a click on 97360724240\n",
      "Search 269 resulted in a click on 97360722345\n",
      "Search 272 resulted in a click on 97360724240\n",
      "Search 280 resulted in a click on 97360722345\n",
      "Search 281 resulted in a click on 826663114164\n",
      "Search 289 resulted in a click on 97360722345\n",
      "Search 293 resulted in a click on 97360724240\n",
      "Search 300 resulted in a click on 826663114164\n",
      "Search 309 resulted in a click on 97360722345\n",
      "Search 319 resulted in a click on 97360724240\n",
      "Search 331 resulted in a click on 826663114164\n",
      "Search 336 resulted in a click on 97360722345\n",
      "Search 349 resulted in a click on 826663114164\n",
      "Search 350 resulted in a click on 826663114164\n",
      "Search 352 resulted in a click on 97360724240\n",
      "Search 361 resulted in a click on 97360724240\n",
      "Search 363 resulted in a click on 97360724240\n",
      "Search 366 resulted in a click on 97360722345\n",
      "Search 367 resulted in a click on 826663114164\n",
      "Search 370 resulted in a click on 97360722345\n",
      "Search 371 resulted in a click on 826663114164\n",
      "Search 372 resulted in a click on 826663114164\n",
      "Search 384 resulted in a click on 97360722345\n",
      "Search 388 resulted in a click on 97360724240\n",
      "Search 402 resulted in a click on 826663114164\n",
      "Search 410 resulted in a click on 826663114164\n",
      "Search 417 resulted in a click on 97360724240\n",
      "Search 421 resulted in a click on 97360724240\n",
      "Search 423 resulted in a click on 97360722345\n",
      "Search 432 resulted in a click on 826663114164\n",
      "Search 436 resulted in a click on 97360722345\n",
      "Search 441 resulted in a click on 97360722345\n",
      "Search 442 resulted in a click on 97360724240\n",
      "Search 446 resulted in a click on 97360724240\n",
      "Search 447 resulted in a click on 826663114164\n",
      "Search 448 resulted in a click on 97360724240\n",
      "Search 452 resulted in a click on 97360724240\n",
      "Search 453 resulted in a click on 97360722345\n",
      "Search 456 resulted in a click on 97360724240\n",
      "Search 461 resulted in a click on 97360724240\n",
      "Search 464 resulted in a click on 826663114164\n",
      "Search 465 resulted in a click on 97360724240\n",
      "Search 472 resulted in a click on 826663114164\n",
      "Search 473 resulted in a click on 826663114164\n",
      "Search 479 resulted in a click on 97360724240\n",
      "Search 486 resulted in a click on 826663114164\n",
      "Search 490 resulted in a click on 826663114164\n",
      "Search 495 resulted in a click on 826663114164\n",
      "Search 498 resulted in a click on 97360722345\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clicked</th>\n",
       "      <th>examined</th>\n",
       "      <th>grade</th>\n",
       "      <th>beta_grade</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>97360724240</th>\n",
       "      <td>45</td>\n",
       "      <td>49</td>\n",
       "      <td>0.918367</td>\n",
       "      <td>0.796610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826663114164</th>\n",
       "      <td>40</td>\n",
       "      <td>47</td>\n",
       "      <td>0.851064</td>\n",
       "      <td>0.736842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97360722345</th>\n",
       "      <td>31</td>\n",
       "      <td>39</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.673469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97363455349</th>\n",
       "      <td>731</td>\n",
       "      <td>2116</td>\n",
       "      <td>0.345463</td>\n",
       "      <td>0.344779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97361312804</th>\n",
       "      <td>726</td>\n",
       "      <td>2107</td>\n",
       "      <td>0.344566</td>\n",
       "      <td>0.343883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97363560449</th>\n",
       "      <td>733</td>\n",
       "      <td>2134</td>\n",
       "      <td>0.343486</td>\n",
       "      <td>0.342817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97361312743</th>\n",
       "      <td>708</td>\n",
       "      <td>2078</td>\n",
       "      <td>0.340712</td>\n",
       "      <td>0.340038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97363532149</th>\n",
       "      <td>692</td>\n",
       "      <td>2101</td>\n",
       "      <td>0.329367</td>\n",
       "      <td>0.328754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97361372389</th>\n",
       "      <td>673</td>\n",
       "      <td>2095</td>\n",
       "      <td>0.321241</td>\n",
       "      <td>0.320665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74108007469</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27242799127</th>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.103448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27242815414</th>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.096774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400192926087</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12505525766</th>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27242813908</th>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47875842328</th>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803238004525</th>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879862003517</th>\n",
       "      <td>37</td>\n",
       "      <td>1862</td>\n",
       "      <td>0.019871</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93624995012</th>\n",
       "      <td>36</td>\n",
       "      <td>1827</td>\n",
       "      <td>0.019704</td>\n",
       "      <td>0.020686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47875842328</th>\n",
       "      <td>32</td>\n",
       "      <td>1808</td>\n",
       "      <td>0.017699</td>\n",
       "      <td>0.018702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708056579746</th>\n",
       "      <td>29</td>\n",
       "      <td>1811</td>\n",
       "      <td>0.016013</td>\n",
       "      <td>0.017024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47875332911</th>\n",
       "      <td>27</td>\n",
       "      <td>1791</td>\n",
       "      <td>0.015075</td>\n",
       "      <td>0.016102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879862003524</th>\n",
       "      <td>25</td>\n",
       "      <td>1830</td>\n",
       "      <td>0.013661</td>\n",
       "      <td>0.014674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47875819733</th>\n",
       "      <td>25</td>\n",
       "      <td>1834</td>\n",
       "      <td>0.013631</td>\n",
       "      <td>0.014642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708056579739</th>\n",
       "      <td>23</td>\n",
       "      <td>1817</td>\n",
       "      <td>0.012658</td>\n",
       "      <td>0.013684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93624974918</th>\n",
       "      <td>20</td>\n",
       "      <td>1784</td>\n",
       "      <td>0.011211</td>\n",
       "      <td>0.012263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47875839090</th>\n",
       "      <td>17</td>\n",
       "      <td>1817</td>\n",
       "      <td>0.009356</td>\n",
       "      <td>0.010400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              clicked  examined     grade  beta_grade\n",
       "doc_id                                               \n",
       "97360724240        45        49  0.918367    0.796610\n",
       "826663114164       40        47  0.851064    0.736842\n",
       "97360722345        31        39  0.794872    0.673469\n",
       "97363455349       731      2116  0.345463    0.344779\n",
       "97361312804       726      2107  0.344566    0.343883\n",
       "97363560449       733      2134  0.343486    0.342817\n",
       "97361312743       708      2078  0.340712    0.340038\n",
       "97363532149       692      2101  0.329367    0.328754\n",
       "97361372389       673      2095  0.321241    0.320665\n",
       "74108007469         2        20  0.100000    0.133333\n",
       "27242799127         1        19  0.052632    0.103448\n",
       "27242815414         1        21  0.047619    0.096774\n",
       "400192926087        0        13  0.000000    0.086957\n",
       "12505525766         0        19  0.000000    0.068966\n",
       "27242813908         0        23  0.000000    0.060606\n",
       "47875842328         0        26  0.000000    0.055556\n",
       "803238004525        0        34  0.000000    0.045455\n",
       "879862003517       37      1862  0.019871    0.020833\n",
       "93624995012        36      1827  0.019704    0.020686\n",
       "47875842328        32      1808  0.017699    0.018702\n",
       "708056579746       29      1811  0.016013    0.017024\n",
       "47875332911        27      1791  0.015075    0.016102\n",
       "879862003524       25      1830  0.013661    0.014674\n",
       "47875819733        25      1834  0.013631    0.014642\n",
       "708056579739       23      1817  0.012658    0.013684\n",
       "93624974918        20      1784  0.011211    0.012263\n",
       "47875839090        17      1817  0.009356    0.010400"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random.seed(1234)\n",
    "\n",
    "query = \"transformers dvd\"\n",
    "sessions_with_exploration = generate_simulated_exploration_sessions(\n",
    "    query, sessions, logged_transformers_judgments, explore_features)\n",
    "training_data_with_exploration = \\\n",
    "    generate_training_data(sessions_with_exploration)\n",
    "display(training_data_with_exploration.loc[\"transformers dvd\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 12.11 - Rebuild model using updated judgments\n",
    "\n",
    "After showing the new document to users, we can rebuild the model using judgments that cover this feature blindspot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dryer': 0.12737002598513025,\n",
       " 'blue ray': 0.08461538461538462,\n",
       " 'headphones': 0.12110565745285455,\n",
       " 'dark of moon': 0.1492224251599605,\n",
       " 'transformers dvd': 0.2525515864145938}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random.seed(1234)\n",
    "\n",
    "promotion_feature_set = [\n",
    "    ltr.generate_fuzzy_query_feature(feature_name=\"name_fuzzy\",\n",
    "                                     field_name=\"name\"),\n",
    "    ltr.generate_bigram_query_feature(feature_name=\"name_bigram\",\n",
    "                                      field_name=\"name\"),\n",
    "    ltr.generate_bigram_query_feature(feature_name=\"short_description_bigram\",\n",
    "                                      field_name=\"short_description\"),\n",
    "    ltr.generate_query_feature(feature_name=\"has_promotion\",\n",
    "                               field_name=\"has_promotion\",\n",
    "                               value=\"true\",\n",
    "                               constant_score=True)]\n",
    "\n",
    "evaluation = train_and_evaluate_model(sessions_with_exploration,\n",
    "                                      \"ltr_model_variant_3\",\n",
    "                                      promotion_feature_set)\n",
    "display(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'name_fuzzy',\n",
       "  'class': 'org.apache.solr.ltr.feature.SolrFeature',\n",
       "  'params': {'q': 'name_ngram:(${keywords})'},\n",
       "  'store': 'ltr_model_variant_3'},\n",
       " {'name': 'name_bigram',\n",
       "  'class': 'org.apache.solr.ltr.feature.SolrFeature',\n",
       "  'params': {'q': '{!edismax qf=name pf2=name}(${keywords})'},\n",
       "  'store': 'ltr_model_variant_3'},\n",
       " {'name': 'short_description_bigram',\n",
       "  'class': 'org.apache.solr.ltr.feature.SolrFeature',\n",
       "  'params': {'q': '{!edismax qf=short_description pf2=short_description}(${keywords})'},\n",
       "  'store': 'ltr_model_variant_3'},\n",
       " {'name': 'has_promotion',\n",
       "  'class': 'org.apache.solr.ltr.feature.SolrFeature',\n",
       "  'params': {'q': 'has_promotion:true^=1'},\n",
       "  'store': 'ltr_model_variant_3'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(promotion_feature_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 12.12 - Searching with the latest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Transformers/Transformers: Revenge of the Fallen: Two-Movie Mega Collection [2 Discs] - Widescreen - DVD',\n",
       " 'Transformers: Revenge of the Fallen - Widescreen - DVD',\n",
       " 'Transformers: Dark of the Moon - Original Soundtrack - CD',\n",
       " 'Transformers: The Complete Series [25th Anniversary Matrix of Leadership Edition] [16 Discs] - DVD',\n",
       " 'Transformers: Dark of the Moon Stealth Force Edition - Nintendo Wii']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = ltr.search_with_model(\"ltr_model_variant_3\",\n",
    "                                query=\"transformers dvd\",\n",
    "                                rerank_query=\"transformers dvd\",\n",
    "                                limit=5)[\"docs\"]\n",
    "display([doc[\"name\"] for doc in results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 12.13 - Rerun A/B test on new `promotion` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ltr_model_variant_1': 21, 'ltr_model_variant_3': 145}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random.seed(1234)\n",
    "\n",
    "results = simulate_user_a_b_test(query=\"transformers dvd\",\n",
    "                                 model_a=\"ltr_model_variant_1\",\n",
    "                                 model_b=\"ltr_model_variant_3\",\n",
    "                                 number_of_users=1000)\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 12.14 - Fully Automated LTR Loop\n",
    "\n",
    "These lines expand Listing 12.13 from the book (the book content is a truncated form of what's below). You could put this in a loop and constantly try new features to try to get closer at a generalized ranking solution of what users actually want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear]Search 0 no docs\n",
      "Search 1 no docs\n",
      "Search 2 no docs\n",
      "Search 3 no docs\n",
      "Search 4 no docs\n",
      "Search 5 no docs\n",
      "Search 6 no docs\n",
      "Search 7 no docs\n",
      "Search 8 no docs\n",
      "Search 9 no docs\n",
      "Search 10 no docs\n",
      "Search 11 no docs\n",
      "Search 12 no docs\n",
      "Search 13 no docs\n",
      "Search 14 no docs\n",
      "Search 15 no docs\n",
      "Search 16 no docs\n",
      "Search 17 no docs\n",
      "Search 18 no docs\n",
      "Search 19 no docs\n",
      "Search 20 no docs\n",
      "Search 21 no docs\n",
      "Search 22 no docs\n",
      "Search 23 no docs\n",
      "Search 24 no docs\n",
      "Search 25 no docs\n",
      "Search 26 no docs\n",
      "Search 27 no docs\n",
      "Search 28 no docs\n",
      "Search 29 no docs\n",
      "Search 30 no docs\n",
      "Search 31 no docs\n",
      "Search 32 no docs\n",
      "Search 33 no docs\n",
      "Search 34 no docs\n",
      "Search 35 no docs\n",
      "Search 36 no docs\n",
      "Search 37 no docs\n",
      "Search 38 no docs\n",
      "Search 39 no docs\n",
      "Search 40 no docs\n",
      "Search 41 no docs\n",
      "Search 42 no docs\n",
      "Search 43 no docs\n",
      "Search 44 no docs\n",
      "Search 45 no docs\n",
      "Search 46 no docs\n",
      "Search 47 no docs\n",
      "Search 48 no docs\n",
      "Search 49 no docs\n",
      "Search 50 no docs\n",
      "Search 51 no docs\n",
      "Search 52 no docs\n",
      "Search 53 no docs\n",
      "Search 54 no docs\n",
      "Search 55 no docs\n",
      "Search 56 no docs\n",
      "Search 57 no docs\n",
      "Search 58 no docs\n",
      "Search 59 no docs\n",
      "Search 60 no docs\n",
      "Search 61 no docs\n",
      "Search 62 no docs\n",
      "Search 63 no docs\n",
      "Search 64 no docs\n",
      "Search 65 no docs\n",
      "Search 66 no docs\n",
      "Search 67 no docs\n",
      "Search 68 no docs\n",
      "Search 69 no docs\n",
      "Search 70 no docs\n",
      "Search 71 no docs\n",
      "Search 72 no docs\n",
      "Search 73 no docs\n",
      "Search 74 no docs\n",
      "Search 75 no docs\n",
      "Search 76 no docs\n",
      "Search 77 no docs\n",
      "Search 78 no docs\n",
      "Search 79 no docs\n",
      "Search 80 no docs\n",
      "Search 81 no docs\n",
      "Search 82 no docs\n",
      "Search 83 no docs\n",
      "Search 84 no docs\n",
      "Search 85 no docs\n",
      "Search 86 no docs\n",
      "Search 87 no docs\n",
      "Search 88 no docs\n",
      "Search 89 no docs\n",
      "Search 90 no docs\n",
      "Search 91 no docs\n",
      "Search 92 no docs\n",
      "Search 93 no docs\n",
      "Search 94 no docs\n",
      "Search 95 no docs\n",
      "Search 96 no docs\n",
      "Search 97 no docs\n",
      "Search 98 no docs\n",
      "Search 99 no docs\n",
      "Search 100 no docs\n",
      "Search 101 no docs\n",
      "Search 102 no docs\n",
      "Search 103 no docs\n",
      "Search 104 no docs\n",
      "Search 105 no docs\n",
      "Search 106 no docs\n",
      "Search 107 no docs\n",
      "Search 108 no docs\n",
      "Search 109 no docs\n",
      "Search 110 no docs\n",
      "Search 111 no docs\n",
      "Search 112 no docs\n",
      "Search 113 no docs\n",
      "Search 114 no docs\n",
      "Search 115 no docs\n",
      "Search 116 no docs\n",
      "Search 117 no docs\n",
      "Search 118 no docs\n",
      "Search 119 no docs\n",
      "Search 120 no docs\n",
      "Search 121 no docs\n",
      "Search 122 no docs\n",
      "Search 123 no docs\n",
      "Search 124 no docs\n",
      "Search 125 no docs\n",
      "Search 126 no docs\n",
      "Search 127 no docs\n",
      "Search 128 no docs\n",
      "Search 129 no docs\n",
      "Search 130 no docs\n",
      "Search 131 no docs\n",
      "Search 132 no docs\n",
      "Search 133 no docs\n",
      "Search 134 no docs\n",
      "Search 135 no docs\n",
      "Search 136 no docs\n",
      "Search 137 no docs\n",
      "Search 138 no docs\n",
      "Search 139 no docs\n",
      "Search 140 no docs\n",
      "Search 141 no docs\n",
      "Search 142 no docs\n",
      "Search 143 no docs\n",
      "Search 144 no docs\n",
      "Search 145 no docs\n",
      "Search 146 no docs\n",
      "Search 147 no docs\n",
      "Search 148 no docs\n",
      "Search 149 no docs\n",
      "Search 150 no docs\n",
      "Search 151 no docs\n",
      "Search 152 no docs\n",
      "Search 153 no docs\n",
      "Search 154 no docs\n",
      "Search 155 no docs\n",
      "Search 156 no docs\n",
      "Search 157 no docs\n",
      "Search 158 no docs\n",
      "Search 159 no docs\n",
      "Search 160 no docs\n",
      "Search 161 no docs\n",
      "Search 162 no docs\n",
      "Search 163 no docs\n",
      "Search 164 no docs\n",
      "Search 165 no docs\n",
      "Search 166 no docs\n",
      "Search 167 no docs\n",
      "Search 168 no docs\n",
      "Search 169 no docs\n",
      "Search 170 no docs\n",
      "Search 171 no docs\n",
      "Search 172 no docs\n",
      "Search 173 no docs\n",
      "Search 174 no docs\n",
      "Search 175 no docs\n",
      "Search 176 no docs\n",
      "Search 177 no docs\n",
      "Search 178 no docs\n",
      "Search 179 no docs\n",
      "Search 180 no docs\n",
      "Search 181 no docs\n",
      "Search 182 no docs\n",
      "Search 183 no docs\n",
      "Search 184 no docs\n",
      "Search 185 no docs\n",
      "Search 186 no docs\n",
      "Search 187 no docs\n",
      "Search 188 no docs\n",
      "Search 189 no docs\n",
      "Search 190 no docs\n",
      "Search 191 no docs\n",
      "Search 192 no docs\n",
      "Search 193 no docs\n",
      "Search 194 no docs\n",
      "Search 195 no docs\n",
      "Search 196 no docs\n",
      "Search 197 no docs\n",
      "Search 198 no docs\n",
      "Search 199 no docs\n",
      "Search 200 no docs\n",
      "Search 201 no docs\n",
      "Search 202 no docs\n",
      "Search 203 no docs\n",
      "Search 204 no docs\n",
      "Search 205 no docs\n",
      "Search 206 no docs\n",
      "Search 207 no docs\n",
      "Search 208 no docs\n",
      "Search 209 no docs\n",
      "Search 210 no docs\n",
      "Search 211 no docs\n",
      "Search 212 no docs\n",
      "Search 213 no docs\n",
      "Search 214 no docs\n",
      "Search 215 no docs\n",
      "Search 216 no docs\n",
      "Search 217 no docs\n",
      "Search 218 no docs\n",
      "Search 219 no docs\n",
      "Search 220 no docs\n",
      "Search 221 no docs\n",
      "Search 222 no docs\n",
      "Search 223 no docs\n",
      "Search 224 no docs\n",
      "Search 225 no docs\n",
      "Search 226 no docs\n",
      "Search 227 no docs\n",
      "Search 228 no docs\n",
      "Search 229 no docs\n",
      "Search 230 no docs\n",
      "Search 231 no docs\n",
      "Search 232 no docs\n",
      "Search 233 no docs\n",
      "Search 234 no docs\n",
      "Search 235 no docs\n",
      "Search 236 no docs\n",
      "Search 237 no docs\n",
      "Search 238 no docs\n",
      "Search 239 no docs\n",
      "Search 240 no docs\n",
      "Search 241 no docs\n",
      "Search 242 no docs\n",
      "Search 243 no docs\n",
      "Search 244 no docs\n",
      "Search 245 no docs\n",
      "Search 246 no docs\n",
      "Search 247 no docs\n",
      "Search 248 no docs\n",
      "Search 249 no docs\n",
      "Search 250 no docs\n",
      "Search 251 no docs\n",
      "Search 252 no docs\n",
      "Search 253 no docs\n",
      "Search 254 no docs\n",
      "Search 255 no docs\n",
      "Search 256 no docs\n",
      "Search 257 no docs\n",
      "Search 258 no docs\n",
      "Search 259 no docs\n",
      "Search 260 no docs\n",
      "Search 261 no docs\n",
      "Search 262 no docs\n",
      "Search 263 no docs\n",
      "Search 264 no docs\n",
      "Search 265 no docs\n",
      "Search 266 no docs\n",
      "Search 267 no docs\n",
      "Search 268 no docs\n",
      "Search 269 no docs\n",
      "Search 270 no docs\n",
      "Search 271 no docs\n",
      "Search 272 no docs\n",
      "Search 273 no docs\n",
      "Search 274 no docs\n",
      "Search 275 no docs\n",
      "Search 276 no docs\n",
      "Search 277 no docs\n",
      "Search 278 no docs\n",
      "Search 279 no docs\n",
      "Search 280 no docs\n",
      "Search 281 no docs\n",
      "Search 282 no docs\n",
      "Search 283 no docs\n",
      "Search 284 no docs\n",
      "Search 285 no docs\n",
      "Search 286 no docs\n",
      "Search 287 no docs\n",
      "Search 288 no docs\n",
      "Search 289 no docs\n",
      "Search 290 no docs\n",
      "Search 291 no docs\n",
      "Search 292 no docs\n",
      "Search 293 no docs\n",
      "Search 294 no docs\n",
      "Search 295 no docs\n",
      "Search 296 no docs\n",
      "Search 297 no docs\n",
      "Search 298 no docs\n",
      "Search 299 no docs\n",
      "Search 300 no docs\n",
      "Search 301 no docs\n",
      "Search 302 no docs\n",
      "Search 303 no docs\n",
      "Search 304 no docs\n",
      "Search 305 no docs\n",
      "Search 306 no docs\n",
      "Search 307 no docs\n",
      "Search 308 no docs\n",
      "Search 309 no docs\n",
      "Search 310 no docs\n",
      "Search 311 no docs\n",
      "Search 312 no docs\n",
      "Search 313 no docs\n",
      "Search 314 no docs\n",
      "Search 315 no docs\n",
      "Search 316 no docs\n",
      "Search 317 no docs\n",
      "Search 318 no docs\n",
      "Search 319 no docs\n",
      "Search 320 no docs\n",
      "Search 321 no docs\n",
      "Search 322 no docs\n",
      "Search 323 no docs\n",
      "Search 324 no docs\n",
      "Search 325 no docs\n",
      "Search 326 no docs\n",
      "Search 327 no docs\n",
      "Search 328 no docs\n",
      "Search 329 no docs\n",
      "Search 330 no docs\n",
      "Search 331 no docs\n",
      "Search 332 no docs\n",
      "Search 333 no docs\n",
      "Search 334 no docs\n",
      "Search 335 no docs\n",
      "Search 336 no docs\n",
      "Search 337 no docs\n",
      "Search 338 no docs\n",
      "Search 339 no docs\n",
      "Search 340 no docs\n",
      "Search 341 no docs\n",
      "Search 342 no docs\n",
      "Search 343 no docs\n",
      "Search 344 no docs\n",
      "Search 345 no docs\n",
      "Search 346 no docs\n",
      "Search 347 no docs\n",
      "Search 348 no docs\n",
      "Search 349 no docs\n",
      "Search 350 no docs\n",
      "Search 351 no docs\n",
      "Search 352 no docs\n",
      "Search 353 no docs\n",
      "Search 354 no docs\n",
      "Search 355 no docs\n",
      "Search 356 no docs\n",
      "Search 357 no docs\n",
      "Search 358 no docs\n",
      "Search 359 no docs\n",
      "Search 360 no docs\n",
      "Search 361 no docs\n",
      "Search 362 no docs\n",
      "Search 363 no docs\n",
      "Search 364 no docs\n",
      "Search 365 no docs\n",
      "Search 366 no docs\n",
      "Search 367 no docs\n",
      "Search 368 no docs\n",
      "Search 369 no docs\n",
      "Search 370 no docs\n",
      "Search 371 no docs\n",
      "Search 372 no docs\n",
      "Search 373 no docs\n",
      "Search 374 no docs\n",
      "Search 375 no docs\n",
      "Search 376 no docs\n",
      "Search 377 no docs\n",
      "Search 378 no docs\n",
      "Search 379 no docs\n",
      "Search 380 no docs\n",
      "Search 381 no docs\n",
      "Search 382 no docs\n",
      "Search 383 no docs\n",
      "Search 384 no docs\n",
      "Search 385 no docs\n",
      "Search 386 no docs\n",
      "Search 387 no docs\n",
      "Search 388 no docs\n",
      "Search 389 no docs\n",
      "Search 390 no docs\n",
      "Search 391 no docs\n",
      "Search 392 no docs\n",
      "Search 393 no docs\n",
      "Search 394 no docs\n",
      "Search 395 no docs\n",
      "Search 396 no docs\n",
      "Search 397 no docs\n",
      "Search 398 no docs\n",
      "Search 399 no docs\n",
      "Search 400 no docs\n",
      "Search 401 no docs\n",
      "Search 402 no docs\n",
      "Search 403 no docs\n",
      "Search 404 no docs\n",
      "Search 405 no docs\n",
      "Search 406 no docs\n",
      "Search 407 no docs\n",
      "Search 408 no docs\n",
      "Search 409 no docs\n",
      "Search 410 no docs\n",
      "Search 411 no docs\n",
      "Search 412 no docs\n",
      "Search 413 no docs\n",
      "Search 414 no docs\n",
      "Search 415 no docs\n",
      "Search 416 no docs\n",
      "Search 417 no docs\n",
      "Search 418 no docs\n",
      "Search 419 no docs\n",
      "Search 420 no docs\n",
      "Search 421 no docs\n",
      "Search 422 no docs\n",
      "Search 423 no docs\n",
      "Search 424 no docs\n",
      "Search 425 no docs\n",
      "Search 426 no docs\n",
      "Search 427 no docs\n",
      "Search 428 no docs\n",
      "Search 429 no docs\n",
      "Search 430 no docs\n",
      "Search 431 no docs\n",
      "Search 432 no docs\n",
      "Search 433 no docs\n",
      "Search 434 no docs\n",
      "Search 435 no docs\n",
      "Search 436 no docs\n",
      "Search 437 no docs\n",
      "Search 438 no docs\n",
      "Search 439 no docs\n",
      "Search 440 no docs\n",
      "Search 441 no docs\n",
      "Search 442 no docs\n",
      "Search 443 no docs\n",
      "Search 444 no docs\n",
      "Search 445 no docs\n",
      "Search 446 no docs\n",
      "Search 447 no docs\n",
      "Search 448 no docs\n",
      "Search 449 no docs\n",
      "Search 450 no docs\n",
      "Search 451 no docs\n",
      "Search 452 no docs\n",
      "Search 453 no docs\n",
      "Search 454 no docs\n",
      "Search 455 no docs\n",
      "Search 456 no docs\n",
      "Search 457 no docs\n",
      "Search 458 no docs\n",
      "Search 459 no docs\n",
      "Search 460 no docs\n",
      "Search 461 no docs\n",
      "Search 462 no docs\n",
      "Search 463 no docs\n",
      "Search 464 no docs\n",
      "Search 465 no docs\n",
      "Search 466 no docs\n",
      "Search 467 no docs\n",
      "Search 468 no docs\n",
      "Search 469 no docs\n",
      "Search 470 no docs\n",
      "Search 471 no docs\n",
      "Search 472 no docs\n",
      "Search 473 no docs\n",
      "Search 474 no docs\n",
      "Search 475 no docs\n",
      "Search 476 no docs\n",
      "Search 477 no docs\n",
      "Search 478 no docs\n",
      "Search 479 no docs\n",
      "Search 480 no docs\n",
      "Search 481 no docs\n",
      "Search 482 no docs\n",
      "Search 483 no docs\n",
      "Search 484 no docs\n",
      "Search 485 no docs\n",
      "Search 486 no docs\n",
      "Search 487 no docs\n",
      "Search 488 no docs\n",
      "Search 489 no docs\n",
      "Search 490 no docs\n",
      "Search 491 no docs\n",
      "Search 492 no docs\n",
      "Search 493 no docs\n",
      "Search 494 no docs\n",
      "Search 495 no docs\n",
      "Search 496 no docs\n",
      "Search 497 no docs\n",
      "Search 498 no docs\n",
      "Search 499 no docs\n",
      "No Results for bluray\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['rank'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 59\u001b[0m\n\u001b[1;32m     55\u001b[0m         wait_for_more_sessions(retrain_frequency)\n\u001b[1;32m     56\u001b[0m         latest_sessions \u001b[38;5;241m=\u001b[39m gather_latest_sessions(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers dvd\u001b[39m\u001b[38;5;124m\"\u001b[39m, latest_sessions,\n\u001b[1;32m     57\u001b[0m                                                  \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexplore_variant_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, explore_features)\n\u001b[0;32m---> 59\u001b[0m \u001b[43mltr_retraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43msessions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[37], line 40\u001b[0m, in \u001b[0;36mltr_retraining_loop\u001b[0;34m(latest_sessions, iterations, retrain_frequency)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     previous_explore_model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexplore_variant_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 40\u001b[0m     exploit_model_evaluation \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexploit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     explore_model_evaluation \u001b[38;5;241m=\u001b[39m evaluate_model(test, previous_explore_model_name, training_data, log\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExploit evaluation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexploit_model_evaluation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 21\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(test_data, model_name, training_data, limit, log)\u001b[0m\n\u001b[1;32m     19\u001b[0m         graded_results \u001b[38;5;241m=\u001b[39m graded_results\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoc_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m log:\n\u001b[0;32m---> 21\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[43mgraded_results\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mindex\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrank\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmanufacturer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshort_description\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                                       \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlong_description\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrade\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     24\u001b[0m         query_results[query] \u001b[38;5;241m=\u001b[39m (graded_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta_grade\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m/\u001b[39m limit)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m query_results\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:5399\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5251\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   5252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m   5253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5260\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5261\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5262\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5263\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5264\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5397\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5398\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5401\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5405\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5406\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5407\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/generic.py:4505\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4503\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4505\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4508\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/generic.py:4546\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4544\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4546\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4547\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4549\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4550\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:6934\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   6933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 6934\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6935\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   6936\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['rank'] not found in axis\""
     ]
    }
   ],
   "source": [
    "random.seed(1234)\n",
    "ltr.delete_feature_store(\"aips_feature_store\")\n",
    "\n",
    "def get_exploit_features():\n",
    "    return [\n",
    "        ltr.generate_fuzzy_query_feature(\"name_fuzzy\", \"name\"),\n",
    "        ltr.generate_query_feature(\"long_description_bm25\", \"long_description\"),\n",
    "        ltr.generate_query_feature(\"short_description_match\", \"short_description\", True)]\n",
    "\n",
    "def gather_latest_sessions(query, sessions, model_name, features):\n",
    "    \"\"\"For the sake of the examples, returns a static list of session data.\n",
    "       In a production environment, this would the most up to date user interactions\"\"\"\n",
    "    training_data = generate_training_data(sessions)\n",
    "    logged_judgments = generate_logged_judgments(training_data, features, model_name)\n",
    "    latest_sessions = generate_simulated_exploration_sessions(query,\n",
    "                                                              sessions,\n",
    "                                                              logged_judgments,\n",
    "                                                              features)\n",
    "    return latest_sessions\n",
    "\n",
    "def is_improvement(evaluation1, evaluation2):\n",
    "    #Model comparison is stubbed out\n",
    "    return True\n",
    "    \n",
    "def wait_for_more_sessions(t):\n",
    "    time.sleep(t)\n",
    "\n",
    "def ltr_retraining_loop(latest_sessions, iterations=sys.maxsize,\n",
    "                        retrain_frequency=60 * 60 * 24):\n",
    "    for i in range(0, iterations):\n",
    "        training_data = generate_training_data(latest_sessions)\n",
    "        train, test = split_training_data(training_data)\n",
    "        if i == 0:\n",
    "            exploit_features = get_exploit_features()\n",
    "            train_and_upload_model(train,\n",
    "                                   \"exploit\",\n",
    "                                   exploit_features)\n",
    "        else:\n",
    "            previous_explore_model_name = f\"explore_variant_{i-1}\"\n",
    "            exploit_model_evaluation = evaluate_model(test, \"exploit\", training_data, log=True)\n",
    "            explore_model_evaluation = evaluate_model(test, previous_explore_model_name, training_data, log=True)\n",
    "            print(f\"Exploit evaluation: {exploit_model_evaluation}\")\n",
    "            print(f\"Explore evaluation: {explore_model_evaluation}\")\n",
    "            if is_improvement(explore_model_evaluation, exploit_model_evaluation):\n",
    "                print(\"Promoting previous explore model\")\n",
    "                train_and_upload_model(train,\n",
    "                                      \"exploit\",\n",
    "                                       explore_features)\n",
    "                \n",
    "        explore_features = get_latest_explore_features()\n",
    "        train_and_upload_model(train,\n",
    "                               f\"explore_variant_{i}\",\n",
    "                               explore_features)\n",
    "        \n",
    "        wait_for_more_sessions(retrain_frequency)\n",
    "        latest_sessions = gather_latest_sessions(\"transformers dvd\", latest_sessions,\n",
    "                                                 f\"explore_variant_{i}\", explore_features)\n",
    "\n",
    "ltr_retraining_loop(sessions, 5, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up next: [Chapter 13: Semantic Search with Dense Vectors](../ch13/1.setting-up-the-outdoors-dataset.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
