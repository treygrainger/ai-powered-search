{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Testing Harness\n",
    "\n",
    "This notebook executes all Jupyter notebooks in the project recursively and reports their completion status.\n",
    "\n",
    "It serves as a testing harness to verify that all notebooks run successfully without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7f97b83fc160>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import nbformat\n",
    "from nbconvert.preprocessors import ExecutePreprocessor\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.driver.memory\", \"7g\")\n",
    "conf.set(\"spark.executor.memory\", \"7g\")\n",
    "conf.set(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "conf.set(\"spark.dynamicAllocation.executorMemoryOverhead\", \"7g\")\n",
    "spark = SparkSession.builder.appName(\"AIPS\").config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KERNEL_NAME = \"python3\"\n",
    "KERNEL_OVERRIDES = {\"1.open-information-extraction.ipynb\": \"ch5-spacy\"}\n",
    "EXCLUDE_DIRS = [\".ipynb_checkpoints\", \"__pycache__\"]\n",
    "EXPORT_RESULTS = False\n",
    "EXCLUDED_NOTEBOOKES = [\"bonus.related-terms-from-documents.ipynb\"\n",
    "                       \"bonus.phrase-detection.ipynb\",\n",
    "                       \"bonus.phrase-detection.ipynb\",\n",
    "                       \"bonus.related-terms-from-documents.ipynb\",\n",
    "                       \"a.defunct.synthesize-search-sessions.ipynb\",\n",
    "                       \"a.synthesize-search-sessions.ipynb\",\n",
    "                       \"a.generate-movie-embeddings.ipynb\",\n",
    "                       \"welcome.ipynb\",\n",
    "                       \"aips-test-suite.ipynb\",\n",
    "                       \"4.train-upload-search-ltr.ipynb\",\n",
    "                       \"ch13-tokenizer-analysis.ipynb\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(seconds):\n",
    "    if seconds < 60:\n",
    "        formatted = f\"{seconds:.2f}s\"\n",
    "    elif seconds < 3600:\n",
    "        formatted =  f\"{seconds / 60:.2f}m\"\n",
    "    else:\n",
    "        formatted =  f\"{seconds / 3600:.2f}h\"\n",
    "    return formatted\n",
    "\n",
    "def export_results(results, filename=\"test_results.json\"):\n",
    "    if results:\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump({\"timestamp\": datetime.now().isoformat(),\n",
    "                       \"results\": results[\"details\"],\n",
    "                       \"summary\": results[\"summary\"]}, f, indent=2)\n",
    "        print(f\"📄 Results exported to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notebook_files(root_dir=\".\", exclude_dirs=None, excluded_notebooks=None):\n",
    "    \"\"\"\n",
    "    Recursively get all .ipynb files in the specified directory and its subdirectories,\n",
    "    excluding specific directories and notebooks.\n",
    "    \n",
    "    Args:\n",
    "        root_dir (str): Root directory to search from\n",
    "        exclude_dirs (list): List of directory names to exclude\n",
    "        exclude_notebooks (list): List of notebook filenames to exclude\n",
    "        \n",
    "    Returns:\n",
    "        list: Sorted list of notebook file paths\n",
    "    \"\"\"\n",
    "    if exclude_dirs is None:\n",
    "        exclude_dirs = []\n",
    "    \n",
    "    if excluded_notebooks is None:\n",
    "        excluded_notebooks = EXCLUDED_NOTEBOOKS\n",
    "        \n",
    "    notebook_files = []\n",
    "    \n",
    "    for directory in os.walk(root_dir):\n",
    "        for path in Path(directory[0]).rglob(\"*.ipynb\"):\n",
    "            if any(exclude_dir in str(path) for exclude_dir in exclude_dirs) or \\\n",
    "                path.name in excluded_notebooks:\n",
    "                continue\n",
    "            if str(path) not in notebook_files:\n",
    "                notebook_files.append(str(path))\n",
    "    \n",
    "    notebook_files = sorted(notebook_files)\n",
    "    return list(map(Path, notebook_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_notebook(notebook_path, timeout=600, kernel_name=\"python3\"):\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "            nb = nbformat.read(f, as_version=4)\n",
    "            ep = ExecutePreprocessor(timeout=timeout, kernel_name=kernel_name)\n",
    "            ep.preprocess(nb, {'metadata': {'path': os.path.dirname(notebook_path)}})\n",
    "        execution_time = (datetime.now() - start_time).total_seconds()\n",
    "        return True, execution_time, None\n",
    "    \n",
    "    except Exception as e:\n",
    "        execution_time = (datetime.now() - start_time).total_seconds()\n",
    "        error_type = type(e).__name__\n",
    "        error_msg = str(e)\n",
    "        # Get traceback for detailed error information\n",
    "        tb = traceback.format_exc()\n",
    "        \n",
    "        return False, execution_time, {\n",
    "            'type': error_type,\n",
    "            'message': error_msg,\n",
    "            'traceback': tb\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_harness(exclude_dirs=None, exclude_notebooks=None, stop_on_failure=False,\n",
    "                     chapter_to_run=None, verbose_errors=True):\n",
    "    \"\"\"\n",
    "    Run the testing harness on all notebooks recursively.\n",
    "    \n",
    "    Args:\n",
    "        root_dir (str): Root directory to search from\n",
    "        timeout (int): Execution timeout in seconds\n",
    "        kernel_name (str): Name of the kernel to use\n",
    "        exclude_dirs (list): List of directory names to exclude\n",
    "        exclude_notebooks (list): List of notebook filenames to exclude\n",
    "        \n",
    "    Returns:\n",
    "        dict: Test results\n",
    "    \"\"\"\n",
    "    root_dir = \".\"\n",
    "    timeout = 600 \n",
    "    print(f\"🔍 Notebook Testing Harness - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"📁 Testing from root directory: {os.path.abspath(root_dir)}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    notebook_files = get_notebook_files(root_dir, exclude_dirs, exclude_notebooks)\n",
    "\n",
    "    if chapter_to_run:\n",
    "        notebook_files = [f for f in notebook_files if (chapter_to_run in str(f))]\n",
    "\n",
    "    print(f\"📋 Found {len(notebook_files)} notebook(s) to test.\")\n",
    "    for nb_file in notebook_files:\n",
    "        # Make paths relative to root_dir for cleaner display\n",
    "        rel_path = os.path.relpath(str(nb_file), root_dir)\n",
    "        print(f\"   • {rel_path}\")\n",
    "    print()\n",
    "    \n",
    "    # Results tracking\n",
    "    results = {\n",
    "        'details': [],\n",
    "        'summary': {\n",
    "            'total': len(notebook_files),\n",
    "            'successful': 0,\n",
    "            'failed': 0,\n",
    "            'total_time': 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Execute each notebook with progress bar\n",
    "    print(\"🚀 Executing notebooks:\")\n",
    "    for notebook_path in tqdm(notebook_files, desc=\"Progress\", colour=\"purple\"):\n",
    "        rel_path = os.path.relpath(str(notebook_path), root_dir)\n",
    "        print(f\"\\n📔 Testing: {rel_path}\")\n",
    "        if \"checkpoint\" in str(notebook_path):\n",
    "            print(f\"\\n📔 Skipping: {rel_path}\")\n",
    "            continue\n",
    "        kernel_name = KERNEL_OVERRIDES.get(notebook_path.name, \"python3\")\n",
    "        success, execution_time, error = execute_notebook(str(notebook_path), timeout, kernel_name)\n",
    "        \n",
    "        results['summary']['total_time'] += execution_time\n",
    "        \n",
    "        if success:\n",
    "            print(f\"   ✅ SUCCESS - Completed in {format_time(execution_time)}\")\n",
    "            results['summary']['successful'] += 1\n",
    "        else:\n",
    "            print(f\"   ❌ FAILED - Error after {format_time(execution_time)}\")\n",
    "            print(f\"      Error: {error['type']}\")\n",
    "            results['summary']['failed'] += 1\n",
    "        \n",
    "        results['details'].append({\n",
    "            'notebook': rel_path,\n",
    "            'success': success,\n",
    "            'execution_time': execution_time,\n",
    "            'error': error\n",
    "        })\n",
    "        if not success and stop_on_failure:\n",
    "            print(\"Terminating test run due to test failure.\")\n",
    "            break\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"📊 SUMMARY:\")\n",
    "    print(f\"   Total notebooks tested: {results['summary']['total']}\")\n",
    "    print(f\"   ✅ Successful: {results['summary']['successful']}\")\n",
    "    print(f\"   ❌ Failed: {results['summary']['failed']}\")\n",
    "    \n",
    "    success_rate = (results['summary']['successful'] / results['summary']['total']) * 100\n",
    "    print(f\"   Success rate: {success_rate:.1f}%\")\n",
    "    print(f\"   Total execution time: {format_time(results['summary']['total_time'])}\")\n",
    "\n",
    "    if results['summary']['failed'] > 0:\n",
    "        print(f\"\\n❌ FAILED NOTEBOOKS:\")\n",
    "        for result in results['details']:\n",
    "            if not result['success']:\n",
    "                if verbose_errors:\n",
    "                    print(f\"   • {result['notebook']}: {result['error']['type']}: {result['error']['message']}\")\n",
    "                else:\n",
    "                    print(f\"   • {result['notebook']}: {result['error']['type']}\")\n",
    "    \n",
    "    print(f\"\\n🏁 Testing completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    if EXPORT_RESULTS and results:\n",
    "        export_results(results, \"AIPS_test_resulsts.json\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Testing Harness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Notebook Testing Harness - 2025-07-28 17:27:16\n",
      "📁 Testing from root directory: /home/jovyan\n",
      "================================================================================\n",
      "📋 Found 2 notebook(s) to test.\n",
      "   • chapters/ch03/1.vectors-and-text-similarity.ipynb\n",
      "   • chapters/ch03/2.controlling-relevance.ipynb\n",
      "\n",
      "🚀 Executing notebooks:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e87b4baddbd48a2baf9f2dd3b82b627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📔 Testing: chapters/ch03/1.vectors-and-text-similarity.ipynb\n",
      "   ✅ SUCCESS - Completed in 1.72s\n",
      "\n",
      "📔 Testing: chapters/ch03/2.controlling-relevance.ipynb\n",
      "   ✅ SUCCESS - Completed in 8.99s\n",
      "\n",
      "================================================================================\n",
      "📊 SUMMARY:\n",
      "   Total notebooks tested: 2\n",
      "   ✅ Successful: 2\n",
      "   ❌ Failed: 0\n",
      "   Success rate: 100.0%\n",
      "   Total execution time: 10.70s\n",
      "\n",
      "🏁 Testing completed at 2025-07-28 17:27:27\n",
      "📄 Results exported to AIPS_test_resulsts.json\n"
     ]
    }
   ],
   "source": [
    "results = run_test_harness(exclude_dirs=EXCLUDE_DIRS, stop_on_failure=True, chapter_to_run=\"ch03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Notebook Testing Harness - 2025-07-28 17:27:27\n",
      "📁 Testing from root directory: /home/jovyan\n",
      "================================================================================\n",
      "📋 Found 2 notebook(s) to test.\n",
      "   • chapters/ch04/1.setting-up-the-retrotech-dataset.ipynb\n",
      "   • chapters/ch04/2.signals-boosting.ipynb\n",
      "\n",
      "🚀 Executing notebooks:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5042957a67d24a018864f4785415b96e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📔 Testing: chapters/ch04/1.setting-up-the-retrotech-dataset.ipynb\n",
      "   ✅ SUCCESS - Completed in 53.77s\n",
      "\n",
      "📔 Testing: chapters/ch04/2.signals-boosting.ipynb\n",
      "   ❌ FAILED - Error after 24.61s\n",
      "      Error: CellExecutionError\n",
      "Terminating test run due to test failure.\n",
      "\n",
      "================================================================================\n",
      "📊 SUMMARY:\n",
      "   Total notebooks tested: 2\n",
      "   ✅ Successful: 1\n",
      "   ❌ Failed: 1\n",
      "   Success rate: 50.0%\n",
      "   Total execution time: 1.31m\n",
      "\n",
      "❌ FAILED NOTEBOOKS:\n",
      "   • chapters/ch04/2.signals-boosting.ipynb: CellExecutionError: An error occurred while executing the following cell:\n",
      "------------------\n",
      "signals_collection = engine.get_collection(\"signals\")\n",
      "print(\"Aggregating Signals to Create Signals Boosts...\")\n",
      "create_view_from_collection(signals_collection, \"signals\")\n",
      "\n",
      "signals_aggregation_query = \"\"\"\n",
      "SELECT q.target AS query, c.target AS doc, COUNT(c.target) AS boost\n",
      "FROM signals c LEFT JOIN signals q ON c.query_id = q.query_id\n",
      "WHERE c.type = 'click' AND q.type = 'query'\n",
      "GROUP BY q.target, doc\n",
      "ORDER BY boost DESC\"\"\"\n",
      "\n",
      "dataframe = spark.sql(signals_aggregation_query)\n",
      "signals_boosting_collection = \\\n",
      "    engine.create_collection(\"signals_boosting\")\n",
      "signals_boosting_collection.write(dataframe)\n",
      "print(\"Signals Aggregation Completed!\")\n",
      "------------------\n",
      "\n",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\n",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m\n",
      "\u001b[1;32m     13\u001b[0m signals_boosting_collection \u001b[38;5;241m=\u001b[39m \\\n",
      "\u001b[1;32m     14\u001b[0m     engine\u001b[38;5;241m.\u001b[39mcreate_collection(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msignals_boosting\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m---> 15\u001b[0m \u001b[43msignals_boosting_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSignals Aggregation Completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "File \u001b[0;32m~/engines/solr/SolrCollection.py:38\u001b[0m, in \u001b[0;36mSolrCollection.write\u001b[0;34m(self, dataframe)\u001b[0m\n",
      "\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit()\n",
      "\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully written \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdataframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m documents\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:804\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    795\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n",
      "\u001b[1;32m    796\u001b[0m \n",
      "\u001b[1;32m    797\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    802\u001b[0m \u001b[38;5;124;03m2\u001b[39;00m\n",
      "\u001b[1;32m    803\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m--> 804\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n",
      "\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n",
      "\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n",
      "\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n",
      "\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n",
      "\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
      "\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n",
      "\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2068\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n",
      "\u001b[1;32m   2065\u001b[0m     traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n",
      "\u001b[1;32m   2066\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;32m-> 2068\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   2069\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n",
      "\u001b[1;32m   2070\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n",
      "\u001b[1;32m   2071\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py:550\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n",
      "\u001b[1;32m    544\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n",
      "\u001b[1;32m    545\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n",
      "\u001b[1;32m    547\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n",
      "\u001b[1;32m    548\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n",
      "\u001b[1;32m    549\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n",
      "\u001b[0;32m--> 550\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n",
      "\u001b[1;32m    551\u001b[0m }\n",
      "\u001b[1;32m    553\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n",
      "\u001b[1;32m    554\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n",
      "\u001b[1;32m    555\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[1;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n",
      "\u001b[0;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n",
      "\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n",
      "\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n",
      "\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n",
      "\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n",
      "\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n",
      "\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n",
      "\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n",
      "\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n",
      "\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n",
      "\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n",
      "\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n",
      "\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "\n",
      "🏁 Testing completed at 2025-07-28 17:28:45\n",
      "📄 Results exported to AIPS_test_resulsts.json\n"
     ]
    }
   ],
   "source": [
    "results = run_test_harness(exclude_dirs=EXCLUDE_DIRS, stop_on_failure=True, chapter_to_run=\"ch04\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Notebook Testing Harness - 2025-07-28 17:28:45\n",
      "📁 Testing from root directory: /home/jovyan\n",
      "================================================================================\n",
      "📋 Found 3 notebook(s) to test.\n",
      "   • chapters/ch05/1.open-information-extraction.ipynb\n",
      "   • chapters/ch05/2.index-datasets.ipynb\n",
      "   • chapters/ch05/3.semantic-knowledge-graph.ipynb\n",
      "\n",
      "🚀 Executing notebooks:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "317343250bda483a8cbd44621b0e6a39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📔 Testing: chapters/ch05/1.open-information-extraction.ipynb\n",
      "   ✅ SUCCESS - Completed in 10.30s\n",
      "\n",
      "📔 Testing: chapters/ch05/2.index-datasets.ipynb\n",
      "   ✅ SUCCESS - Completed in 2.25m\n",
      "\n",
      "📔 Testing: chapters/ch05/3.semantic-knowledge-graph.ipynb\n",
      "   ✅ SUCCESS - Completed in 11.63s\n",
      "\n",
      "================================================================================\n",
      "📊 SUMMARY:\n",
      "   Total notebooks tested: 3\n",
      "   ✅ Successful: 3\n",
      "   ❌ Failed: 0\n",
      "   Success rate: 100.0%\n",
      "   Total execution time: 2.62m\n",
      "\n",
      "🏁 Testing completed at 2025-07-28 17:31:22\n",
      "📄 Results exported to AIPS_test_resulsts.json\n"
     ]
    }
   ],
   "source": [
    "results = run_test_harness(exclude_dirs=EXCLUDE_DIRS, stop_on_failure=True, chapter_to_run=\"ch05\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Notebook Testing Harness - 2025-07-28 17:31:22\n",
      "📁 Testing from root directory: /home/jovyan\n",
      "================================================================================\n",
      "📋 Found 3 notebook(s) to test.\n",
      "   • chapters/ch06/1.skg-classification-disambiguation.ipynb\n",
      "   • chapters/ch06/2.related-keywords-from-signals.ipynb\n",
      "   • chapters/ch06/3.spell-correction.ipynb\n",
      "\n",
      "🚀 Executing notebooks:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff0a21593e14f35bbb7377c8bf413dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📔 Testing: chapters/ch06/1.skg-classification-disambiguation.ipynb\n",
      "   ✅ SUCCESS - Completed in 6.84s\n",
      "\n",
      "📔 Testing: chapters/ch06/2.related-keywords-from-signals.ipynb\n",
      "   ✅ SUCCESS - Completed in 41.25s\n",
      "\n",
      "📔 Testing: chapters/ch06/3.spell-correction.ipynb\n",
      "   ✅ SUCCESS - Completed in 2.38m\n",
      "\n",
      "================================================================================\n",
      "📊 SUMMARY:\n",
      "   Total notebooks tested: 3\n",
      "   ✅ Successful: 3\n",
      "   ❌ Failed: 0\n",
      "   Success rate: 100.0%\n",
      "   Total execution time: 3.18m\n",
      "\n",
      "🏁 Testing completed at 2025-07-28 17:34:33\n",
      "📄 Results exported to AIPS_test_resulsts.json\n"
     ]
    }
   ],
   "source": [
    "results = run_test_harness(exclude_dirs=EXCLUDE_DIRS, stop_on_failure=True, chapter_to_run=\"ch06\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Notebook Testing Harness - 2025-07-28 17:34:33\n",
      "📁 Testing from root directory: /home/jovyan\n",
      "================================================================================\n",
      "📋 Found 2 notebook(s) to test.\n",
      "   • chapters/ch07/1.index-datasets.ipynb\n",
      "   • chapters/ch07/2.semantic-search.ipynb\n",
      "\n",
      "🚀 Executing notebooks:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8f7c73f5a0c447682abc5432af047dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📔 Testing: chapters/ch07/1.index-datasets.ipynb\n",
      "   ✅ SUCCESS - Completed in 1.76m\n",
      "\n",
      "📔 Testing: chapters/ch07/2.semantic-search.ipynb\n",
      "   ✅ SUCCESS - Completed in 21.00s\n",
      "\n",
      "================================================================================\n",
      "📊 SUMMARY:\n",
      "   Total notebooks tested: 2\n",
      "   ✅ Successful: 2\n",
      "   ❌ Failed: 0\n",
      "   Success rate: 100.0%\n",
      "   Total execution time: 2.11m\n",
      "\n",
      "🏁 Testing completed at 2025-07-28 17:36:40\n",
      "📄 Results exported to AIPS_test_resulsts.json\n"
     ]
    }
   ],
   "source": [
    "results = run_test_harness(exclude_dirs=EXCLUDE_DIRS, stop_on_failure=True, chapter_to_run=\"ch07\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Notebook Testing Harness - 2025-07-28 17:36:40\n",
      "📁 Testing from root directory: /home/jovyan\n",
      "================================================================================\n",
      "📋 Found 1 notebook(s) to test.\n",
      "   • chapters/ch08/1.signals-boosting.ipynb\n",
      "\n",
      "🚀 Executing notebooks:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec6f92ea3ac4930b87dec76fb679c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📔 Testing: chapters/ch08/1.signals-boosting.ipynb\n",
      "   ❌ FAILED - Error after 1.53m\n",
      "      Error: CellExecutionError\n",
      "Terminating test run due to test failure.\n",
      "\n",
      "================================================================================\n",
      "📊 SUMMARY:\n",
      "   Total notebooks tested: 1\n",
      "   ✅ Successful: 0\n",
      "   ❌ Failed: 1\n",
      "   Success rate: 0.0%\n",
      "   Total execution time: 1.53m\n",
      "\n",
      "❌ FAILED NOTEBOOKS:\n",
      "   • chapters/ch08/1.signals-boosting.ipynb: CellExecutionError: An error occurred while executing the following cell:\n",
      "------------------\n",
      "#One Signal per User - Anti-Spam\n",
      "#Sometimes needs rerunning\n",
      "mixed_signal_types_aggregation_query = \"\"\"\n",
      "SELECT query, doc, ((1 * click_boost) + (10 * add_to_cart_boost) +\n",
      "                    (25 * purchase_boost)) AS boost FROM (\n",
      "  SELECT query, doc, \n",
      "    SUM(click) AS click_boost,\n",
      "    SUM(add_to_cart) AS add_to_cart_boost,\n",
      "    SUM(purchase) AS purchase_boost FROM (  \n",
      "      SELECT lower(q.target) AS query, cap.target AS doc, \n",
      "        IF(cap.type = 'click', 1, 0) AS click, \n",
      "        IF(cap.type = 'add-to-cart', 1, 0) AS  add_to_cart, \n",
      "        IF(cap.type = 'purchase', 1, 0) AS purchase\n",
      "      FROM signals cap LEFT JOIN signals q on cap.query_id = q.query_id\n",
      "      WHERE (cap.type != 'query' AND q.type = 'query')\n",
      "    ) AS raw_signals\n",
      "  GROUP BY query, doc) AS per_type_boosts\"\"\"\n",
      "\n",
      "type_weighted_collection = \\\n",
      "  aggregate_signals(signals_collection, \"signals_boosts_weighted_types\",\n",
      "                    mixed_signal_types_aggregation_query)\n",
      "------------------\n",
      "\n",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\n",
      "Cell \u001b[0;32mIn[19], line 20\u001b[0m\n",
      "\u001b[1;32m      3\u001b[0m mixed_signal_types_aggregation_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n",
      "\u001b[1;32m      4\u001b[0m \u001b[38;5;124mSELECT query, doc, ((1 * click_boost) + (10 * add_to_cart_boost) +\u001b[39m\n",
      "\u001b[1;32m      5\u001b[0m \u001b[38;5;124m                    (25 * purchase_boost)) AS boost FROM (\u001b[39m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m     16\u001b[0m \u001b[38;5;124m    ) AS raw_signals\u001b[39m\n",
      "\u001b[1;32m     17\u001b[0m \u001b[38;5;124m  GROUP BY query, doc) AS per_type_boosts\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n",
      "\u001b[1;32m     19\u001b[0m type_weighted_collection \u001b[38;5;241m=\u001b[39m \\\n",
      "\u001b[0;32m---> 20\u001b[0m   \u001b[43maggregate_signals\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignals_collection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msignals_boosts_weighted_types\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     21\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmixed_signal_types_aggregation_query\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m, in \u001b[0;36maggregate_signals\u001b[0;34m(signals_collection, collection_name, signals_agg_query)\u001b[0m\n",
      "\u001b[1;32m      4\u001b[0m collection \u001b[38;5;241m=\u001b[39m engine\u001b[38;5;241m.\u001b[39mcreate_collection(collection_name)\n",
      "\u001b[0;32m----> 5\u001b[0m \u001b[43mcollection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrom_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignals_agg_query\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m      6\u001b[0m collection\u001b[38;5;241m.\u001b[39mcommit()\n",
      "\n",
      "File \u001b[0;32m~/engines/solr/SolrCollection.py:36\u001b[0m, in \u001b[0;36mSolrCollection.write\u001b[0;34m(self, dataframe)\u001b[0m\n",
      "\u001b[1;32m     34\u001b[0m opts \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzkhost\u001b[39m\u001b[38;5;124m\"\u001b[39m: AIPS_ZK_HOST, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollection\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n",
      "\u001b[1;32m     35\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgen_uniq_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommit_within\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5000\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n",
      "\u001b[0;32m---> 36\u001b[0m \u001b[43mdataframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msolr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit()\n",
      "\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:966\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n",
      "\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;32m--> 966\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n",
      "\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n",
      "\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n",
      "\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n",
      "\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n",
      "\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
      "\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n",
      "\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2068\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n",
      "\u001b[1;32m   2065\u001b[0m     traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n",
      "\u001b[1;32m   2066\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;32m-> 2068\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   2069\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n",
      "\u001b[1;32m   2070\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n",
      "\u001b[1;32m   2071\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py:550\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n",
      "\u001b[1;32m    544\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n",
      "\u001b[1;32m    545\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n",
      "\u001b[1;32m    547\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n",
      "\u001b[1;32m    548\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n",
      "\u001b[1;32m    549\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n",
      "\u001b[0;32m--> 550\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n",
      "\u001b[1;32m    551\u001b[0m }\n",
      "\u001b[1;32m    553\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n",
      "\u001b[1;32m    554\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n",
      "\u001b[1;32m    555\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[1;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n",
      "\u001b[0;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n",
      "\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n",
      "\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n",
      "\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n",
      "\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n",
      "\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n",
      "\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n",
      "\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n",
      "\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n",
      "\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n",
      "\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n",
      "\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n",
      "\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "\n",
      "🏁 Testing completed at 2025-07-28 17:38:12\n",
      "📄 Results exported to AIPS_test_resulsts.json\n"
     ]
    }
   ],
   "source": [
    "results = run_test_harness(exclude_dirs=EXCLUDE_DIRS, stop_on_failure=True, chapter_to_run=\"ch08\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Notebook Testing Harness - 2025-07-28 17:38:12\n",
      "📁 Testing from root directory: /home/jovyan\n",
      "================================================================================\n",
      "📋 Found 2 notebook(s) to test.\n",
      "   • chapters/ch09/1.personalization.ipynb\n",
      "   • chapters/ch09/2.embedding-based-personalization.ipynb\n",
      "\n",
      "🚀 Executing notebooks:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb407e30bfc54e75aa3f4502c8d17044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📔 Testing: chapters/ch09/1.personalization.ipynb\n",
      "   ❌ FAILED - Error after 1.43m\n",
      "      Error: CellExecutionError\n",
      "Terminating test run due to test failure.\n",
      "\n",
      "================================================================================\n",
      "📊 SUMMARY:\n",
      "   Total notebooks tested: 2\n",
      "   ✅ Successful: 0\n",
      "   ❌ Failed: 1\n",
      "   Success rate: 0.0%\n",
      "   Total execution time: 1.43m\n",
      "\n",
      "❌ FAILED NOTEBOOKS:\n",
      "   • chapters/ch09/1.personalization.ipynb: CellExecutionError: An error occurred while executing the following cell:\n",
      "------------------\n",
      "from pyspark.ml.evaluation import RegressionEvaluator\n",
      "from pyspark.ml.recommendation import ALS\n",
      "from pyspark.sql import Row\n",
      "\n",
      "random.seed(0)\n",
      "\n",
      "als = ALS(maxIter=3, rank=10, regParam=0.15, implicitPrefs=True,\n",
      "          userCol=\"userIndex\", itemCol=\"productIndex\", ratingCol=\"rating\",\n",
      "          coldStartStrategy=\"drop\", seed=0)\n",
      "\n",
      "(training_data, test_data) = user_prefs.randomSplit([0.95, 0.05], 0)\n",
      "training_data = strings_to_indexes(training_data, user_indexer, product_indexer)\n",
      "test_data = strings_to_indexes(test_data, user_indexer, product_indexer)\n",
      "\n",
      "print(\"Beginning model training\")\n",
      "model = als.fit(training_data)\n",
      "\n",
      "print(\"Beginning predictions\")\n",
      "predictions = model.transform(test_data)\n",
      "\n",
      "print(\"Beginning evaluation\")\n",
      "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
      "                                predictionCol=\"prediction\")\n",
      "rmse = evaluator.evaluate(predictions)\n",
      "print(f\"Root-mean-square error = {rmse}\")\n",
      "------------------\n",
      "\n",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\n",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m\n",
      "\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBeginning model training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m---> 16\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mals\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBeginning predictions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n",
      "\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:383\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n",
      "\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n",
      "\u001b[0;32m--> 383\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    384\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n",
      "\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:380\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n",
      "\u001b[1;32m    379\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n",
      "\u001b[0;32m--> 380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n",
      "\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n",
      "\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n",
      "\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n",
      "\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n",
      "\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
      "\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n",
      "\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2068\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n",
      "\u001b[1;32m   2065\u001b[0m     traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n",
      "\u001b[1;32m   2066\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;32m-> 2068\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   2069\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n",
      "\u001b[1;32m   2070\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n",
      "\u001b[1;32m   2071\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py:550\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n",
      "\u001b[1;32m    544\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n",
      "\u001b[1;32m    545\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n",
      "\u001b[1;32m    547\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n",
      "\u001b[1;32m    548\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n",
      "\u001b[1;32m    549\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n",
      "\u001b[0;32m--> 550\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n",
      "\u001b[1;32m    551\u001b[0m }\n",
      "\u001b[1;32m    553\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n",
      "\u001b[1;32m    554\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n",
      "\u001b[1;32m    555\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[1;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n",
      "\u001b[0;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n",
      "\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n",
      "\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n",
      "\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n",
      "\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n",
      "\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n",
      "\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n",
      "\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n",
      "\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n",
      "\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n",
      "\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n",
      "\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n",
      "\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "\n",
      "🏁 Testing completed at 2025-07-28 17:39:37\n",
      "📄 Results exported to AIPS_test_resulsts.json\n"
     ]
    }
   ],
   "source": [
    "results = run_test_harness(exclude_dirs=EXCLUDE_DIRS, stop_on_failure=True, chapter_to_run=\"ch09\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Notebook Testing Harness - 2025-07-28 17:39:37\n",
      "📁 Testing from root directory: /home/jovyan\n",
      "================================================================================\n",
      "📋 Found 4 notebook(s) to test.\n",
      "   • chapters/ch10/1.setup-the-movie-db.ipynb\n",
      "   • chapters/ch10/2.judgments-and-logging.ipynb\n",
      "   • chapters/ch10/3.pairwise-transform.ipynb\n",
      "   • chapters/ch10/4.train-and-evaluate-the-model.ipynb\n",
      "\n",
      "🚀 Executing notebooks:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa0879560d2f4dabab1aa19b179d0221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📔 Testing: chapters/ch10/1.setup-the-movie-db.ipynb\n",
      "   ✅ SUCCESS - Completed in 34.02s\n",
      "\n",
      "📔 Testing: chapters/ch10/2.judgments-and-logging.ipynb\n",
      "   ✅ SUCCESS - Completed in 3.06s\n",
      "\n",
      "📔 Testing: chapters/ch10/3.pairwise-transform.ipynb\n",
      "   ✅ SUCCESS - Completed in 4.27s\n",
      "\n",
      "📔 Testing: chapters/ch10/4.train-and-evaluate-the-model.ipynb\n",
      "   ✅ SUCCESS - Completed in 4.04s\n",
      "\n",
      "================================================================================\n",
      "📊 SUMMARY:\n",
      "   Total notebooks tested: 4\n",
      "   ✅ Successful: 4\n",
      "   ❌ Failed: 0\n",
      "   Success rate: 100.0%\n",
      "   Total execution time: 45.40s\n",
      "\n",
      "🏁 Testing completed at 2025-07-28 17:40:23\n",
      "📄 Results exported to AIPS_test_resulsts.json\n"
     ]
    }
   ],
   "source": [
    "results = run_test_harness(exclude_dirs=EXCLUDE_DIRS, stop_on_failure=True, chapter_to_run=\"ch10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Notebook Testing Harness - 2025-07-28 17:40:23\n",
      "📁 Testing from root directory: /home/jovyan\n",
      "================================================================================\n",
      "📋 Found 3 notebook(s) to test.\n",
      "   • chapters/ch11/1.click-through-rate-judgments.ipynb\n",
      "   • chapters/ch11/2.sdbn-judgments-to-overcome-position-bias.ipynb\n",
      "   • chapters/ch11/3.SDBN-Confidence-Bias.ipynb\n",
      "\n",
      "🚀 Executing notebooks:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1395cbf8bbd24e58a51b212b34e3d365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📔 Testing: chapters/ch11/1.click-through-rate-judgments.ipynb\n",
      "   ✅ SUCCESS - Completed in 8.84s\n",
      "\n",
      "📔 Testing: chapters/ch11/2.sdbn-judgments-to-overcome-position-bias.ipynb\n",
      "   ✅ SUCCESS - Completed in 4.99s\n",
      "\n",
      "📔 Testing: chapters/ch11/3.SDBN-Confidence-Bias.ipynb\n",
      "   ✅ SUCCESS - Completed in 6.66s\n",
      "\n",
      "================================================================================\n",
      "📊 SUMMARY:\n",
      "   Total notebooks tested: 3\n",
      "   ✅ Successful: 3\n",
      "   ❌ Failed: 0\n",
      "   Success rate: 100.0%\n",
      "   Total execution time: 20.50s\n",
      "\n",
      "🏁 Testing completed at 2025-07-28 17:40:43\n",
      "📄 Results exported to AIPS_test_resulsts.json\n"
     ]
    }
   ],
   "source": [
    "results = run_test_harness(exclude_dirs=EXCLUDE_DIRS, stop_on_failure=True, chapter_to_run=\"ch11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Notebook Testing Harness - 2025-07-28 17:40:43\n",
      "📁 Testing from root directory: /home/jovyan\n",
      "================================================================================\n",
      "📋 Found 2 notebook(s) to test.\n",
      "   • chapters/ch12/0.setup.ipynb\n",
      "   • chapters/ch12/1.ab-testing-to-active-learning.ipynb\n",
      "\n",
      "🚀 Executing notebooks:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "768f93379c6d4d8f9465bb52662f9fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📔 Testing: chapters/ch12/0.setup.ipynb\n",
      "   ✅ SUCCESS - Completed in 18.83s\n",
      "\n",
      "📔 Testing: chapters/ch12/1.ab-testing-to-active-learning.ipynb\n",
      "   ❌ FAILED - Error after 7.43s\n",
      "      Error: CellExecutionError\n",
      "Terminating test run due to test failure.\n",
      "\n",
      "================================================================================\n",
      "📊 SUMMARY:\n",
      "   Total notebooks tested: 2\n",
      "   ✅ Successful: 1\n",
      "   ❌ Failed: 1\n",
      "   Success rate: 50.0%\n",
      "   Total execution time: 26.27s\n",
      "\n",
      "❌ FAILED NOTEBOOKS:\n",
      "   • chapters/ch12/1.ab-testing-to-active-learning.ipynb: CellExecutionError: An error occurred while executing the following cell:\n",
      "------------------\n",
      "random.seed(1234)\n",
      "feature_set = [\n",
      "    ltr.generate_query_feature(feature_name=\"long_description_bm25\",\n",
      "                               field_name=\"long_description\"),\n",
      "    ltr.generate_query_feature(feature_name=\"short_description_constant\",\n",
      "                               field_name=\"short_description\",\n",
      "                               constant_score=True)]\n",
      "\n",
      "evaluation = train_and_evaluate_model(sessions, \"ltr_model_variant_1\", feature_set)\n",
      "display(evaluation)\n",
      "------------------\n",
      "\n",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m1234\u001b[39m)\n",
      "\u001b[1;32m      2\u001b[0m feature_set \u001b[38;5;241m=\u001b[39m [\n",
      "\u001b[1;32m      3\u001b[0m     ltr\u001b[38;5;241m.\u001b[39mgenerate_query_feature(feature_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlong_description_bm25\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[1;32m      4\u001b[0m                                field_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlong_description\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n",
      "\u001b[1;32m      5\u001b[0m     ltr\u001b[38;5;241m.\u001b[39mgenerate_query_feature(feature_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshort_description_constant\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[1;32m      6\u001b[0m                                field_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshort_description\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
      "\u001b[1;32m      7\u001b[0m                                constant_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)]\n",
      "\u001b[0;32m----> 9\u001b[0m evaluation \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msessions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mltr_model_variant_1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_set\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     10\u001b[0m display(evaluation)\n",
      "\n",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m, in \u001b[0;36mtrain_and_evaluate_model\u001b[0;34m(sessions, model_name, features, log)\u001b[0m\n",
      "\u001b[1;32m      2\u001b[0m training_data \u001b[38;5;241m=\u001b[39m generate_training_data(sessions)\n",
      "\u001b[1;32m      3\u001b[0m train, test \u001b[38;5;241m=\u001b[39m split_training_data(training_data, \u001b[38;5;241m0.8\u001b[39m)\n",
      "\u001b[0;32m----> 4\u001b[0m \u001b[43mtrain_and_upload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m      5\u001b[0m evaluation \u001b[38;5;241m=\u001b[39m evaluate_model(test, model_name, training_data, log\u001b[38;5;241m=\u001b[39mlog)\n",
      "\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m evaluation\n",
      "\n",
      "Cell \u001b[0;32mIn[9], line 27\u001b[0m, in \u001b[0;36mtrain_and_upload_model\u001b[0;34m(training_data, model_name, features, log)\u001b[0m\n",
      "\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m qid, query_judgments \u001b[38;5;129;01min\u001b[39;00m groupby(judgments, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m j: j\u001b[38;5;241m.\u001b[39mqid):\n",
      "\u001b[1;32m     24\u001b[0m     ftr_logger\u001b[38;5;241m.\u001b[39mlog_for_qid(judgments\u001b[38;5;241m=\u001b[39mquery_judgments, \n",
      "\u001b[1;32m     25\u001b[0m                            qid\u001b[38;5;241m=\u001b[39mqid, log\u001b[38;5;241m=\u001b[39mlog)\n",
      "\u001b[0;32m---> 27\u001b[0m linear_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_svm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mftr_logger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogged\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     28\u001b[0m ltr\u001b[38;5;241m.\u001b[39mupload_model(linear_model, log\u001b[38;5;241m=\u001b[39mlog)\n",
      "\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m linear_model\n",
      "\n",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m, in \u001b[0;36mtrain_svm_model\u001b[0;34m(model_name, features, logged_judgments)\u001b[0m\n",
      "\u001b[1;32m      3\u001b[0m feature_deltas, predictor_deltas \u001b[38;5;241m=\u001b[39m pairwise_transform(normed_judgments)\n",
      "\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m svm\u001b[38;5;241m.\u001b[39mLinearSVC(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_deltas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictor_deltas\u001b[49m\u001b[43m)\u001b[49m \n",
      "\u001b[1;32m      8\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m [ftr[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m ftr \u001b[38;5;129;01min\u001b[39;00m features]\n",
      "\u001b[1;32m      9\u001b[0m linear_model \u001b[38;5;241m=\u001b[39m ltr\u001b[38;5;241m.\u001b[39mgenerate_model(model_name, feature_names,\n",
      "\u001b[1;32m     10\u001b[0m                                   means, std_devs, model\u001b[38;5;241m.\u001b[39mcoef_[\u001b[38;5;241m0\u001b[39m])\n",
      "\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/svm/_classes.py:263\u001b[0m, in \u001b[0;36mLinearSVC.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n",
      "\u001b[1;32m    238\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model according to the given training data.\u001b[39;00m\n",
      "\u001b[1;32m    239\u001b[0m \n",
      "\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    259\u001b[0m \u001b[38;5;124;03m    An instance of the estimator.\u001b[39;00m\n",
      "\u001b[1;32m    260\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m    261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n",
      "\u001b[0;32m--> 263\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    270\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    271\u001b[0m check_classification_targets(y)\n",
      "\u001b[1;32m    272\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n",
      "\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n",
      "\u001b[1;32m    563\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n",
      "\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m--> 565\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    566\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n",
      "\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:1106\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n",
      "\u001b[1;32m   1101\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n",
      "\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n",
      "\u001b[1;32m   1103\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m   1104\u001b[0m     )\n",
      "\u001b[0;32m-> 1106\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1111\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1120\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1122\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n",
      "\u001b[1;32m   1124\u001b[0m check_consistent_length(X, y)\n",
      "\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n",
      "\u001b[1;32m    915\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n",
      "\u001b[1;32m    916\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    917\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n",
      "\u001b[1;32m    918\u001b[0m         )\n",
      "\u001b[1;32m    920\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n",
      "\u001b[0;32m--> 921\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    922\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    923\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    924\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    925\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;32m    929\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n",
      "\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n",
      "\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n",
      "\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n",
      "\u001b[1;32m    147\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n",
      "\u001b[1;32m    148\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    160\u001b[0m     )\n",
      "\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\n",
      "LinearSVC does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "ValueError: Input X contains NaN.\n",
      "LinearSVC does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "\n",
      "🏁 Testing completed at 2025-07-28 17:41:10\n",
      "📄 Results exported to AIPS_test_resulsts.json\n"
     ]
    }
   ],
   "source": [
    "results = run_test_harness(exclude_dirs=EXCLUDE_DIRS, stop_on_failure=True, chapter_to_run=\"ch12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Notebook Testing Harness - 2025-07-28 17:41:10\n",
      "📁 Testing from root directory: /home/jovyan\n",
      "================================================================================\n",
      "📋 Found 6 notebook(s) to test.\n",
      "   • chapters/ch13/1.setting-up-the-outdoors-dataset.ipynb\n",
      "   • chapters/ch13/2.introduction-to-transformers.ipynb\n",
      "   • chapters/ch13/3.natural-language-autocomplete.ipynb\n",
      "   • chapters/ch13/4.semantic-search.ipynb\n",
      "   • chapters/ch13/5.quantization.ipynb\n",
      "   • chapters/ch13/ch13-tokenizer-analysis.ipynb\n",
      "\n",
      "🚀 Executing notebooks:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d729c860c8411e9bbbf0a1059030ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📔 Testing: chapters/ch13/1.setting-up-the-outdoors-dataset.ipynb\n",
      "   ✅ SUCCESS - Completed in 55.09s\n",
      "\n",
      "📔 Testing: chapters/ch13/2.introduction-to-transformers.ipynb\n",
      "   ✅ SUCCESS - Completed in 24.07s\n",
      "\n",
      "📔 Testing: chapters/ch13/3.natural-language-autocomplete.ipynb\n",
      "   ✅ SUCCESS - Completed in 4.34m\n",
      "\n",
      "📔 Testing: chapters/ch13/4.semantic-search.ipynb\n",
      "   ✅ SUCCESS - Completed in 1.49m\n",
      "\n",
      "📔 Testing: chapters/ch13/5.quantization.ipynb\n",
      "   ✅ SUCCESS - Completed in 1.27h\n",
      "\n",
      "📔 Testing: chapters/ch13/ch13-tokenizer-analysis.ipynb\n",
      "   ❌ FAILED - Error after 5.02s\n",
      "      Error: CellExecutionError\n",
      "Terminating test run due to test failure.\n",
      "\n",
      "================================================================================\n",
      "📊 SUMMARY:\n",
      "   Total notebooks tested: 6\n",
      "   ✅ Successful: 5\n",
      "   ❌ Failed: 1\n",
      "   Success rate: 83.3%\n",
      "   Total execution time: 1.39h\n",
      "\n",
      "❌ FAILED NOTEBOOKS:\n",
      "   • chapters/ch13/ch13-tokenizer-analysis.ipynb: CellExecutionError: An error occurred while executing the following cell:\n",
      "------------------\n",
      "from transformers import BertTokenizer\n",
      "tokenizer = BertTokenizer(vocab_file=\"./bert-vocab.txt\")\n",
      "tok_ids = tokenizer.encode(\"it's raining hard\")\n",
      "tokenset = tokenizer.convert_ids_to_tokens(tok_ids,skip_special_tokens=True)\n",
      "print(tokenset)\n",
      "------------------\n",
      "\n",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer\n",
      "\u001b[0;32m----> 2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mBertTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./bert-vocab.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m      3\u001b[0m tok_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms raining hard\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m      4\u001b[0m tokenset \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mconvert_ids_to_tokens(tok_ids,skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bert/tokenization_bert.py:115\u001b[0m, in \u001b[0;36mBertTokenizer.__init__\u001b[0;34m(self, vocab_file, do_lower_case, do_basic_tokenize, never_split, unk_token, sep_token, pad_token, cls_token, mask_token, tokenize_chinese_chars, strip_accents, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n",
      "\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n",
      "\u001b[1;32m     99\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n",
      "\u001b[1;32m    100\u001b[0m     vocab_file,\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    112\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n",
      "\u001b[1;32m    113\u001b[0m ):\n",
      "\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(vocab_file):\n",
      "\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n",
      "\u001b[1;32m    116\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a vocabulary file at path \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvocab_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. To load the vocabulary from a Google pretrained\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    117\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    118\u001b[0m         )\n",
      "\u001b[1;32m    119\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab \u001b[38;5;241m=\u001b[39m load_vocab(vocab_file)\n",
      "\u001b[1;32m    120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mids_to_tokens \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mOrderedDict([(ids, tok) \u001b[38;5;28;01mfor\u001b[39;00m tok, ids \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mitems()])\n",
      "\n",
      "\u001b[0;31mValueError\u001b[0m: Can't find a vocabulary file at path './bert-vocab.txt'. To load the vocabulary from a Google pretrained model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\n",
      "ValueError: Can't find a vocabulary file at path './bert-vocab.txt'. To load the vocabulary from a Google pretrained model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\n",
      "\n",
      "\n",
      "🏁 Testing completed at 2025-07-28 19:04:41\n",
      "📄 Results exported to AIPS_test_resulsts.json\n"
     ]
    }
   ],
   "source": [
    "results = run_test_harness(exclude_dirs=EXCLUDE_DIRS, stop_on_failure=True, chapter_to_run=\"ch13\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Notebook Testing Harness - 2025-07-28 19:04:41\n",
      "📁 Testing from root directory: /home/jovyan\n",
      "================================================================================\n",
      "📋 Found 4 notebook(s) to test.\n",
      "   • chapters/ch14/1.question-answering-visualizer.ipynb\n",
      "   • chapters/ch14/2.question-answering-data-preparation.ipynb\n",
      "   • chapters/ch14/3.question-answering-fine-tuning.ipynb\n",
      "   • chapters/ch14/4.question-answering-demo-application.ipynb\n",
      "\n",
      "🚀 Executing notebooks:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6511214983164ac1bfeb119325acc3ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📔 Testing: chapters/ch14/1.question-answering-visualizer.ipynb\n",
      "   ✅ SUCCESS - Completed in 21.56s\n",
      "\n",
      "📔 Testing: chapters/ch14/2.question-answering-data-preparation.ipynb\n",
      "   ✅ SUCCESS - Completed in 4.31m\n",
      "\n",
      "📔 Testing: chapters/ch14/3.question-answering-fine-tuning.ipynb\n",
      "   ❌ FAILED - Error after 4.32s\n",
      "      Error: CellExecutionError\n",
      "Terminating test run due to test failure.\n",
      "\n",
      "================================================================================\n",
      "📊 SUMMARY:\n",
      "   Total notebooks tested: 4\n",
      "   ✅ Successful: 2\n",
      "   ❌ Failed: 1\n",
      "   Success rate: 50.0%\n",
      "   Total execution time: 4.75m\n",
      "\n",
      "❌ FAILED NOTEBOOKS:\n",
      "   • chapters/ch14/3.question-answering-fine-tuning.ipynb: CellExecutionError: An error occurred while executing the following cell:\n",
      "------------------\n",
      "#This method adopted from the following example notebook:\n",
      "#https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb\n",
      "#Copyright 2021, Huggingface.  Apache 2.0 license.\n",
      "import datasets\n",
      "\n",
      "file = \"data/question-answering/question-answering-training-set\"\n",
      "datadict = datasets.load_from_disk(file)\n",
      "\n",
      "def tokenize_dataset(examples):\n",
      "\n",
      "    maximum_tokens = 384 # This will be the number of tokens in BOTH the question and context\n",
      "    document_overlap = 128 # Sometimes we need to split the context into smaller chunks, so we will overlap with this window\n",
      "    pad_on_right = tokenizer.padding_side == \"right\"\n",
      "    \n",
      "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
      "    # in one example possible giving several features when a context is long, each of those features having a\n",
      "    # context that overlaps a bit the context of the previous feature.\n",
      "    tokenized_examples = tokenizer(\n",
      "        examples[\"question\" if pad_on_right else \"context\"],\n",
      "        examples[\"context\" if pad_on_right else \"question\"],\n",
      "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
      "        max_length=maximum_tokens,\n",
      "        stride=document_overlap,\n",
      "        return_overflowing_tokens=True,\n",
      "        return_offsets_mapping=True,\n",
      "        padding=\"max_length\"\n",
      "    )\n",
      "    \n",
      "    print(tokenized_examples[0])\n",
      "\n",
      "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
      "    # its corresponding example. This key gives us just that.\n",
      "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
      "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
      "    # help us compute the start_positions and end_positions.\n",
      "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
      "\n",
      "    # Let's label those examples!\n",
      "    tokenized_examples[\"start_positions\"] = []\n",
      "    tokenized_examples[\"end_positions\"] = []\n",
      "\n",
      "    for i, offsets in enumerate(offset_mapping):\n",
      "        # We will label impossible answers with the index of the CLS token.\n",
      "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
      "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
      "\n",
      "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
      "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
      "\n",
      "        # One example can give several spans, this is the index of the example containing this span of text.\n",
      "        sample_index = sample_mapping[i]\n",
      "        answers = examples[\"answers\"][sample_index]\n",
      "        # If no answers are given, set the cls_index as answer.\n",
      "        if len(answers[\"answer_start\"]) == 0:\n",
      "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
      "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
      "        else:\n",
      "            # Start/end character index of the answer in the text.\n",
      "            start_char = answers[\"answer_start\"][0]\n",
      "            end_char = start_char + len(answers[\"text\"][0])\n",
      "\n",
      "            # Start token index of the current span in the text.\n",
      "            token_start_index = 0\n",
      "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
      "                token_start_index += 1\n",
      "\n",
      "            # End token index of the current span in the text.\n",
      "            token_end_index = len(input_ids) - 1\n",
      "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
      "                token_end_index -= 1\n",
      "\n",
      "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
      "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
      "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
      "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
      "            else:\n",
      "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
      "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
      "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
      "                    token_start_index += 1\n",
      "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
      "                while offsets[token_end_index][1] >= end_char:\n",
      "                    token_end_index -= 1\n",
      "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
      "\n",
      "    return tokenized_examples\n",
      "\"\"\"\n",
      "To apply this function on all the sentences (or pairs of sentences) in our dataset, we just use the map method of our dataset object we created earlier. \n",
      "This will apply the function on all the elements of all the splits in dataset, so our training, validation and testing data will be preprocessed in one single command. \n",
      "Since our preprocessing changes the number of samples, we need to remove the old columns when applying it.\n",
      " --Huggingface\n",
      "\"\"\"\n",
      "tokenized_datasets = datadict.map(tokenize_dataset, batched=True, remove_columns=datadict[\"train\"].column_names)\n",
      "------------------\n",
      "\n",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n",
      "\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\n",
      "\u001b[1;32m      6\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/question-answering/question-answering-training-set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m----> 7\u001b[0m datadict \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_dataset\u001b[39m(examples):\n",
      "\u001b[1;32m     11\u001b[0m     maximum_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m384\u001b[39m \u001b[38;5;66;03m# This will be the number of tokens in BOTH the question and context\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/load.py:2152\u001b[0m, in \u001b[0;36mload_from_disk\u001b[0;34m(dataset_path, keep_in_memory, storage_options)\u001b[0m\n",
      "\u001b[1;32m   2150\u001b[0m fs, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;241m=\u001b[39m url_to_fs(dataset_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(storage_options \u001b[38;5;129;01mor\u001b[39;00m {}))\n",
      "\u001b[1;32m   2151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mexists(dataset_path):\n",
      "\u001b[0;32m-> 2152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirectory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m   2153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fs\u001b[38;5;241m.\u001b[39misfile(posixpath\u001b[38;5;241m.\u001b[39mjoin(dataset_path, config\u001b[38;5;241m.\u001b[39mDATASET_INFO_FILENAME)) \u001b[38;5;129;01mand\u001b[39;00m fs\u001b[38;5;241m.\u001b[39misfile(\n",
      "\u001b[1;32m   2154\u001b[0m     posixpath\u001b[38;5;241m.\u001b[39mjoin(dataset_path, config\u001b[38;5;241m.\u001b[39mDATASET_STATE_JSON_FILENAME)\n",
      "\u001b[1;32m   2155\u001b[0m ):\n",
      "\u001b[1;32m   2156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39mload_from_disk(dataset_path, keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory, storage_options\u001b[38;5;241m=\u001b[39mstorage_options)\n",
      "\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Directory data/question-answering/question-answering-training-set not found\n",
      "FileNotFoundError: Directory data/question-answering/question-answering-training-set not found\n",
      "\n",
      "\n",
      "🏁 Testing completed at 2025-07-28 19:09:26\n",
      "📄 Results exported to AIPS_test_resulsts.json\n"
     ]
    }
   ],
   "source": [
    "results = run_test_harness(exclude_dirs=EXCLUDE_DIRS, stop_on_failure=True, chapter_to_run=\"ch14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Notebook Testing Harness - 2025-07-28 19:09:26\n",
      "📁 Testing from root directory: /home/jovyan\n",
      "================================================================================\n",
      "📋 Found 2 notebook(s) to test.\n",
      "   • chapters/ch15/1.llm-exploration.ipynb\n",
      "   • chapters/ch15/2.multimodal-and-hybrid-search.ipynb\n",
      "\n",
      "🚀 Executing notebooks:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "867c82622ebf4b86bdeb9d202a0e6c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📔 Testing: chapters/ch15/1.llm-exploration.ipynb\n",
      "   ❌ FAILED - Error after 7.46s\n",
      "      Error: CellExecutionError\n",
      "Terminating test run due to test failure.\n",
      "\n",
      "================================================================================\n",
      "📊 SUMMARY:\n",
      "   Total notebooks tested: 2\n",
      "   ✅ Successful: 0\n",
      "   ❌ Failed: 1\n",
      "   Success rate: 0.0%\n",
      "   Total execution time: 7.46s\n",
      "\n",
      "❌ FAILED NOTEBOOKS:\n",
      "   • chapters/ch15/1.llm-exploration.ipynb: CellExecutionError: An error occurred while executing the following cell:\n",
      "------------------\n",
      "r = get_generative_response(\"What is a unicorn?\")\n",
      "------------------\n",
      "\n",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mget_generative_response\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is a unicorn?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\n",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m, in \u001b[0;36mget_generative_response\u001b[0;34m(prompt)\u001b[0m\n",
      "\u001b[1;32m      4\u001b[0m response \u001b[38;5;241m=\u001b[39m mockedGenerativeResponses\u001b[38;5;241m.\u001b[39mloc[mockedGenerativeResponses[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n",
      "\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(response) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;32m      6\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'''\u001b[39m\n",
      "\u001b[1;32m      7\u001b[0m \u001b[38;5;124m    <div style=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflow: auto; margin-bottom: 10px;\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\n",
      "\u001b[1;32m      8\u001b[0m \u001b[38;5;124m      <h3 style=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat: left; width: 100px; margin-right: 10px;\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>Query:</h3>\u001b[39m\n",
      "\u001b[0;32m----> 9\u001b[0m \u001b[38;5;124m      <p style=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflow: hidden; margin-left: 110px;\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m><pre>\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mhtml\u001b[49m\u001b[38;5;241m.\u001b[39mescape(prompt)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m<pre></p>\u001b[39m\n",
      "\u001b[1;32m     10\u001b[0m \u001b[38;5;124m    </div>\u001b[39m\n",
      "\u001b[1;32m     11\u001b[0m \n",
      "\u001b[1;32m     12\u001b[0m \u001b[38;5;124m    <div style=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflow: auto; margin-bottom: 10px;\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\n",
      "\u001b[1;32m     13\u001b[0m \u001b[38;5;124m      <h3 style=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat: left; width: 100px; margin-right: 10px;\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>Response:</h3>\u001b[39m\n",
      "\u001b[1;32m     14\u001b[0m \u001b[38;5;124m      <p style=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflow: hidden; margin-left: 110px;\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m><pre>\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhtml\u001b[38;5;241m.\u001b[39mescape(response[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m</pre></p>\u001b[39m\n",
      "\u001b[1;32m     15\u001b[0m \u001b[38;5;124m    </div>\u001b[39m\n",
      "\u001b[1;32m     16\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m'''\u001b[39m\n",
      "\u001b[1;32m     17\u001b[0m     display(HTML(output))\n",
      "\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[38;5;241m0\u001b[39m]\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'html' is not defined\n",
      "NameError: name 'html' is not defined\n",
      "\n",
      "\n",
      "🏁 Testing completed at 2025-07-28 19:09:34\n",
      "📄 Results exported to AIPS_test_resulsts.json\n"
     ]
    }
   ],
   "source": [
    "results = run_test_harness(exclude_dirs=EXCLUDE_DIRS, stop_on_failure=True, chapter_to_run=\"ch15\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results (Optional)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
