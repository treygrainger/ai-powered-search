{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Testing Harness\n",
    "\n",
    "This notebook executes all Jupyter notebooks in the project recursively and reports their completion status.\n",
    "\n",
    "It serves as a testing harness to verify that all notebooks run successfully without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aips.spark import get_spark_session\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import nbformat\n",
    "from nbconvert.preprocessors import ExecutePreprocessor\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "spark = get_spark_session()\n",
    "\n",
    "from aips import set_engine; set_engine(\"opensearch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "KERNEL_NAME = \"python3\"\n",
    "KERNEL_OVERRIDES = {\"1.open-information-extraction.ipynb\": \"ch5-spacy\"}\n",
    "EXCLUDE_DIRS = [\".ipynb_checkpoints\", \"__pycache__\"]\n",
    "EXPORT_RESULTS = False\n",
    "EXCLUDED_NOTEBOOKS = [\"bonus.related-terms-from-documents.ipynb\"\n",
    "                      \"bonus.phrase-detection.ipynb\",\n",
    "                      \"bonus.phrase-detection.ipynb\",\n",
    "                      \"bonus.related-terms-from-documents.ipynb\",\n",
    "                      \"a.defunct.synthesize-search-sessions.ipynb\",\n",
    "                      \"a.synthesize-search-sessions.ipynb\",\n",
    "                      \"a.generate-movie-embeddings.ipynb\",\n",
    "                      \"welcome.ipynb\",\n",
    "                      \"aips-test-suite.ipynb\",\n",
    "                      \"4.train-upload-search-ltr.ipynb\",\n",
    "                      \"ch13-tokenizer-analysis.ipynb\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(seconds):\n",
    "    if seconds < 60:\n",
    "        formatted = f\"{seconds:.2f}s\"\n",
    "    elif seconds < 3600:\n",
    "        formatted =  f\"{seconds / 60:.2f}m\"\n",
    "    else:\n",
    "        formatted =  f\"{seconds / 3600:.2f}h\"\n",
    "    return formatted\n",
    "\n",
    "def export_results(results, filename=\"test_results.json\"):\n",
    "    if results:\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump({\"timestamp\": datetime.now().isoformat(),\n",
    "                       \"results\": results[\"details\"],\n",
    "                       \"summary\": results[\"summary\"]}, f, indent=2)\n",
    "        print(f\"üìÑ Results exported to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notebook_files(root_dir=\".\", exclude_dirs=None, excluded_notebooks=None):\n",
    "    \"\"\"\n",
    "    Recursively get all .ipynb files in the specified directory and its subdirectories,\n",
    "    excluding specific directories and notebooks.\n",
    "    \n",
    "    Args:\n",
    "        root_dir (str): Root directory to search from\n",
    "        exclude_dirs (list): List of directory names to exclude\n",
    "        exclude_notebooks (list): List of notebook filenames to exclude\n",
    "        \n",
    "    Returns:\n",
    "        list: Sorted list of notebook file paths\n",
    "    \"\"\"\n",
    "    if exclude_dirs is None:\n",
    "        exclude_dirs = EXCLUDE_DIRS\n",
    "    \n",
    "    if excluded_notebooks is None:\n",
    "        excluded_notebooks = EXCLUDED_NOTEBOOKS\n",
    "        \n",
    "    notebook_files = []\n",
    "    \n",
    "    for directory in os.walk(root_dir):\n",
    "        for path in Path(directory[0]).rglob(\"*.ipynb\"):\n",
    "            if any(exclude_dir in str(path) for exclude_dir in exclude_dirs) or \\\n",
    "                path.name in excluded_notebooks:\n",
    "                continue\n",
    "            if str(path) not in notebook_files:\n",
    "                notebook_files.append(str(path))\n",
    "    \n",
    "    notebook_files = sorted(notebook_files)\n",
    "    return list(map(Path, notebook_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_notebook(notebook_path, timeout=600, kernel_name=\"python3\"):\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "            nb = nbformat.read(f, as_version=4)\n",
    "            ep = ExecutePreprocessor(timeout=timeout, kernel_name=kernel_name)\n",
    "            ep.preprocess(nb, {'metadata': {'path': os.path.dirname(notebook_path)}})\n",
    "        execution_time = (datetime.now() - start_time).total_seconds()\n",
    "        return True, execution_time, None\n",
    "    \n",
    "    except Exception as e:\n",
    "        execution_time = (datetime.now() - start_time).total_seconds()\n",
    "        error_type = type(e).__name__\n",
    "        error_msg = str(e)\n",
    "        # Get traceback for detailed error information\n",
    "        tb = traceback.format_exc()\n",
    "        \n",
    "        return False, execution_time, {\n",
    "            'type': error_type,\n",
    "            'message': error_msg,\n",
    "            'traceback': tb\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_harness(exclude_dirs=None, exclude_notebooks=None, stop_on_failure=True,\n",
    "                     chapter_to_run=None, verbose_errors=True):\n",
    "    root_dir = \".\"\n",
    "    timeout = 600 \n",
    "    print(f\"üîç Notebook Testing Harness - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"üìÅ Testing from root directory: {os.path.abspath(root_dir)}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    notebook_files = get_notebook_files(root_dir, exclude_dirs, exclude_notebooks)\n",
    "\n",
    "    if chapter_to_run:\n",
    "        notebook_files = [f for f in notebook_files if (chapter_to_run in str(f))]\n",
    "\n",
    "    print(f\"üìã Found {len(notebook_files)} notebook(s) to test.\")\n",
    "    for nb_file in notebook_files:\n",
    "        rel_path = os.path.relpath(str(nb_file), root_dir)\n",
    "        print(f\"   ‚Ä¢ {rel_path}\")\n",
    "    print()\n",
    "    \n",
    "    results = {\n",
    "        'details': [],\n",
    "        'summary': {\n",
    "            'total': len(notebook_files),\n",
    "            'successful': 0,\n",
    "            'failed': 0,\n",
    "            'total_time': 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"üöÄ Executing notebooks:\")\n",
    "    for notebook_path in tqdm(notebook_files, desc=\"Progress\", colour=\"purple\"):\n",
    "        rel_path = os.path.relpath(str(notebook_path), root_dir)\n",
    "        print(f\"\\nüìî Testing: {rel_path}\")\n",
    "        if \"checkpoint\" in str(notebook_path):\n",
    "            print(f\"\\nüìî Skipping: {rel_path}\")\n",
    "            continue\n",
    "        kernel_name = KERNEL_OVERRIDES.get(notebook_path.name, \"python3\")\n",
    "        success, execution_time, error = execute_notebook(str(notebook_path), timeout, kernel_name)\n",
    "        \n",
    "        results['summary']['total_time'] += execution_time\n",
    "        \n",
    "        if success:\n",
    "            print(f\"   ‚úÖ SUCCESS - Completed in {format_time(execution_time)}\")\n",
    "            results['summary']['successful'] += 1\n",
    "        else:\n",
    "            print(f\"   ‚ùå FAILED - Error after {format_time(execution_time)}\")\n",
    "            print(f\"      Error: {error['type']}\")\n",
    "            results['summary']['failed'] += 1\n",
    "        \n",
    "        results['details'].append({\n",
    "            'notebook': rel_path,\n",
    "            'success': success,\n",
    "            'execution_time': execution_time,\n",
    "            'error': error\n",
    "        })\n",
    "        if not success and stop_on_failure:\n",
    "            print(\"Terminating test run due to test failure.\")\n",
    "            break\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"üìä SUMMARY:\")\n",
    "    print(f\"   Total notebooks tested: {results['summary']['total']}\")\n",
    "    print(f\"   ‚úÖ Successful: {results['summary']['successful']}\")\n",
    "    if results['summary']['failed'] > 0:\n",
    "        print(f\"   ‚ùå Failed: {results['summary']['failed']}\")\n",
    "    \n",
    "    success_rate = (results['summary']['successful'] / results['summary']['total']) * 100\n",
    "    print(f\"   Success rate: {success_rate:.1f}%\")\n",
    "    print(f\"   Total execution time: {format_time(results['summary']['total_time'])}\")\n",
    "\n",
    "    if results['summary']['failed'] > 0:\n",
    "        print(f\"\\n‚ùå FAILED NOTEBOOKS:\")\n",
    "        for result in results['details']:\n",
    "            if not result['success']:\n",
    "                if verbose_errors:\n",
    "                    print(f\"   ‚Ä¢ {result['notebook']}: {result['error']['type']}: {result['error']['message']}\")\n",
    "                else:\n",
    "                    print(f\"   ‚Ä¢ {result['notebook']}: {result['error']['type']}\")\n",
    "    \n",
    "    print(f\"\\nüèÅ Testing completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    if EXPORT_RESULTS and results:\n",
    "        export_results(results, \"AIPS_test_resulsts.json\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Notebook Testing Harness - 2025-09-07 23:16:36\n",
      "üìÅ Testing from root directory: /home/jovyan\n",
      "================================================================================\n",
      "üìã Found 4 notebook(s) to test.\n",
      "   ‚Ä¢ chapters/ch14/1.question-answering-visualizer.ipynb\n",
      "   ‚Ä¢ chapters/ch14/2.question-answering-data-preparation.ipynb\n",
      "   ‚Ä¢ chapters/ch14/3.question-answering-fine-tuning.ipynb\n",
      "   ‚Ä¢ chapters/ch14/4.question-answering-demo-application.ipynb\n",
      "\n",
      "üöÄ Executing notebooks:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc3e58ff3f740028dd3d99622dab593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìî Testing: chapters/ch14/1.question-answering-visualizer.ipynb\n",
      "   ‚úÖ SUCCESS - Completed in 8.63s\n",
      "\n",
      "üìî Testing: chapters/ch14/2.question-answering-data-preparation.ipynb\n",
      "   ‚úÖ SUCCESS - Completed in 4.68m\n",
      "\n",
      "üìî Testing: chapters/ch14/3.question-answering-fine-tuning.ipynb\n",
      "   ‚úÖ SUCCESS - Completed in 36.39m\n",
      "\n",
      "üìî Testing: chapters/ch14/4.question-answering-demo-application.ipynb\n",
      "   ‚úÖ SUCCESS - Completed in 19.44s\n",
      "\n",
      "================================================================================\n",
      "üìä SUMMARY:\n",
      "   Total notebooks tested: 4\n",
      "   ‚úÖ Successful: 4\n",
      "   Success rate: 100.0%\n",
      "   Total execution time: 41.54m\n",
      "\n",
      "üèÅ Testing completed at 2025-09-07 23:58:08\n"
     ]
    }
   ],
   "source": [
    "# Last cell in chapter 3 will fail for most non-solr notebookes without {!funct} translation capabilities\n",
    "results = run_test_harness(chapter_to_run=\"ch03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Notebook Testing Harness - 2025-09-07 23:58:08\n",
      "üìÅ Testing from root directory: /home/jovyan\n",
      "================================================================================\n",
      "üìã Found 2 notebook(s) to test.\n",
      "   ‚Ä¢ chapters/ch04/1.setting-up-the-retrotech-dataset.ipynb\n",
      "   ‚Ä¢ chapters/ch04/2.signals-boosting.ipynb\n",
      "\n",
      "üöÄ Executing notebooks:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6330f55a40174ab7a8367a55108fdfc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìî Testing: chapters/ch04/1.setting-up-the-retrotech-dataset.ipynb\n",
      "   ‚úÖ SUCCESS - Completed in 22.69s\n",
      "\n",
      "üìî Testing: chapters/ch04/2.signals-boosting.ipynb\n",
      "   ‚úÖ SUCCESS - Completed in 49.99s\n",
      "\n",
      "================================================================================\n",
      "üìä SUMMARY:\n",
      "   Total notebooks tested: 2\n",
      "   ‚úÖ Successful: 2\n",
      "   Success rate: 100.0%\n",
      "   Total execution time: 1.21m\n",
      "\n",
      "üèÅ Testing completed at 2025-09-07 23:59:21\n"
     ]
    }
   ],
   "source": [
    "results = run_test_harness(chapter_to_run=\"ch04\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Notebook Testing Harness - 2025-09-07 23:59:21\n",
      "üìÅ Testing from root directory: /home/jovyan\n",
      "================================================================================\n",
      "üìã Found 3 notebook(s) to test.\n",
      "   ‚Ä¢ chapters/ch05/1.open-information-extraction.ipynb\n",
      "   ‚Ä¢ chapters/ch05/2.index-datasets.ipynb\n",
      "   ‚Ä¢ chapters/ch05/3.semantic-knowledge-graph.ipynb\n",
      "\n",
      "üöÄ Executing notebooks:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d63c2a58b9b4c15bf6b1c5c53aca704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìî Testing: chapters/ch05/1.open-information-extraction.ipynb\n",
      "   ‚úÖ SUCCESS - Completed in 9.10s\n",
      "\n",
      "üìî Testing: chapters/ch05/2.index-datasets.ipynb\n",
      "   ‚úÖ SUCCESS - Completed in 55.42s\n",
      "\n",
      "üìî Testing: chapters/ch05/3.semantic-knowledge-graph.ipynb\n",
      "   ‚úÖ SUCCESS - Completed in 1.04m\n",
      "\n",
      "================================================================================\n",
      "üìä SUMMARY:\n",
      "   Total notebooks tested: 3\n",
      "   ‚úÖ Successful: 3\n",
      "   Success rate: 100.0%\n",
      "   Total execution time: 2.11m\n",
      "\n",
      "üèÅ Testing completed at 2025-09-08 00:01:28\n"
     ]
    }
   ],
   "source": [
    "results = run_test_harness(chapter_to_run=\"ch05\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Notebook Testing Harness - 2025-09-08 00:01:28\n",
      "üìÅ Testing from root directory: /home/jovyan\n",
      "================================================================================\n",
      "üìã Found 3 notebook(s) to test.\n",
      "   ‚Ä¢ chapters/ch06/1.skg-classification-disambiguation.ipynb\n",
      "   ‚Ä¢ chapters/ch06/2.related-keywords-from-signals.ipynb\n",
      "   ‚Ä¢ chapters/ch06/3.spell-correction.ipynb\n",
      "\n",
      "üöÄ Executing notebooks:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d67bfa38da74428dac3ded2c81249249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìî Testing: chapters/ch06/1.skg-classification-disambiguation.ipynb\n",
      "   ‚úÖ SUCCESS - Completed in 32.83s\n",
      "\n",
      "üìî Testing: chapters/ch06/2.related-keywords-from-signals.ipynb\n",
      "   ‚úÖ SUCCESS - Completed in 3.38m\n",
      "\n",
      "üìî Testing: chapters/ch06/3.spell-correction.ipynb\n",
      "   ‚úÖ SUCCESS - Completed in 3.27m\n",
      "\n",
      "================================================================================\n",
      "üìä SUMMARY:\n",
      "   Total notebooks tested: 3\n",
      "   ‚úÖ Successful: 3\n",
      "   Success rate: 100.0%\n",
      "   Total execution time: 7.20m\n",
      "\n",
      "üèÅ Testing completed at 2025-09-08 00:08:40\n"
     ]
    }
   ],
   "source": [
    "results = run_test_harness(chapter_to_run=\"ch06\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Notebook Testing Harness - 2025-09-08 00:08:40\n",
      "üìÅ Testing from root directory: /home/jovyan\n",
      "================================================================================\n",
      "üìã Found 2 notebook(s) to test.\n",
      "   ‚Ä¢ chapters/ch07/1.index-datasets.ipynb\n",
      "   ‚Ä¢ chapters/ch07/2.semantic-search.ipynb\n",
      "\n",
      "üöÄ Executing notebooks:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ac76312a88941ee9a65e5ac77b6f760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìî Testing: chapters/ch07/1.index-datasets.ipynb\n",
      "   ‚úÖ SUCCESS - Completed in 1.31m\n",
      "\n",
      "üìî Testing: chapters/ch07/2.semantic-search.ipynb\n",
      "   ‚úÖ SUCCESS - Completed in 25.76s\n",
      "\n",
      "================================================================================\n",
      "üìä SUMMARY:\n",
      "   Total notebooks tested: 2\n",
      "   ‚úÖ Successful: 2\n",
      "   Success rate: 100.0%\n",
      "   Total execution time: 1.74m\n",
      "\n",
      "üèÅ Testing completed at 2025-09-08 00:10:24\n"
     ]
    }
   ],
   "source": [
    "results = run_test_harness(chapter_to_run=\"ch07\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Notebook Testing Harness - 2025-09-08 00:10:24\n",
      "üìÅ Testing from root directory: /home/jovyan\n",
      "================================================================================\n",
      "üìã Found 1 notebook(s) to test.\n",
      "   ‚Ä¢ chapters/ch08/1.signals-boosting.ipynb\n",
      "\n",
      "üöÄ Executing notebooks:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "313c00d8bd75468ea7f896d129273f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìî Testing: chapters/ch08/1.signals-boosting.ipynb\n",
      "   ‚ùå FAILED - Error after 4.41m\n",
      "      Error: CellExecutionError\n",
      "Terminating test run due to test failure.\n",
      "\n",
      "================================================================================\n",
      "üìä SUMMARY:\n",
      "   Total notebooks tested: 1\n",
      "   ‚úÖ Successful: 0\n",
      "   ‚ùå Failed: 1\n",
      "   Success rate: 0.0%\n",
      "   Total execution time: 4.41m\n",
      "\n",
      "‚ùå FAILED NOTEBOOKS:\n",
      "   ‚Ä¢ chapters/ch08/1.signals-boosting.ipynb: CellExecutionError: An error occurred while executing the following cell:\n",
      "------------------\n",
      "from aips.data_loaders import index_time_boosts\n",
      "\n",
      "boosts_collection = engine.get_collection(\"normalized_signals_boosts\")\n",
      "boosted_products_collection = \\\n",
      "    engine.get_collection(\"products_with_signals_boosts\")\n",
      "\n",
      "boosted_products = index_time_boosts.load_dataframe(\n",
      "    products_collection,\n",
      "    boosts_collection)\n",
      "\n",
      "boosted_products_collection.write(boosted_products)\n",
      "------------------\n",
      "\n",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[23], line 11\u001b[0m\n",
      "\u001b[1;32m      4\u001b[0m boosted_products_collection \u001b[38;5;241m=\u001b[39m \\\n",
      "\u001b[1;32m      5\u001b[0m     engine\u001b[38;5;241m.\u001b[39mget_collection(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproducts_with_signals_boosts\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m      7\u001b[0m boosted_products \u001b[38;5;241m=\u001b[39m index_time_boosts\u001b[38;5;241m.\u001b[39mload_dataframe(\n",
      "\u001b[1;32m      8\u001b[0m     products_collection,\n",
      "\u001b[1;32m      9\u001b[0m     boosts_collection)\n",
      "\u001b[0;32m---> 11\u001b[0m \u001b[43mboosted_products_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboosted_products\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/engines/opensearch/OpenSearchCollection.py:99\u001b[0m, in \u001b[0;36mOpenSearchCollection.write\u001b[0;34m(self, dataframe, overwrite)\u001b[0m\n",
      "\u001b[1;32m     97\u001b[0m     opts[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopensearch.mapping.id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid_field\n",
      "\u001b[1;32m     98\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m overwrite \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m---> 99\u001b[0m \u001b[43mdataframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mopensearch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit()\n",
      "\u001b[1;32m    101\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully written \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataframe\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m documents\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:968\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n",
      "\u001b[1;32m    966\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n",
      "\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m--> 968\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n",
      "\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n",
      "\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n",
      "\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n",
      "\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n",
      "\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n",
      "\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n",
      "\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n",
      "\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n",
      "\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n",
      "\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
      "\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n",
      "\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n",
      "\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
      "\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o861.save.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 235.0 failed 1 times, most recent failure: Lost task 3.0 in stage 235.0 (TID 933) (3e38557cbbe9 executor driver): org.opensearch.hadoop.rest.OpenSearchHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[172.21.0.4:9200]] \n",
      "\tat org.opensearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:170)\n",
      "\tat org.opensearch.hadoop.rest.RestClient.execute(RestClient.java:443)\n",
      "\tat org.opensearch.hadoop.rest.RestClient.execute(RestClient.java:439)\n",
      "\tat org.opensearch.hadoop.rest.RestClient.execute(RestClient.java:419)\n",
      "\tat org.opensearch.hadoop.rest.RestClient.bulk(RestClient.java:251)\n",
      "\tat org.opensearch.hadoop.rest.bulk.BulkProcessor.tryFlush(BulkProcessor.java:225)\n",
      "\tat org.opensearch.hadoop.rest.bulk.BulkProcessor.flush(BulkProcessor.java:528)\n",
      "\tat org.opensearch.hadoop.rest.bulk.BulkProcessor.add(BulkProcessor.java:156)\n",
      "\tat org.opensearch.hadoop.rest.RestRepository.doWriteToIndex(RestRepository.java:199)\n",
      "\tat org.opensearch.hadoop.rest.RestRepository.writeToIndex(RestRepository.java:179)\n",
      "\tat org.opensearch.spark.rdd.OpenSearchRDDWriter.write(OpenSearchRDDWriter.scala:89)\n",
      "\tat org.opensearch.spark.sql.OpenSearchSparkSQL$.$anonfun$saveToOpenSearch$1(OpenSearchSparkSQL.scala:113)\n",
      "\tat org.opensearch.spark.sql.OpenSearchSparkSQL$.$anonfun$saveToOpenSearch$1$adapted(OpenSearchSparkSQL.scala:113)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2281)\n",
      "\tat org.opensearch.spark.sql.OpenSearchSparkSQL$.saveToOpenSearch(OpenSearchSparkSQL.scala:113)\n",
      "\tat org.opensearch.spark.sql.OpenSearchRelation.insert(DefaultSource.scala:563)\n",
      "\tat org.opensearch.spark.sql.DefaultSource.createRelation(DefaultSource.scala:107)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.opensearch.hadoop.rest.OpenSearchHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[172.21.0.4:9200]] \n",
      "\tat org.opensearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:170)\n",
      "\tat org.opensearch.hadoop.rest.RestClient.execute(RestClient.java:443)\n",
      "\tat org.opensearch.hadoop.rest.RestClient.execute(RestClient.java:439)\n",
      "\tat org.opensearch.hadoop.rest.RestClient.execute(RestClient.java:419)\n",
      "\tat org.opensearch.hadoop.rest.RestClient.bulk(RestClient.java:251)\n",
      "\tat org.opensearch.hadoop.rest.bulk.BulkProcessor.tryFlush(BulkProcessor.java:225)\n",
      "\tat org.opensearch.hadoop.rest.bulk.BulkProcessor.flush(BulkProcessor.java:528)\n",
      "\tat org.opensearch.hadoop.rest.bulk.BulkProcessor.add(BulkProcessor.java:156)\n",
      "\tat org.opensearch.hadoop.rest.RestRepository.doWriteToIndex(RestRepository.java:199)\n",
      "\tat org.opensearch.hadoop.rest.RestRepository.writeToIndex(RestRepository.java:179)\n",
      "\tat org.opensearch.spark.rdd.OpenSearchRDDWriter.write(OpenSearchRDDWriter.scala:89)\n",
      "\tat org.opensearch.spark.sql.OpenSearchSparkSQL$.$anonfun$saveToOpenSearch$1(OpenSearchSparkSQL.scala:113)\n",
      "\tat org.opensearch.spark.sql.OpenSearchSparkSQL$.$anonfun$saveToOpenSearch$1$adapted(OpenSearchSparkSQL.scala:113)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\t... 1 more\n",
      "\n",
      "Py4JJavaError: An error occurred while calling o861.save.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 235.0 failed 1 times, most recent failure: Lost task 3.0 in stage 235.0 (TID 933) (3e38557cbbe9 executor driver): org.opensearch.hadoop.rest.OpenSearchHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[172.21.0.4:9200]] \n",
      "\tat org.opensearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:170)\n",
      "\tat org.opensearch.hadoop.rest.RestClient.execute(RestClient.java:443)\n",
      "\tat org.opensearch.hadoop.rest.RestClient.execute(RestClient.java:439)\n",
      "\tat org.opensearch.hadoop.rest.RestClient.execute(RestClient.java:419)\n",
      "\tat org.opensearch.hadoop.rest.RestClient.bulk(RestClient.java:251)\n",
      "\tat org.opensearch.hadoop.rest.bulk.BulkProcessor.tryFlush(BulkProcessor.java:225)\n",
      "\tat org.opensearch.hadoop.rest.bulk.BulkProcessor.flush(BulkProcessor.java:528)\n",
      "\tat org.opensearch.hadoop.rest.bulk.BulkProcessor.add(BulkProcessor.java:156)\n",
      "\tat org.opensearch.hadoop.rest.RestRepository.doWriteToIndex(RestRepository.java:199)\n",
      "\tat org.opensearch.hadoop.rest.RestRepository.writeToIndex(RestRepository.java:179)\n",
      "\tat org.opensearch.spark.rdd.OpenSearchRDDWriter.write(OpenSearchRDDWriter.scala:89)\n",
      "\tat org.opensearch.spark.sql.OpenSearchSparkSQL$.$anonfun$saveToOpenSearch$1(OpenSearchSparkSQL.scala:113)\n",
      "\tat org.opensearch.spark.sql.OpenSearchSparkSQL$.$anonfun$saveToOpenSearch$1$adapted(OpenSearchSparkSQL.scala:113)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2281)\n",
      "\tat org.opensearch.spark.sql.OpenSearchSparkSQL$.saveToOpenSearch(OpenSearchSparkSQL.scala:113)\n",
      "\tat org.opensearch.spark.sql.OpenSearchRelation.insert(DefaultSource.scala:563)\n",
      "\tat org.opensearch.spark.sql.DefaultSource.createRelation(DefaultSource.scala:107)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.opensearch.hadoop.rest.OpenSearchHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[172.21.0.4:9200]] \n",
      "\tat org.opensearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:170)\n",
      "\tat org.opensearch.hadoop.rest.RestClient.execute(RestClient.java:443)\n",
      "\tat org.opensearch.hadoop.rest.RestClient.execute(RestClient.java:439)\n",
      "\tat org.opensearch.hadoop.rest.RestClient.execute(RestClient.java:419)\n",
      "\tat org.opensearch.hadoop.rest.RestClient.bulk(RestClient.java:251)\n",
      "\tat org.opensearch.hadoop.rest.bulk.BulkProcessor.tryFlush(BulkProcessor.java:225)\n",
      "\tat org.opensearch.hadoop.rest.bulk.BulkProcessor.flush(BulkProcessor.java:528)\n",
      "\tat org.opensearch.hadoop.rest.bulk.BulkProcessor.add(BulkProcessor.java:156)\n",
      "\tat org.opensearch.hadoop.rest.RestRepository.doWriteToIndex(RestRepository.java:199)\n",
      "\tat org.opensearch.hadoop.rest.RestRepository.writeToIndex(RestRepository.java:179)\n",
      "\tat org.opensearch.spark.rdd.OpenSearchRDDWriter.write(OpenSearchRDDWriter.scala:89)\n",
      "\tat org.opensearch.spark.sql.OpenSearchSparkSQL$.$anonfun$saveToOpenSearch$1(OpenSearchSparkSQL.scala:113)\n",
      "\tat org.opensearch.spark.sql.OpenSearchSparkSQL$.$anonfun$saveToOpenSearch$1$adapted(OpenSearchSparkSQL.scala:113)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\t... 1 more\n",
      "\n",
      "\n",
      "\n",
      "üèÅ Testing completed at 2025-09-08 00:14:49\n"
     ]
    }
   ],
   "source": [
    "results = run_test_harness(chapter_to_run=\"ch08\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Notebook Testing Harness - 2025-09-08 00:14:49\n",
      "üìÅ Testing from root directory: /home/jovyan\n",
      "================================================================================\n",
      "üìã Found 2 notebook(s) to test.\n",
      "   ‚Ä¢ chapters/ch09/1.personalization.ipynb\n",
      "   ‚Ä¢ chapters/ch09/2.embedding-based-personalization.ipynb\n",
      "\n",
      "üöÄ Executing notebooks:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fe2bdbcb8f446e9b6b867b133ab37ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìî Testing: chapters/ch09/1.personalization.ipynb\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nbclient/client.py:601\u001b[0m, in \u001b[0;36mNotebookClient.setup_kernel\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 601\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nbconvert/preprocessors/execute.py:100\u001b[0m, in \u001b[0;36mExecutePreprocessor.preprocess\u001b[0;34m(self, nb, resources, km)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index, cell \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb\u001b[38;5;241m.\u001b[39mcells):\n\u001b[0;32m--> 100\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_cell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_widgets_metadata()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nbconvert/preprocessors/execute.py:121\u001b[0m, in \u001b[0;36mExecutePreprocessor.preprocess_cell\u001b[0;34m(self, cell, resources, index)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_assign_resources(resources)\n\u001b[0;32m--> 121\u001b[0m cell \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_cell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cell, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresources\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/jupyter_core/utils/__init__.py:158\u001b[0m, in \u001b[0;36mrun_sync.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m         _runner_map[name] \u001b[38;5;241m=\u001b[39m _TaskRunner()\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_runner_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/jupyter_core/utils/__init__.py:125\u001b[0m, in \u001b[0;36m_TaskRunner.run\u001b[0;34m(self, coro)\u001b[0m\n\u001b[1;32m    124\u001b[0m fut \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun_coroutine_threadsafe(coro, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__io_loop)\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_test_harness\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchapter_to_run\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mch09\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 55\u001b[0m, in \u001b[0;36mrun_test_harness\u001b[0;34m(exclude_dirs, exclude_notebooks, stop_on_failure, chapter_to_run, verbose_errors)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     54\u001b[0m kernel_name \u001b[38;5;241m=\u001b[39m KERNEL_OVERRIDES\u001b[38;5;241m.\u001b[39mget(notebook_path\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 55\u001b[0m success, execution_time, error \u001b[38;5;241m=\u001b[39m \u001b[43mexecute_notebook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnotebook_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m execution_time\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m, in \u001b[0;36mexecute_notebook\u001b[0;34m(notebook_path, timeout, kernel_name)\u001b[0m\n\u001b[1;32m      6\u001b[0m     nb \u001b[38;5;241m=\u001b[39m nbformat\u001b[38;5;241m.\u001b[39mread(f, as_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m      7\u001b[0m     ep \u001b[38;5;241m=\u001b[39m ExecutePreprocessor(timeout\u001b[38;5;241m=\u001b[39mtimeout, kernel_name\u001b[38;5;241m=\u001b[39mkernel_name)\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirname\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnotebook_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m execution_time \u001b[38;5;241m=\u001b[39m (datetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m start_time)\u001b[38;5;241m.\u001b[39mtotal_seconds()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m, execution_time, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nbconvert/preprocessors/execute.py:94\u001b[0m, in \u001b[0;36mExecutePreprocessor.preprocess\u001b[0;34m(self, nb, resources, km)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_execution_trackers()\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_assign_resources(resources)\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_kernel():\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkc\n\u001b[1;32m     96\u001b[0m     info_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait_for_reply(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkc\u001b[38;5;241m.\u001b[39mkernel_info())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    151\u001b[0m     value \u001b[38;5;241m=\u001b[39m typ()\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nbclient/client.py:604\u001b[0m, in \u001b[0;36mNotebookClient.setup_kernel\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cleanup_kc:\n\u001b[0;32m--> 604\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cleanup_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/jupyter_core/utils/__init__.py:158\u001b[0m, in \u001b[0;36mrun_sync.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _runner_map:\n\u001b[1;32m    157\u001b[0m         _runner_map[name] \u001b[38;5;241m=\u001b[39m _TaskRunner()\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_runner_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/jupyter_core/utils/__init__.py:125\u001b[0m, in \u001b[0;36m_TaskRunner.run\u001b[0;34m(self, coro)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__runner_thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    124\u001b[0m fut \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun_coroutine_threadsafe(coro, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__io_loop)\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = run_test_harness(chapter_to_run=\"ch09\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_test_harness(chapter_to_run=\"ch10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_test_harness(chapter_to_run=\"ch11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_test_harness(chapter_to_run=\"ch12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_test_harness(chapter_to_run=\"ch13\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_test_harness(chapter_to_run=\"ch14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_test_harness(chapter_to_run=\"ch15\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results (Optional)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
