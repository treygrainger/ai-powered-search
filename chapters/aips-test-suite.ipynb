{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aips.spark import get_spark_session\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import nbformat\n",
    "from nbconvert.preprocessors import ExecutePreprocessor\n",
    "from datetime import datetime\n",
    "import sys\n",
    "from aips import set_engine\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "spark = get_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KERNEL_NAME = \"python3\"\n",
    "KERNEL_OVERRIDES = {\"1.open-information-extraction.ipynb\": \"ch5-spacy\"}\n",
    "EXCLUDE_DIRS = [\".ipynb_checkpoints\", \"__pycache__\"]\n",
    "EXPORT_RESULTS = False\n",
    "EXCLUDED_NOTEBOOKS = [\"bonus.related-terms-from-documents.ipynb\"\n",
    "                      \"bonus.phrase-detection.ipynb\",\n",
    "                      \"bonus.phrase-detection.ipynb\",\n",
    "                      \"bonus.related-terms-from-documents.ipynb\",\n",
    "                      \"a.defunct.synthesize-search-sessions.ipynb\",\n",
    "                      \"a.synthesize-search-sessions.ipynb\",\n",
    "                      \"a.generate-movie-embeddings.ipynb\",\n",
    "                      \"welcome.ipynb\",\n",
    "                      \"aips-test-suite.ipynb\",\n",
    "                      \"4.train-upload-search-ltr.ipynb\",\n",
    "                      \"ch13-tokenizer-analysis.ipynb\",\n",
    "                      \"a.opensearch-ubi-indexer.ipynb\"]\n",
    "ALL_CHAPTERS = [\"ch03\", \"ch04\", \"ch05\", \"ch06\", \"ch07\", \"ch08\",\"ch09\", \"ch10\", \"ch11\", \n",
    "                \"ch12\", \"ch13\", \"ch14\", \"ch15\"]\n",
    "ALL_ENGINES = [\"solr\", \"opensearch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(seconds):\n",
    "    if seconds < 60:\n",
    "        formatted = f\"{seconds:.2f}s\"\n",
    "    elif seconds < 3600:\n",
    "        formatted =  f\"{seconds / 60:.2f}m\"\n",
    "    else:\n",
    "        formatted =  f\"{seconds / 3600:.2f}h\"\n",
    "    return formatted\n",
    "\n",
    "def export_results(results, filename=\"test_results.json\"):\n",
    "    if results:\n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump({\"timestamp\": datetime.now().isoformat(),\n",
    "                       \"results\": results[\"details\"],\n",
    "                       \"summary\": results[\"summary\"]}, f, indent=2)\n",
    "        print(f\"üìÑ Results exported to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notebook_files(root_dir=\".\", exclude_dirs=None, excluded_notebooks=None):\n",
    "    exclude_dirs = exclude_dirs or EXCLUDE_DIRS    \n",
    "    excluded_notebooks = excluded_notebooks or EXCLUDED_NOTEBOOKS\n",
    "    notebook_files = []\n",
    "    for directory in os.walk(root_dir):\n",
    "        for path in Path(directory[0]).rglob(\"*.ipynb\"):\n",
    "            if any(exclude_dir in str(path) for exclude_dir in exclude_dirs) or \\\n",
    "                path.name in excluded_notebooks:\n",
    "                continue\n",
    "            if str(path) not in notebook_files:\n",
    "                notebook_files.append(str(path))\n",
    "    return list(map(Path, sorted(notebook_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_notebook(notebook_path, timeout=600, kernel_name=\"python3\"):\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        with open(notebook_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            nb = nbformat.read(f, as_version=4)\n",
    "            ep = ExecutePreprocessor(timeout=timeout, kernel_name=kernel_name)\n",
    "            ep.preprocess(nb, {\"metadata\": {\"path\": os.path.dirname(notebook_path)}})\n",
    "        execution_time = (datetime.now() - start_time).total_seconds()\n",
    "        return True, execution_time, None\n",
    "    \n",
    "    except Exception as e:\n",
    "        execution_time = (datetime.now() - start_time).total_seconds()\n",
    "        error_type = type(e).__name__\n",
    "        error_msg = str(e)\n",
    "        tb = traceback.format_exc()\n",
    "        \n",
    "        return False, execution_time, {\"type\": error_type,\n",
    "                                       \"message\": error_msg,\n",
    "                                       \"traceback\": tb}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_harness(exclude_dirs=None, exclude_notebooks=None, stop_on_failure=True,\n",
    "                     chapter_to_run=None, verbose_errors=True):\n",
    "    root_dir = \".\"\n",
    "    timeout = 600 \n",
    "    print(f\"üîç Notebook Testing Harness - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"üìÅ Testing from root directory: {os.path.abspath(root_dir)}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    notebook_files = get_notebook_files(root_dir, exclude_dirs, exclude_notebooks)\n",
    "\n",
    "    if chapter_to_run:\n",
    "        notebook_files = [f for f in notebook_files if (chapter_to_run in str(f))]\n",
    "\n",
    "    print(f\"üìã Found {len(notebook_files)} notebook(s) to test.\")\n",
    "    for nb_file in notebook_files:\n",
    "        rel_path = os.path.relpath(str(nb_file), root_dir)\n",
    "        print(f\"   ‚Ä¢ {rel_path}\")\n",
    "    \n",
    "    results = {\"details\": [],\n",
    "               \"summary\": {\"total\": len(notebook_files),\n",
    "                           \"successful\": 0,\n",
    "                           \"failed\": 0,\n",
    "                           \"total_time\": 0}}\n",
    "    \n",
    "    for notebook_path in notebook_files:\n",
    "        rel_path = os.path.relpath(str(notebook_path), root_dir)\n",
    "        print(f\"\\nüìî Testing: {rel_path}\")\n",
    "        if \"checkpoint\" in str(notebook_path):\n",
    "            print(f\"\\nüìî Skipping: {rel_path}\")\n",
    "            continue\n",
    "        kernel_name = KERNEL_OVERRIDES.get(notebook_path.name, \"python3\")\n",
    "        success, execution_time, error = execute_notebook(str(notebook_path), timeout, kernel_name)\n",
    "        \n",
    "        results[\"summary\"][\"total_time\"] += execution_time\n",
    "        \n",
    "        if success:\n",
    "            print(f\"   ‚úÖ SUCCESS - Completed in {format_time(execution_time)}\")\n",
    "            results[\"summary\"][\"successful\"] += 1\n",
    "        else:\n",
    "            print(f\"   ‚ùå FAILED - Error after {format_time(execution_time)}\")\n",
    "            print(f\"      Error: {error['type']}\")\n",
    "            results[\"summary\"][\"failed\"] += 1\n",
    "        \n",
    "        results[\"details\"].append({\"notebook\": rel_path,\n",
    "                                   \"success\": success,\n",
    "                                   \"execution_time\": execution_time,\n",
    "                                   \"error\": error})\n",
    "        if not success and stop_on_failure:\n",
    "            print(\"Terminating test run due to test failure.\")\n",
    "            break\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"üìä SUMMARY:\")\n",
    "    print(f\"   Total notebooks tested: {results['summary']['total']}\")\n",
    "    print(f\"   ‚úÖ Successful: {results['summary']['successful']}\")\n",
    "    if results[\"summary\"][\"failed\"] > 0:\n",
    "        print(f\"   ‚ùå Failed: {results['summary']['failed']}\")\n",
    "    \n",
    "    success_rate = (results[\"summary\"][\"successful\"] / results[\"summary\"][\"total\"]) * 100\n",
    "    print(f\"   Success rate: {success_rate:.1f}%\")\n",
    "    print(f\"   Total execution time: {format_time(results['summary']['total_time'])}\")\n",
    "\n",
    "    if results[\"summary\"][\"failed\"] > 0:\n",
    "        print(f\"\\n‚ùå FAILED NOTEBOOKS:\")\n",
    "        for result in results[\"details\"]:\n",
    "            if not result[\"success\"]:\n",
    "                if verbose_errors:\n",
    "                    print(f\"   ‚Ä¢ {result['notebook']}: {result['error']['type']}: {result['error']['message']}\")\n",
    "                else:\n",
    "                    print(f\"   ‚Ä¢ {result['notebook']}: {result['error']['type']}\")\n",
    "    \n",
    "    print(f\"\\nüèÅ Testing completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    if EXPORT_RESULTS and results:\n",
    "        export_results(results, \"AIPS_test_results.json\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Notebook Testing Harness - 2025-09-11 00:48:53\n",
      "üìÅ Testing from root directory: /home/jovyan\n",
      "================================================================================\n",
      "üìã Found 36 notebook(s) to test.\n",
      "   ‚Ä¢ chapters/ch03/1.vectors-and-text-similarity.ipynb\n",
      "   ‚Ä¢ chapters/ch03/2.controlling-relevance.ipynb\n",
      "   ‚Ä¢ chapters/ch04/1.setting-up-the-retrotech-dataset.ipynb\n",
      "   ‚Ä¢ chapters/ch04/2.signals-boosting.ipynb\n",
      "   ‚Ä¢ chapters/ch05/1.open-information-extraction.ipynb\n",
      "   ‚Ä¢ chapters/ch05/2.index-datasets.ipynb\n",
      "   ‚Ä¢ chapters/ch05/3.semantic-knowledge-graph.ipynb\n",
      "   ‚Ä¢ chapters/ch06/1.skg-classification-disambiguation.ipynb\n",
      "   ‚Ä¢ chapters/ch06/2.related-keywords-from-signals.ipynb\n",
      "   ‚Ä¢ chapters/ch06/3.spell-correction.ipynb\n",
      "   ‚Ä¢ chapters/ch07/1.index-datasets.ipynb\n",
      "   ‚Ä¢ chapters/ch07/2.semantic-search.ipynb\n",
      "   ‚Ä¢ chapters/ch08/1.signals-boosting.ipynb\n",
      "   ‚Ä¢ chapters/ch09/1.personalization.ipynb\n",
      "   ‚Ä¢ chapters/ch09/2.embedding-based-personalization.ipynb\n",
      "   ‚Ä¢ chapters/ch10/1.setup-the-movie-db.ipynb\n",
      "   ‚Ä¢ chapters/ch10/2.judgments-and-logging.ipynb\n",
      "   ‚Ä¢ chapters/ch10/3.pairwise-transform.ipynb\n",
      "   ‚Ä¢ chapters/ch10/4.train-and-evaluate-the-model.ipynb\n",
      "   ‚Ä¢ chapters/ch11/1.click-through-rate-judgments.ipynb\n",
      "   ‚Ä¢ chapters/ch11/2.sdbn-judgments-to-overcome-position-bias.ipynb\n",
      "   ‚Ä¢ chapters/ch11/3.SDBN-Confidence-Bias.ipynb\n",
      "   ‚Ä¢ chapters/ch12/0.setup.ipynb\n",
      "   ‚Ä¢ chapters/ch12/1.ab-testing-to-active-learning.ipynb\n",
      "   ‚Ä¢ chapters/ch13/1.setting-up-the-outdoors-dataset.ipynb\n",
      "   ‚Ä¢ chapters/ch13/2.introduction-to-transformers.ipynb\n",
      "   ‚Ä¢ chapters/ch13/3.natural-language-autocomplete.ipynb\n",
      "   ‚Ä¢ chapters/ch13/4.semantic-search.ipynb\n",
      "   ‚Ä¢ chapters/ch13/5.quantization.ipynb\n",
      "   ‚Ä¢ chapters/ch14/1.question-answering-visualizer.ipynb\n",
      "   ‚Ä¢ chapters/ch14/2.question-answering-data-preparation.ipynb\n",
      "   ‚Ä¢ chapters/ch14/3.question-answering-fine-tuning.ipynb\n",
      "   ‚Ä¢ chapters/ch14/4.question-answering-demo-application.ipynb\n",
      "   ‚Ä¢ chapters/ch15/1.llm-exploration.ipynb\n",
      "   ‚Ä¢ chapters/ch15/2.multimodal-and-hybrid-search.ipynb\n",
      "   ‚Ä¢ engines/opensearch/notebooks/a.opensearch-ubi-indexer.ipynb\n",
      "\n",
      "\n",
      "üìî Testing: chapters/ch03/1.vectors-and-text-similarity.ipynb\n",
      "   ‚úÖ SUCCESS - Completed in 1.29s\n",
      "\n",
      "üìî Testing: chapters/ch03/2.controlling-relevance.ipynb\n",
      "   ‚úÖ SUCCESS - Completed in 8.92s\n",
      "\n",
      "üìî Testing: chapters/ch04/1.setting-up-the-retrotech-dataset.ipynb\n",
      "   ‚úÖ SUCCESS - Completed in 20.08s\n",
      "\n",
      "üìî Testing: chapters/ch04/2.signals-boosting.ipynb\n",
      "   ‚úÖ SUCCESS - Completed in 39.18s\n",
      "\n",
      "üìî Testing: chapters/ch05/1.open-information-extraction.ipynb\n",
      "   ‚úÖ SUCCESS - Completed in 8.99s\n",
      "\n",
      "üìî Testing: chapters/ch05/2.index-datasets.ipynb\n",
      "   ‚úÖ SUCCESS - Completed in 40.62s\n",
      "\n",
      "üìî Testing: chapters/ch05/3.semantic-knowledge-graph.ipynb\n",
      "   ‚úÖ SUCCESS - Completed in 40.33s\n",
      "\n",
      "üìî Testing: chapters/ch06/1.skg-classification-disambiguation.ipynb\n",
      "   ‚úÖ SUCCESS - Completed in 32.94s\n",
      "\n",
      "üìî Testing: chapters/ch06/2.related-keywords-from-signals.ipynb\n"
     ]
    }
   ],
   "source": [
    "for engine in [\"solr\", \"opensearch\"]:  \n",
    "    set_engine(engine)\n",
    "    run_test_harness(stop_on_failure=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
