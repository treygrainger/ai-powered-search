{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Testing Harness\n",
    "\n",
    "This notebook executes all Jupyter notebooks in the project recursively and reports their completion status.\n",
    "\n",
    "It serves as a testing harness to verify that all notebooks run successfully without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aips.spark import get_spark_session\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import nbformat\n",
    "from nbconvert.preprocessors import ExecutePreprocessor\n",
    "from datetime import datetime\n",
    "import sys\n",
    "from aips import set_engine\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "spark = get_spark_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KERNEL_NAME = \"python3\"\n",
    "KERNEL_OVERRIDES = {\"1.open-information-extraction.ipynb\": \"ch5-spacy\"}\n",
    "EXCLUDE_DIRS = [\".ipynb_checkpoints\", \"__pycache__\"]\n",
    "EXPORT_RESULTS = False\n",
    "EXCLUDED_NOTEBOOKS = [\"bonus.related-terms-from-documents.ipynb\"\n",
    "                      \"bonus.phrase-detection.ipynb\",\n",
    "                      \"bonus.phrase-detection.ipynb\",\n",
    "                      \"bonus.related-terms-from-documents.ipynb\",\n",
    "                      \"a.defunct.synthesize-search-sessions.ipynb\",\n",
    "                      \"a.synthesize-search-sessions.ipynb\",\n",
    "                      \"a.generate-movie-embeddings.ipynb\",\n",
    "                      \"welcome.ipynb\",\n",
    "                      \"aips-test-suite.ipynb\",\n",
    "                      \"4.train-upload-search-ltr.ipynb\",\n",
    "                      \"ch13-tokenizer-analysis.ipynb\"]\n",
    "ALL_CHAPTERS = [\"ch3\", \"ch4\", \"ch5\", \"ch6\", \"ch7\", \"ch8\",\"ch9\", \"ch10\", \"ch11\", \n",
    "                \"ch12\", \"ch13\", \"ch14\", \"ch15\"]\n",
    "ALL_ENGINES = [\"solr\", \"opensearch\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(seconds):\n",
    "    if seconds < 60:\n",
    "        formatted = f\"{seconds:.2f}s\"\n",
    "    elif seconds < 3600:\n",
    "        formatted =  f\"{seconds / 60:.2f}m\"\n",
    "    else:\n",
    "        formatted =  f\"{seconds / 3600:.2f}h\"\n",
    "    return formatted\n",
    "\n",
    "def export_results(results, filename=\"test_results.json\"):\n",
    "    if results:\n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump({\"timestamp\": datetime.now().isoformat(),\n",
    "                       \"results\": results[\"details\"],\n",
    "                       \"summary\": results[\"summary\"]}, f, indent=2)\n",
    "        print(f\"üìÑ Results exported to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notebook_files(root_dir=\".\", exclude_dirs=None, excluded_notebooks=None):\n",
    "    exclude_dirs = exclude_dirs or EXCLUDE_DIRS    \n",
    "    excluded_notebooks = excluded_notebooks or EXCLUDED_NOTEBOOKS\n",
    "    notebook_files = []\n",
    "    for directory in os.walk(root_dir):\n",
    "        for path in Path(directory[0]).rglob(\"*.ipynb\"):\n",
    "            if any(exclude_dir in str(path) for exclude_dir in exclude_dirs) or \\\n",
    "                path.name in excluded_notebooks:\n",
    "                continue\n",
    "            if str(path) not in notebook_files:\n",
    "                notebook_files.append(str(path))\n",
    "    return list(map(Path, sorted(notebook_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_notebook(notebook_path, timeout=600, kernel_name=\"python3\"):\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        with open(notebook_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            nb = nbformat.read(f, as_version=4)\n",
    "            ep = ExecutePreprocessor(timeout=timeout, kernel_name=kernel_name)\n",
    "            ep.preprocess(nb, {\"metadata\": {\"path\": os.path.dirname(notebook_path)}})\n",
    "        execution_time = (datetime.now() - start_time).total_seconds()\n",
    "        return True, execution_time, None\n",
    "    \n",
    "    except Exception as e:\n",
    "        execution_time = (datetime.now() - start_time).total_seconds()\n",
    "        error_type = type(e).__name__\n",
    "        error_msg = str(e)\n",
    "        tb = traceback.format_exc()\n",
    "        \n",
    "        return False, execution_time, {\"type\": error_type,\n",
    "                                       \"message\": error_msg,\n",
    "                                       \"traceback\": tb}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_harness(exclude_dirs=None, exclude_notebooks=None, stop_on_failure=True,\n",
    "                     chapter_to_run=None, verbose_errors=True):\n",
    "    root_dir = \".\"\n",
    "    timeout = 600 \n",
    "    print(f\"üîç Notebook Testing Harness - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"üìÅ Testing from root directory: {os.path.abspath(root_dir)}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    notebook_files = get_notebook_files(root_dir, exclude_dirs, exclude_notebooks)\n",
    "\n",
    "    if chapter_to_run:\n",
    "        notebook_files = [f for f in notebook_files if (chapter_to_run in str(f))]\n",
    "\n",
    "    print(f\"üìã Found {len(notebook_files)} notebook(s) to test.\")\n",
    "    for nb_file in notebook_files:\n",
    "        rel_path = os.path.relpath(str(nb_file), root_dir)\n",
    "        print(f\"   ‚Ä¢ {rel_path}\")\n",
    "    print()\n",
    "    \n",
    "    results = {\"details\": [],\n",
    "               \"summary\": {\"total\": len(notebook_files),\n",
    "                           \"successful\": 0,\n",
    "                           \"failed\": 0,\n",
    "                           \"total_time\": 0}}\n",
    "    \n",
    "    print(\"üöÄ Executing notebooks:\")\n",
    "    for notebook_path in tqdm(notebook_files, desc=\"Progress\", colour=\"purple\"):\n",
    "        rel_path = os.path.relpath(str(notebook_path), root_dir)\n",
    "        print(f\"\\nüìî Testing: {rel_path}\")\n",
    "        if \"checkpoint\" in str(notebook_path):\n",
    "            print(f\"\\nüìî Skipping: {rel_path}\")\n",
    "            continue\n",
    "        kernel_name = KERNEL_OVERRIDES.get(notebook_path.name, \"python3\")\n",
    "        success, execution_time, error = execute_notebook(str(notebook_path), timeout, kernel_name)\n",
    "        \n",
    "        results[\"summary\"][\"total_time\"] += execution_time\n",
    "        \n",
    "        if success:\n",
    "            print(f\"   ‚úÖ SUCCESS - Completed in {format_time(execution_time)}\")\n",
    "            results[\"summary\"][\"successful\"] += 1\n",
    "        else:\n",
    "            print(f\"   ‚ùå FAILED - Error after {format_time(execution_time)}\")\n",
    "            print(f\"      Error: {error['type']}\")\n",
    "            results[\"summary\"][\"failed\"] += 1\n",
    "        \n",
    "        results[\"details\"].append({\"notebook\": rel_path,\n",
    "                                   \"success\": success,\n",
    "                                   \"execution_time\": execution_time,\n",
    "                                   \"error\": error})\n",
    "        if not success and stop_on_failure:\n",
    "            print(\"Terminating test run due to test failure.\")\n",
    "            break\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"üìä SUMMARY:\")\n",
    "    print(f\"   Total notebooks tested: {results['summary']['total']}\")\n",
    "    print(f\"   ‚úÖ Successful: {results['summary']['successful']}\")\n",
    "    if results[\"summary\"][\"failed\"] > 0:\n",
    "        print(f\"   ‚ùå Failed: {results['summary']['failed']}\")\n",
    "    \n",
    "    success_rate = (results[\"summary\"][\"successful\"] / results[\"summary\"][\"total\"]) * 100\n",
    "    print(f\"   Success rate: {success_rate:.1f}%\")\n",
    "    print(f\"   Total execution time: {format_time(results['summary']['total_time'])}\")\n",
    "\n",
    "    if results[\"summary\"][\"failed\"] > 0:\n",
    "        print(f\"\\n‚ùå FAILED NOTEBOOKS:\")\n",
    "        for result in results[\"details\"]:\n",
    "            if not result[\"success\"]:\n",
    "                if verbose_errors:\n",
    "                    print(f\"   ‚Ä¢ {result['notebook']}: {result['error']['type']}: {result['error']['message']}\")\n",
    "                else:\n",
    "                    print(f\"   ‚Ä¢ {result['notebook']}: {result['error']['type']}\")\n",
    "    \n",
    "    print(f\"\\nüèÅ Testing completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    if EXPORT_RESULTS and results:\n",
    "        export_results(results, \"AIPS_test_resulsts.json\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last cell in chapter 3 will fail for most non-solr notebookes without {!funct} translation capabilities\n",
    "# Chapter 15 for opensearch fails when creating a view from the tmdb_with_embeddings collection. The image embedding vector fails to load into the datafram\n",
    "for engine in ALL_ENGINES:\n",
    "    set_engine(engine)    \n",
    "    results = run_test_harness(stop_on_failure=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
