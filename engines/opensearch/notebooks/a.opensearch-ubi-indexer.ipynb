{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aips import get_engine, set_engine\n",
    "from aips.spark.dataframe import from_sql\n",
    "from aips.spark import create_view_from_collection\n",
    "import tqdm\n",
    "\n",
    "engine = get_engine(\"opensearch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current state:\n",
    "- Plugin successfully install and loads, all indexing are mostly functional, but data isn't landing in the appropriate buckets\n",
    "- Debug why query ingestion isn't sticking (maybe initialization of UBI collections?)\n",
    "- UBI Dashboard needs to be pre-installed into the docker image (and configured?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run chapters/ch04/1.setting-up-the-retrotech-dataset.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Install and configure the OpenSearch UBI plugin\n",
    "\n",
    "bin/opensearch-plugin install https://github.com/o19s/opensearch-ubi/releases/download/release-v0.0.12.1-os2.14.0/opensearch-ubi-plugin-v0.0.12.1-os2.14.0.zip --batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Example ingestion of query data by adding the `ext` object to search requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'took': 4,\n",
       " 'timed_out': False,\n",
       " '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0},\n",
       " 'hits': {'total': {'value': 10000, 'relation': 'gte'},\n",
       "  'max_score': 1.0,\n",
       "  'hits': [{'_index': 'products',\n",
       "    '_id': '28944744927',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'upc': '28944744927',\n",
       "     'name': 'Faust Symphony - CD',\n",
       "     'manufacturer': 'UNKNOWN',\n",
       "     'short_description': ' ',\n",
       "     'long_description': ' '}},\n",
       "   {'_index': 'products',\n",
       "    '_id': '786936811605',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'upc': '786936811605',\n",
       "     'name': \"Grey's Anatomy: The Complete Seventh Season [6 Discs] - DVD\",\n",
       "     'manufacturer': ' ',\n",
       "     'short_description': ' ',\n",
       "     'long_description': ' '}},\n",
       "   {'_index': 'products',\n",
       "    '_id': '24543753100',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'upc': '24543753100',\n",
       "     'name': 'Burn Notice: The Fall of Sam Axe - Widescreen Subtitle AC3 - DVD',\n",
       "     'manufacturer': ' ',\n",
       "     'short_description': ' ',\n",
       "     'long_description': ' '}},\n",
       "   {'_index': 'products',\n",
       "    '_id': '786936811988',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'upc': '786936811988',\n",
       "     'name': 'Castle: The Complete Third Season [5 Discs] - Widescreen Subtitle AC3 - DVD',\n",
       "     'manufacturer': ' ',\n",
       "     'short_description': ' ',\n",
       "     'long_description': ' '}},\n",
       "   {'_index': 'products',\n",
       "    '_id': '31398134916',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'upc': '31398134916',\n",
       "     'name': 'Boy Meets World: The Complete Sixth Season [3 Discs] - Fullscreen Dolby - DVD',\n",
       "     'manufacturer': ' ',\n",
       "     'short_description': ' ',\n",
       "     'long_description': ' '}},\n",
       "   {'_index': 'products',\n",
       "    '_id': '883929173778',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'upc': '883929173778',\n",
       "     'name': 'Everwood: The Complete Fourth Season [5 Discs] - Widescreen Subtitle - DVD',\n",
       "     'manufacturer': ' ',\n",
       "     'short_description': ' ',\n",
       "     'long_description': ' '}},\n",
       "   {'_index': 'products',\n",
       "    '_id': '826663126549',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'upc': '826663126549',\n",
       "     'name': 'Spin City: The Complete Fifth Season [4 Discs] - Fullscreen - DVD',\n",
       "     'manufacturer': ' ',\n",
       "     'short_description': ' ',\n",
       "     'long_description': ' '}},\n",
       "   {'_index': 'products',\n",
       "    '_id': '43396348226',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'upc': '43396348226',\n",
       "     'name': 'Friends With Benefits - Original Soundtrack - CD',\n",
       "     'manufacturer': 'Madison Gate Records',\n",
       "     'short_description': ' ',\n",
       "     'long_description': ' '}},\n",
       "   {'_index': 'products',\n",
       "    '_id': '796019823845',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'upc': '796019823845',\n",
       "     'name': 'Ironclad - Widescreen Dolby - Blu-ray Disc',\n",
       "     'manufacturer': ' ',\n",
       "     'short_description': ' ',\n",
       "     'long_description': ' '}},\n",
       "   {'_index': 'products',\n",
       "    '_id': '859450002450',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'upc': '859450002450',\n",
       "     'name': 'The Sentence [PA] - CD',\n",
       "     'manufacturer': 'Siccness',\n",
       "     'short_description': ' ',\n",
       "     'long_description': ' '}}]},\n",
       " 'ext': {'ubi': {'query_id': '1234'}}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(f\"http://opensearch-node1:9200/products/_search\",\n",
    "                        json={\"ext\": {\"ubi\": {\"query_id\": \"1234\"}}})\n",
    "display(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Bulk ingest events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_events_dataframe():\n",
    "    signals_collection = engine.get_collection(\"signals\")\n",
    "    create_view_from_collection(signals_collection, \"signals\")\n",
    "    query = \"\"\"SELECT type AS action_name, query_id, user AS client_id,\n",
    "                      signal_time AS timestamp, type AS message_type,\n",
    "                      target AS message, target AS target\n",
    "               FROM signals WHERE type != 'query'\"\"\"\n",
    "    events = from_sql(query)\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiping \"ubi_events\" collection\n",
      "Creating \"ubi_events\" collection\n",
      "Successfully written 1447146 documents\n"
     ]
    }
   ],
   "source": [
    "events_collection = engine.create_collection(\"ubi_events\")\n",
    "ubi_events_dataframe = get_events_dataframe()\n",
    "events_collection.write(ubi_events_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Step 3 - Bulk ingest queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_queries_dataframe():\n",
    "    signals_collection = engine.get_collection(\"signals\")\n",
    "    create_view_from_collection(signals_collection, \"signals\")\n",
    "    queries = from_sql(\"SELECT * FROM signals WHERE type = 'query'\")\n",
    "    queries_transformed = queries.rdd.map(lambda r: \n",
    "        (r[\"signal_time\"], r[\"query_id\"], r[\"user\"], r[\"target\"]))\n",
    "    ubi_queries_dataframe = queries_transformed.toDF(\n",
    "        [\"timestamp\", \"query_id\", \"client_id\", \"user_query\"])\n",
    "    return ubi_queries_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Indexing search signals upon executing a search (intended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_search(collection, signal, log=False):\n",
    "    request = {\"query\": signal[\"user_query\"],\n",
    "               \"query_fields\": [\"name\", \"manufacturer\",\n",
    "                                \"long_description\", \"short_description\"],\n",
    "               \"return_fields\": [\"*\"],\n",
    "               \"limit\": 10,\n",
    "               \"ubi\": signal | {\"store_name\": \"aips_store\"}}\n",
    "    try:\n",
    "        return collection.search(**request)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 725459/725459 [01:44<00:00, 6915.09it/s]\n"
     ]
    }
   ],
   "source": [
    "products_collection = engine.get_collection(\"products\")\n",
    "ubi_queries_dataframe = get_queries_dataframe()\n",
    "for q in tqdm.tqdm(ubi_queries_dataframe.collect(), total=ubi_queries_dataframe.count()):\n",
    "    execute_search(products_collection, q.asDict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_ingest_queries():\n",
    "    queries_collection = engine.create_collection(\"ubi_queries\")\n",
    "    ubi_queries_dataframe = get_queries_dataframe()\n",
    "    queries_collection.write(ubi_queries_dataframe)\n",
    "\n",
    "#batch_ingest_queries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generating AIPS Signals collection from  - Indexing search signals upon executing a search (intended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-------+------------+--------+\n",
      "|  signal_time|   query_id|   user|      target|    type|\n",
      "+-------------+-----------+-------+------------+--------+\n",
      "|1585854599197|u530435_0_1|u530435|786936817218|purchase|\n",
      "|1586692556578|u595885_1_2|u595885|786936817218|purchase|\n",
      "|1586669080953|u695122_0_1|u695122|786936817218|purchase|\n",
      "|1581706502786|u734452_0_1|u734452|786936817218|purchase|\n",
      "|1588861261426|u135438_0_1|u135438|786936817218|purchase|\n",
      "|1585184225191|u219056_0_1|u219056|786936817218|purchase|\n",
      "|1588843902245| u72400_0_1| u72400|786936817218|purchase|\n",
      "|1585057472120|u442883_0_1|u442883|786936817218|purchase|\n",
      "|1577072668322|u353276_0_1|u353276|786936817218|purchase|\n",
      "|1587130028623|  u7654_0_1|  u7654|786936817218|purchase|\n",
      "+-------------+-----------+-------+------------+--------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+---------+--------------------+--------+----------------------+--------------------+--------------------+----------+\n",
      "|client_id|               query|query_id|query_response_hit_ids|   query_response_id|           timestamp|user_query|\n",
      "+---------+--------------------+--------+----------------------+--------------------+--------------------+----------+\n",
      "|     null|{\"ext\":{\"query_id...|    1234|                  null|aa98fde6-eb83-452...|2024-10-24 12:36:...|      null|\n",
      "|     null|{\"ext\":{\"query_id...|    1234|                  null|4bccab23-1cd9-48a...|2024-10-24 12:54:...|      null|\n",
      "|     null|{\"ext\":{\"query_id...|    1234|                  null|0a6a379e-a5ca-422...|2024-10-24 12:58:...|      null|\n",
      "+---------+--------------------+--------+----------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Some of types cannot be determined by the first 100 rows, please try again with sampling",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m events \u001b[38;5;241m=\u001b[39m create_events_dataframe()\n\u001b[1;32m     36\u001b[0m events\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m queries \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_queries_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m queries\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#signals_collection.write(queries.union(events))\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 19\u001b[0m, in \u001b[0;36mcreate_queries_dataframe\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m queries\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     16\u001b[0m queries_transformed \u001b[38;5;241m=\u001b[39m queries\u001b[38;5;241m.\u001b[39mrdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m r: \n\u001b[1;32m     17\u001b[0m     (r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m], r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery_id\u001b[39m\u001b[38;5;124m\"\u001b[39m], r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient_id\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     18\u001b[0m      r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_query\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mqueries_transformed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoDF\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msignal_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtarget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:102\u001b[0m, in \u001b[0;36m_monkey_patch_RDD.<locals>.toDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;129m@no_type_check\u001b[39m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtoDF\u001b[39m(\u001b[38;5;28mself\u001b[39m, schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sampleRatio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m    Converts current :class:`RDD` into a :class:`DataFrame`\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    [Row(name='Alice', age=1)]\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampleRatio\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:894\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pandas\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m    892\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m    893\u001b[0m     )\n\u001b[0;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    896\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:934\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, RDD):\n\u001b[0;32m--> 934\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromRDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    936\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromLocal(\u001b[38;5;28mmap\u001b[39m(prepare, data), schema)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:600\u001b[0m, in \u001b[0;36mSparkSession._createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03mCreate an RDD for DataFrame from an existing RDD, returns the RDD and schema.\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 600\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[1;32m    602\u001b[0m     tupled_rdd \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmap(converter)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:573\u001b[0m, in \u001b[0;36mSparkSession._inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m    571\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    572\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 573\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome of types cannot be determined by the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst 100 rows, please try again with sampling\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m             )\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m samplingRatio \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.99\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: Some of types cannot be determined by the first 100 rows, please try again with sampling"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_unixtime\n",
    "def create_events_dataframe():\n",
    "    ubi_events_collection = engine.get_collection(\"ubi_events\")\n",
    "    create_view_from_collection(ubi_events_collection, \"ubi_events\")\n",
    "    events = from_sql(\"SELECT * FROM ubi_events\")\n",
    "    events_transformed = events.rdd.map(lambda r: \n",
    "        (r[\"timestamp\"], r[\"query_id\"], r[\"client_id\"],\n",
    "         r[\"message\"], r[\"message_type\"]))\n",
    "    return events_transformed.toDF([\"signal_time\", \"query_id\", \"user\", \"target\", \"type\"])\n",
    "\n",
    "def create_queries_dataframe():\n",
    "    ubi_queries_collection = engine.get_collection(\"ubi_queries\")\n",
    "    create_view_from_collection(ubi_queries_collection, \"ubi_queries\")\n",
    "    queries = from_sql(\"SELECT * FROM ubi_queries\")\n",
    "    queries.show(10)\n",
    "    queries_transformed = queries.rdd.map(lambda r: \n",
    "        (r[\"timestamp\"], r[\"query_id\"], r[\"client_id\"],\n",
    "         r[\"user_query\"], \"query\"))\n",
    "    return queries_transformed.toDF([\"signal_time\", \"query_id\", \"user\", \"target\", \"type\"])\n",
    "\n",
    "def _create_queries_dataframe():\n",
    "    ubi_queries_collection = engine.get_collection(\"ubi_queries\")\n",
    "    create_view_from_collection(ubi_queries_collection, \"ubi_queries\")\n",
    "    queries = from_sql(\"SELECT * FROM ubi_queries\")\n",
    "    queries.show(10)\n",
    "    queries_transformed = queries.rdd.map(lambda r: \n",
    "        (r[\"timestamp\"], r[\"query_id\"], r[\"client_id\"],\n",
    "         r[\"user_query\"], \"query\"))\n",
    "    return queries_transformed.toDF([\"signal_time\", \"query_id\", \"user\",\n",
    "                                                    \"target\", \"type\"])\n",
    "    return queries_transformed.withColumn(\"signal_time\", from_unixtime(\"signal_time\"))\n",
    "    \n",
    "\n",
    "#signals_collection = engine.create_collection(\"signals\")\n",
    "events = create_events_dataframe()\n",
    "events.show(10)\n",
    "queries = create_queries_dataframe()\n",
    "queries.show(10)\n",
    "#signals_collection.write(queries.union(events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
