{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opensearch User Behavior Insights (UBI)\n",
    "\n",
    "### This notebook covers the basics around setting up UBI, ingesting data using the UBI plugin, and setting up a basic UBI opensearch dashboard\n",
    "\n",
    "**Information regarding UBI:**\n",
    "\n",
    "https://opensearch.org/docs/latest/search-plugins/ubi/index\n",
    "\n",
    "https://github.com/opensearch-project/user-behavior-insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aips import get_engine, set_engine\n",
    "from aips.spark.dataframe import from_sql\n",
    "from aips.spark import create_view_from_collection\n",
    "import tqdm\n",
    "import aips.indexer\n",
    "import requests, json\n",
    "engine = get_engine(\"opensearch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiping \"products\" collection\n",
      "Creating \"products\" collection\n",
      "Loading Products\n",
      "Schema: \n",
      "root\n",
      " |-- upc: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- manufacturer: string (nullable = true)\n",
      " |-- short_description: string (nullable = true)\n",
      " |-- long_description: string (nullable = true)\n",
      "\n",
      "<Response [200]>\n",
      "{'_shards': {'total': 2, 'successful': 1, 'failed': 0}}\n",
      "Successfully written 48194 documents\n",
      "Wiping \"signals\" collection\n",
      "Creating \"signals\" collection\n",
      "Loading data/retrotech/signals.csv\n",
      "Schema: \n",
      "root\n",
      " |-- query_id: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- target: string (nullable = true)\n",
      " |-- signal_time: timestamp (nullable = true)\n",
      "\n",
      "<Response [200]>\n",
      "{'_shards': {'total': 2, 'successful': 1, 'failed': 0}}\n",
      "Successfully written 2172605 documents\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<engines.opensearch.OpenSearchCollection.OpenSearchCollection at 0x7f6a94331090>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aips.indexer.build_collection(engine, \"products\")\n",
    "aips.indexer.build_collection(engine, \"signals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1**: Install and configure the OpenSearch UBI plugin\n",
    "\n",
    "To install UBI on an opensearch cluster, execute the following command on a node or during the building of an image. This command has already been run on the AIPS opensearch node.\n",
    "\n",
    "**bin/opensearch-plugin install https://github.com/o19s/opensearch-ubi/releases/download/release-v0.0.12.1-os2.14.0/opensearch-ubi-plugin-v0.0.12.1-os2.14.0.zip --batch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 2**: - Bulk ingesting historic signals\n",
    "\n",
    "Historic user events and queries should be bulk ingested into the appropriate UBI collections.\n",
    "\n",
    "Here we bulk write all AIPS queries into the `ubi_queries` collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_queries_dataframe():\n",
    "    signals_collection = engine.get_collection(\"signals\")\n",
    "    create_view_from_collection(signals_collection, \"signals\")\n",
    "    queries = from_sql(\"SELECT * FROM signals WHERE type = 'query'\")\n",
    "    queries_transformed = queries.rdd.map(lambda r: \n",
    "        (r[\"signal_time\"], r[\"query_id\"], r[\"user\"], r[\"target\"]))\n",
    "    ubi_queries_dataframe = queries_transformed.toDF(\n",
    "        [\"timestamp\", \"query_id\", \"client_id\", \"user_query\"])\n",
    "    return ubi_queries_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_ingest_queries():\n",
    "    queries_collection = engine.create_collection(\"ubi_queries\")\n",
    "    ubi_queries_dataframe = get_queries_dataframe()\n",
    "    queries_collection.write(ubi_queries_dataframe)\n",
    "    return queries_collection\n",
    "\n",
    "#This line commented as batch query ingestion is done in a \n",
    "#different manner later with the extension for examples sake.\n",
    "#queries_collection = batch_ingest_queries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can index events into the `ubi_events` collection which is intended to hold all non-query signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_events_dataframe():\n",
    "    signals_collection = engine.get_collection(\"signals\")\n",
    "    products_collection = engine.get_collection(\"products\")\n",
    "    create_view_from_collection(signals_collection, \"signals\")\n",
    "    create_view_from_collection(products_collection, \"products\")\n",
    "    query = \"\"\"SELECT REPLACE(type, '-', '_') AS action_name, query_id, user AS client_id,\n",
    "                      signal_time AS timestamp, type AS message_type,\n",
    "                      target AS target, p.name AS message\n",
    "               FROM signals s\n",
    "               LEFT JOIN products p ON s.target == p.upc\n",
    "               WHERE type != 'query'\"\"\"\n",
    "    events = from_sql(query)\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiping \"ubi_events\" collection\n",
      "Creating \"ubi_events\" collection\n",
      "<Response [200]>\n",
      "{'_shards': {'total': 2, 'successful': 1, 'failed': 0}}\n",
      "Successfully written 1447146 documents\n"
     ]
    }
   ],
   "source": [
    "def batch_ingest_signals():\n",
    "    events_collection = engine.create_collection(\"ubi_events\")\n",
    "    ubi_events_dataframe = get_events_dataframe()\n",
    "    events_collection.write(ubi_events_dataframe)\n",
    "    return events_collection\n",
    "\n",
    "events_collection = batch_ingest_signals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3**: Live logging of queries and events\n",
    "\n",
    "Queries and events must be ingested correctly and with complete data into UBI for best results. UBI stores queries seperate from other events, each in their respective collection `ubi_queries` and `ubi_events`. Live signal data collection should be hooked into the appropriate places in your stack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logging event data is as simple as writing an event document directly to the ubi_events collection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_index': 'ubi_events',\n",
       " '_id': 'fFAxMJkB4ljHNAOlcSCq',\n",
       " '_version': 1,\n",
       " 'result': 'created',\n",
       " '_shards': {'total': 2, 'successful': 1, 'failed': 0},\n",
       " '_seq_no': 1447146,\n",
       " '_primary_term': 1}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def add_example_event_to_ubi():\n",
    "    collection = \"products\"\n",
    "    event_doc = {\"action_name\": \"purchase\", #This is a name of the type of event/action that occurred\n",
    "                 \"client_id\": \"uid_000001\", #This is id of the user/session taking the action\n",
    "                 \"message_type\": \"one_click_buy\", #An additional action type, used for further action grouping\n",
    "                 \"message\": \"Succeeded\", #An optional message string for the event\n",
    "                 \"query_id\": \"qid_000001\", #The id of the query that led to this action\n",
    "                 \"target\": \"pid_000001\"} #Any string representing the target of the action. Normally a doc/item id?\n",
    "\n",
    "    response = requests.post(f\"http://opensearch-node1:9200/ubi_events/_doc?\",\n",
    "                             json=event_doc)\n",
    "    display(response.json())\n",
    "\n",
    "add_example_event_to_ubi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queries should be collected at query time utilizing the UBI extension request handler. Here is an example of ingesting query data by adding an `ubi` property to the `ext` object during a search request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'took': 5,\n",
       " 'timed_out': False,\n",
       " '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0},\n",
       " 'hits': {'total': {'value': 1165, 'relation': 'eq'},\n",
       "  'max_score': 7.091094,\n",
       "  'hits': [{'_index': 'products',\n",
       "    '_id': '50644382727',\n",
       "    '_score': 7.091094,\n",
       "    '_source': {'upc': '50644382727',\n",
       "     'name': \"Monster Cable - 50' Mini-Spool Speaker Cable\",\n",
       "     'manufacturer': 'Monster Cable',\n",
       "     'short_description': \"Navajo white speaker cable; 50' length; special LPE insulation reduces signal loss\",\n",
       "     'long_description': 'The Magnetic Flux Tube construction and special cable windings provide natural music reproduction with impressive clarity, bass response and dynamic range in a compact design. Special LPE insulation reduces signal loss and distortion. Paintable Navajo white jacket matches all interiors.'},\n",
       "    'fields': {'short_description': [\"Navajo white speaker cable; 50' length; special LPE insulation reduces signal loss\"],\n",
       "     'name': [\"Monster Cable - 50' Mini-Spool Speaker Cable\"],\n",
       "     'upc': ['50644382727'],\n",
       "     'long_description': ['The Magnetic Flux Tube construction and special cable windings provide natural music reproduction with impressive clarity, bass response and dynamic range in a compact design. Special LPE insulation reduces signal loss and distortion. Paintable Navajo white jacket matches all interiors.'],\n",
       "     'manufacturer': ['Monster Cable']}},\n",
       "   {'_index': 'products',\n",
       "    '_id': '50644304606',\n",
       "    '_score': 7.091094,\n",
       "    '_source': {'upc': '50644304606',\n",
       "     'name': 'Monster Cable - TV Screen Cleaning Kit',\n",
       "     'manufacturer': 'Monster Cable',\n",
       "     'short_description': 'High-tech reusable MicroFiber cloth cleans your screen without scratching it',\n",
       "     'long_description': \"Keep your big screen in pristine condition with this complete monitor cleaning kit. The advanced formula removes dust and fingerprints without streaks or stains. The reusable MicroFiber cloth won't scratch your screen.\"},\n",
       "    'fields': {'short_description': ['High-tech reusable MicroFiber cloth cleans your screen without scratching it'],\n",
       "     'name': ['Monster Cable - TV Screen Cleaning Kit'],\n",
       "     'upc': ['50644304606'],\n",
       "     'long_description': [\"Keep your big screen in pristine condition with this complete monitor cleaning kit. The advanced formula removes dust and fingerprints without streaks or stains. The reusable MicroFiber cloth won't scratch your screen.\"],\n",
       "     'manufacturer': ['Monster Cable']}},\n",
       "   {'_index': 'products',\n",
       "    '_id': '50644514142',\n",
       "    '_score': 7.091094,\n",
       "    '_source': {'upc': '50644514142',\n",
       "     'name': 'Monster Cable - 8 ft HDMI A/V Cable',\n",
       "     'manufacturer': 'Monster Cable',\n",
       "     'short_description': 'HDMI - 8ft Monster Cable HDMI Cable For Xbox 360',\n",
       "     'long_description': 'Monster Cable HDMI Cable For Xbox 360 - HDMI - 8ft'},\n",
       "    'fields': {'short_description': ['HDMI - 8ft Monster Cable HDMI Cable For Xbox 360'],\n",
       "     'name': ['Monster Cable - 8 ft HDMI A/V Cable'],\n",
       "     'upc': ['50644514142'],\n",
       "     'long_description': ['Monster Cable HDMI Cable For Xbox 360 - HDMI - 8ft'],\n",
       "     'manufacturer': ['Monster Cable']}},\n",
       "   {'_index': 'products',\n",
       "    '_id': '50644382802',\n",
       "    '_score': 7.091094,\n",
       "    '_source': {'upc': '50644382802',\n",
       "     'name': \"Monster Cable - 20' 16-Gauge Mini-Spool\",\n",
       "     'manufacturer': 'Monster Cable',\n",
       "     'short_description': 'Magnetic Flux Tube construction and special cable windings for natural music reproduction',\n",
       "     'long_description': 'Magnetic Flux Tube construction and special cable windings for natural music reproduction. Impressive clarity, bass response and dynamic range in a compact design.'},\n",
       "    'fields': {'short_description': ['Magnetic Flux Tube construction and special cable windings for natural music reproduction'],\n",
       "     'name': [\"Monster Cable - 20' 16-Gauge Mini-Spool\"],\n",
       "     'upc': ['50644382802'],\n",
       "     'long_description': ['Magnetic Flux Tube construction and special cable windings for natural music reproduction. Impressive clarity, bass response and dynamic range in a compact design.'],\n",
       "     'manufacturer': ['Monster Cable']}},\n",
       "   {'_index': 'products',\n",
       "    '_id': '50644455803',\n",
       "    '_score': 7.091094,\n",
       "    '_source': {'upc': '50644455803',\n",
       "     'name': 'Monster Cable - Quick Lock Gold Spade Connector (2 Pairs)',\n",
       "     'manufacturer': 'Monster Cable',\n",
       "     'short_description': ' ',\n",
       "     'long_description': 'Learn more about Magnolia Premium Installation   Protect your speaker wire from corrosion with these connectors that feature a simple 2-piece, screw-on design.'},\n",
       "    'fields': {'short_description': [' '],\n",
       "     'name': ['Monster Cable - Quick Lock Gold Spade Connector (2 Pairs)'],\n",
       "     'upc': ['50644455803'],\n",
       "     'long_description': ['Learn more about Magnolia Premium Installation   Protect your speaker wire from corrosion with these connectors that feature a simple 2-piece, screw-on design.'],\n",
       "     'manufacturer': ['Monster Cable']}},\n",
       "   {'_index': 'products',\n",
       "    '_id': '50644382789',\n",
       "    '_score': 7.091094,\n",
       "    '_source': {'upc': '50644382789',\n",
       "     'name': \"Monster Cable - XP 50' Mini Spool Speaker Cable\",\n",
       "     'manufacturer': 'Monster Cable',\n",
       "     'short_description': 'Compatible with stereo and home theater speakers; Magnetic Flux Tube design for enhanced sound reproduction; LPE insulation for improved signal transfer',\n",
       "     'long_description': \"Wire your stereo or home theater speakers for impressive, cinema-quality sound with this 50' spool of speaker wire that features a Magnetic Flux Tube design for enhanced sound reproduction and LPE insulation for improved signal transfer.\"},\n",
       "    'fields': {'short_description': ['Compatible with stereo and home theater speakers; Magnetic Flux Tube design for enhanced sound reproduction; LPE insulation for improved signal transfer'],\n",
       "     'name': [\"Monster Cable - XP 50' Mini Spool Speaker Cable\"],\n",
       "     'upc': ['50644382789'],\n",
       "     'long_description': [\"Wire your stereo or home theater speakers for impressive, cinema-quality sound with this 50' spool of speaker wire that features a Magnetic Flux Tube design for enhanced sound reproduction and LPE insulation for improved signal transfer.\"],\n",
       "     'manufacturer': ['Monster Cable']}},\n",
       "   {'_index': 'products',\n",
       "    '_id': '50644382741',\n",
       "    '_score': 7.091094,\n",
       "    '_source': {'upc': '50644382741',\n",
       "     'name': \"Monster Cable - 20' Navajo White Speaker Wire\",\n",
       "     'manufacturer': 'Monster Cable',\n",
       "     'short_description': 'Special LPE insulation reduces signal loss and distortion; paintable Navajo white jacket',\n",
       "     'long_description': 'Impressive clarity, bass response and dynamic range in a compact design. Special LPE insulation reduces signal loss and distortion. Paintable Navajo white jacket matches all interiors.'},\n",
       "    'fields': {'short_description': ['Special LPE insulation reduces signal loss and distortion; paintable Navajo white jacket'],\n",
       "     'name': [\"Monster Cable - 20' Navajo White Speaker Wire\"],\n",
       "     'upc': ['50644382741'],\n",
       "     'long_description': ['Impressive clarity, bass response and dynamic range in a compact design. Special LPE insulation reduces signal loss and distortion. Paintable Navajo white jacket matches all interiors.'],\n",
       "     'manufacturer': ['Monster Cable']}},\n",
       "   {'_index': 'products',\n",
       "    '_id': '50644459092',\n",
       "    '_score': 7.091094,\n",
       "    '_source': {'upc': '50644459092',\n",
       "     'name': \"Monster Cable - 100' Spool Speaker Wire\",\n",
       "     'manufacturer': 'Monster Cable',\n",
       "     'short_description': 'Magnetic Flux Tube construction and special cable windings for natural music reproduction',\n",
       "     'long_description': 'Magnetic Flux Tube construction and special cable windings for natural music reproduction. Impressive clarity, bass response and dynamic range in a compact design.'},\n",
       "    'fields': {'short_description': ['Magnetic Flux Tube construction and special cable windings for natural music reproduction'],\n",
       "     'name': [\"Monster Cable - 100' Spool Speaker Wire\"],\n",
       "     'upc': ['50644459092'],\n",
       "     'long_description': ['Magnetic Flux Tube construction and special cable windings for natural music reproduction. Impressive clarity, bass response and dynamic range in a compact design.'],\n",
       "     'manufacturer': ['Monster Cable']}},\n",
       "   {'_index': 'products',\n",
       "    '_id': '50644459122',\n",
       "    '_score': 7.091094,\n",
       "    '_source': {'upc': '50644459122',\n",
       "     'name': \"Monster Cable - 100' Navajo White Speaker Wire\",\n",
       "     'manufacturer': 'Monster Cable',\n",
       "     'short_description': 'Magnetic Flux Tube construction; special LPE insulation; paintable Navajo white jacket',\n",
       "     'long_description': 'Magnetic Flux Tube construction and special cable windings for natural music reproduction. Impressive clarity, bass response and dynamic range in a compact design. Special LPE insulation reduces signal loss and distortion. Paintable Navajo white jacket matches all interiors.'},\n",
       "    'fields': {'short_description': ['Magnetic Flux Tube construction; special LPE insulation; paintable Navajo white jacket'],\n",
       "     'name': [\"Monster Cable - 100' Navajo White Speaker Wire\"],\n",
       "     'upc': ['50644459122'],\n",
       "     'long_description': ['Magnetic Flux Tube construction and special cable windings for natural music reproduction. Impressive clarity, bass response and dynamic range in a compact design. Special LPE insulation reduces signal loss and distortion. Paintable Navajo white jacket matches all interiors.'],\n",
       "     'manufacturer': ['Monster Cable']}},\n",
       "   {'_index': 'products',\n",
       "    '_id': '640282090599',\n",
       "    '_score': 7.091094,\n",
       "    '_source': {'upc': '640282090599',\n",
       "     'name': \"Accu Cable - 5' 3-Pin DMX Cable\",\n",
       "     'manufacturer': 'Accu Cable',\n",
       "     'short_description': \"DMX connector; 5' length; 3 pins; 22 AWG; 110 ohms impedance\",\n",
       "     'long_description': \"Connect lighting units with this 5' 3-pin DMX cable that provides safety and shielding from transmission interference.\"},\n",
       "    'fields': {'short_description': [\"DMX connector; 5' length; 3 pins; 22 AWG; 110 ohms impedance\"],\n",
       "     'name': [\"Accu Cable - 5' 3-Pin DMX Cable\"],\n",
       "     'upc': ['640282090599'],\n",
       "     'long_description': [\"Connect lighting units with this 5' 3-pin DMX cable that provides safety and shielding from transmission interference.\"],\n",
       "     'manufacturer': ['Accu Cable']}},\n",
       "   {'_index': 'products',\n",
       "    '_id': '50644499517',\n",
       "    '_score': 6.2097235,\n",
       "    '_source': {'upc': '50644499517',\n",
       "     'name': \"Monster - 8' Small Cable-It Cable Wrap - Black\",\n",
       "     'manufacturer': 'Monster',\n",
       "     'short_description': 'From our expanded online assortment; holds 3-5 cables along walls; can be cut into custom lengths; can be used in home theaters, offices and studios',\n",
       "     'long_description': 'Make cable clutter a thing of the past with this cable wrap that neatly holds 3-5 cables against the wall in your home theater, office or studio and can be cut into custom lengths for a precise fit.'},\n",
       "    'fields': {'short_description': ['From our expanded online assortment; holds 3-5 cables along walls; can be cut into custom lengths; can be used in home theaters, offices and studios'],\n",
       "     'name': [\"Monster - 8' Small Cable-It Cable Wrap - Black\"],\n",
       "     'upc': ['50644499517'],\n",
       "     'long_description': ['Make cable clutter a thing of the past with this cable wrap that neatly holds 3-5 cables against the wall in your home theater, office or studio and can be cut into custom lengths for a precise fit.'],\n",
       "     'manufacturer': ['Monster']}}]},\n",
       " 'ext': {'ubi': {'query_id': 'qid_000001'}}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def execute_example_query_with_ubi():        \n",
    "    collection = \"products\"\n",
    "    query = \"cable\"\n",
    "    ubi_extension_data = {\"ubi\": {\"query_id\": \"qid_000001\",\n",
    "                                  \"client_id\": \"cid_000001\",\n",
    "                                  \"user_query\": query}}\n",
    "    search_request = {\n",
    "        \"query\": {\"query_string\": {\"query\": query,\n",
    "                                   \"fields\": [\"name\", \"manufacturer\",\n",
    "                                              \"long_description\", \"short_description\"]}},\n",
    "        \"size\": 11, \n",
    "        \"fields\": [\"*\"],\n",
    "        \"ext\": ubi_extension_data\n",
    "    }\n",
    "\n",
    "    response = requests.post(f\"http://opensearch-node1:9200/{collection}/_search?\",\n",
    "                             json=search_request)\n",
    "    display(response.json())\n",
    "\n",
    "execute_example_query_with_ubi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice UBI information is returned on the search response object with at least the ubi signal id linking to the ingested query. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will load all query signals into UBI by simulating user searches. This serves as a batch import of data for examples sake. Batch importing should normally just be done by batch indexing query signals directly into `ubi_queries` directly as shown earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_search(collection, signal, log=False):\n",
    "    signal.pop(\"timestamp\", None) #The timestamp of a query is the time of search and cannot be passed in\n",
    "    request = {\"query\": signal[\"user_query\"],\n",
    "               \"query_fields\": [\"name\", \"manufacturer\",\n",
    "                                \"long_description\", \"short_description\"],\n",
    "               \"return_fields\": [\"*\"],\n",
    "               \"limit\": 10,\n",
    "               \"ubi\": signal | {\"store_name\": \"aips_store\"}}\n",
    "    try:\n",
    "        return collection.search(**request)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def search_and_log_all_query_signals():\n",
    "    products_collection = engine.get_collection(\"products\")\n",
    "    ubi_queries_dataframe = get_queries_dataframe()\n",
    "    for q in tqdm.tqdm(ubi_queries_dataframe.collect(), total=ubi_queries_dataframe.count()):\n",
    "        execute_search(products_collection, q.asDict())\n",
    "\n",
    "#search_and_log_all_query_signals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading UBI queries and events into AIPS\n",
    "\n",
    "If you wish to load in UBI queries/events from your Opensearch cluster to work with the book, you can do so with the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiping \"signals\" collection\n",
      "Creating \"signals\" collection\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 28.0 failed 1 times, most recent failure: Lost task 0.0 in stage 28.0 (TID 159) (ad2e8ea97b1e executor driver): java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: scala.collection.convert.Wrappers$JListWrapper is not a valid external type for schema of string\nif (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, client_id), StringType, true), true, false, true) AS client_id#381\nif (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, query), StringType, true), true, false, true) AS query#382\nif (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 2, query_id), StringType, true), true, false, true) AS query_id#383\nif (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 3, query_response_hit_ids), StringType, true), true, false, true) AS query_response_hit_ids#384\nif (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 4, query_response_id), StringType, true), true, false, true) AS query_response_id#385\nif (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.sql.catalyst.util.DateTimeUtils$, TimestampType, anyToMicros, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 5, timestamp), TimestampType, true), true, false, true) AS timestamp#386\nif (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 6, user_query), StringType, true), true, false, true) AS user_query#387\nif (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else newInstance(class org.apache.spark.sql.catalyst.util.ArrayBasedMapData) AS _metadata#388\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.expressionEncodingError(QueryExecutionErrors.scala:1237)\n\tat org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Serializer.apply(ExpressionEncoder.scala:210)\n\tat org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Serializer.apply(ExpressionEncoder.scala:193)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1213)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\nCaused by: java.lang.RuntimeException: scala.collection.convert.Wrappers$JListWrapper is not a valid external type for schema of string\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.StaticInvoke_3$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.writeFields_0_1$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Serializer.apply(ExpressionEncoder.scala:207)\n\t... 22 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: scala.collection.convert.Wrappers$JListWrapper is not a valid external type for schema of string\nif (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, client_id), StringType, true), true, false, true) AS client_id#381\nif (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, query), StringType, true), true, false, true) AS query#382\nif (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 2, query_id), StringType, true), true, false, true) AS query_id#383\nif (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 3, query_response_hit_ids), StringType, true), true, false, true) AS query_response_hit_ids#384\nif (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 4, query_response_id), StringType, true), true, false, true) AS query_response_id#385\nif (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.sql.catalyst.util.DateTimeUtils$, TimestampType, anyToMicros, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 5, timestamp), TimestampType, true), true, false, true) AS timestamp#386\nif (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 6, user_query), StringType, true), true, false, true) AS user_query#387\nif (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else newInstance(class org.apache.spark.sql.catalyst.util.ArrayBasedMapData) AS _metadata#388\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.expressionEncodingError(QueryExecutionErrors.scala:1237)\n\tat org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Serializer.apply(ExpressionEncoder.scala:210)\n\tat org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Serializer.apply(ExpressionEncoder.scala:193)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1213)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\nCaused by: java.lang.RuntimeException: scala.collection.convert.Wrappers$JListWrapper is not a valid external type for schema of string\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.StaticInvoke_3$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.writeFields_0_1$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Serializer.apply(ExpressionEncoder.scala:207)\n\t... 22 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m     signals_collection\u001b[38;5;241m.\u001b[39mwrite(events, overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m signals_collection\n\u001b[0;32m---> 27\u001b[0m signals_collection \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_signals_collection_with_ubi_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 22\u001b[0m, in \u001b[0;36mcreate_signals_collection_with_ubi_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m signals_collection \u001b[38;5;241m=\u001b[39m engine\u001b[38;5;241m.\u001b[39mcreate_collection(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msignals\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m events \u001b[38;5;241m=\u001b[39m load_ubi_events_as_aips_dataframe()\n\u001b[0;32m---> 22\u001b[0m queries \u001b[38;5;241m=\u001b[39m \u001b[43mload_ubi_queries_as_aips_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m signals_collection\u001b[38;5;241m.\u001b[39mwrite(queries)\n\u001b[1;32m     24\u001b[0m signals_collection\u001b[38;5;241m.\u001b[39mwrite(events, overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[10], line 17\u001b[0m, in \u001b[0;36mload_ubi_queries_as_aips_dataframe\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m queries \u001b[38;5;241m=\u001b[39m from_sql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM ubi_queries\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m queries_transformed \u001b[38;5;241m=\u001b[39m queries\u001b[38;5;241m.\u001b[39mrdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m r: \n\u001b[1;32m     15\u001b[0m     (r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m], r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery_id\u001b[39m\u001b[38;5;124m\"\u001b[39m], r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient_id\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     16\u001b[0m      r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_query\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mqueries_transformed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoDF\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msignal_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtarget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:102\u001b[0m, in \u001b[0;36m_monkey_patch_RDD.<locals>.toDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;129m@no_type_check\u001b[39m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtoDF\u001b[39m(\u001b[38;5;28mself\u001b[39m, schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sampleRatio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m    Converts current :class:`RDD` into a :class:`DataFrame`\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    [Row(name='Alice', age=1)]\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampleRatio\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:894\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pandas\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m    892\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m    893\u001b[0m     )\n\u001b[0;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    896\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:934\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, RDD):\n\u001b[0;32m--> 934\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromRDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    936\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromLocal(\u001b[38;5;28mmap\u001b[39m(prepare, data), schema)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:600\u001b[0m, in \u001b[0;36mSparkSession._createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03mCreate an RDD for DataFrame from an existing RDD, returns the RDD and schema.\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 600\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[1;32m    602\u001b[0m     tupled_rdd \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmap(converter)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:546\u001b[0m, in \u001b[0;36mSparkSession._inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_inferSchema\u001b[39m(\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    527\u001b[0m     rdd: RDD[Any],\n\u001b[1;32m    528\u001b[0m     samplingRatio: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    529\u001b[0m     names: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    530\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m StructType:\n\u001b[1;32m    531\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;124;03m    Infer schema from an RDD of Row, dict, or tuple.\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;124;03m    :class:`pyspark.sql.types.StructType`\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 546\u001b[0m     first \u001b[38;5;241m=\u001b[39m \u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m first:\n\u001b[1;32m    548\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe first row in RDD is empty, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan not infer schema\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:1903\u001b[0m, in \u001b[0;36mRDD.first\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfirst\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[T]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m   1891\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1892\u001b[0m \u001b[38;5;124;03m    Return the first element in this RDD.\u001b[39;00m\n\u001b[1;32m   1893\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1901\u001b[0m \u001b[38;5;124;03m    ValueError: RDD is empty\u001b[39;00m\n\u001b[1;32m   1902\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1903\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1904\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rs:\n\u001b[1;32m   1905\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:1883\u001b[0m, in \u001b[0;36mRDD.take\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1880\u001b[0m         taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1882\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned \u001b[38;5;241m+\u001b[39m numPartsToTry, totalParts))\n\u001b[0;32m-> 1883\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1885\u001b[0m items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m   1886\u001b[0m partsScanned \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m numPartsToTry\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/context.py:1486\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1484\u001b[0m mappedRDD \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1486\u001b[0m sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmappedRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 28.0 failed 1 times, most recent failure: Lost task 0.0 in stage 28.0 (TID 159) (ad2e8ea97b1e executor driver): java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: scala.collection.convert.Wrappers$JListWrapper is not a valid external type for schema of string\nif (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, client_id), StringType, true), true, false, true) AS client_id#381\nif (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, query), StringType, true), true, false, true) AS query#382\nif (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 2, query_id), StringType, true), true, false, true) AS query_id#383\nif (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 3, query_response_hit_ids), StringType, true), true, false, true) AS query_response_hit_ids#384\nif (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 4, query_response_id), StringType, true), true, false, true) AS query_response_id#385\nif (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.sql.catalyst.util.DateTimeUtils$, TimestampType, anyToMicros, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 5, timestamp), TimestampType, true), true, false, true) AS timestamp#386\nif (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 6, user_query), StringType, true), true, false, true) AS user_query#387\nif (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else newInstance(class org.apache.spark.sql.catalyst.util.ArrayBasedMapData) AS _metadata#388\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.expressionEncodingError(QueryExecutionErrors.scala:1237)\n\tat org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Serializer.apply(ExpressionEncoder.scala:210)\n\tat org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Serializer.apply(ExpressionEncoder.scala:193)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1213)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\nCaused by: java.lang.RuntimeException: scala.collection.convert.Wrappers$JListWrapper is not a valid external type for schema of string\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.StaticInvoke_3$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.writeFields_0_1$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Serializer.apply(ExpressionEncoder.scala:207)\n\t... 22 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: scala.collection.convert.Wrappers$JListWrapper is not a valid external type for schema of string\nif (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, client_id), StringType, true), true, false, true) AS client_id#381\nif (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, query), StringType, true), true, false, true) AS query#382\nif (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 2, query_id), StringType, true), true, false, true) AS query_id#383\nif (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 3, query_response_hit_ids), StringType, true), true, false, true) AS query_response_hit_ids#384\nif (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 4, query_response_id), StringType, true), true, false, true) AS query_response_id#385\nif (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.sql.catalyst.util.DateTimeUtils$, TimestampType, anyToMicros, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 5, timestamp), TimestampType, true), true, false, true) AS timestamp#386\nif (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 6, user_query), StringType, true), true, false, true) AS user_query#387\nif (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else newInstance(class org.apache.spark.sql.catalyst.util.ArrayBasedMapData) AS _metadata#388\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.expressionEncodingError(QueryExecutionErrors.scala:1237)\n\tat org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Serializer.apply(ExpressionEncoder.scala:210)\n\tat org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Serializer.apply(ExpressionEncoder.scala:193)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1213)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\nCaused by: java.lang.RuntimeException: scala.collection.convert.Wrappers$JListWrapper is not a valid external type for schema of string\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.StaticInvoke_3$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.writeFields_0_1$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Serializer.apply(ExpressionEncoder.scala:207)\n\t... 22 more\n"
     ]
    }
   ],
   "source": [
    "def load_ubi_events_as_aips_dataframe():\n",
    "    ubi_events_collection = engine.get_collection(\"ubi_events\")\n",
    "    create_view_from_collection(ubi_events_collection, \"ubi_events\")\n",
    "    events = from_sql(\"SELECT * FROM ubi_events\")\n",
    "    events_transformed = events.rdd.map(lambda r: \n",
    "        (r[\"timestamp\"], r[\"query_id\"], r[\"client_id\"],\n",
    "         r[\"message\"], r[\"message_type\"]))\n",
    "    return events_transformed.toDF([\"signal_time\", \"query_id\", \"user\", \"target\", \"type\"])\n",
    "\n",
    "def load_ubi_queries_as_aips_dataframe():\n",
    "    ubi_queries_collection = engine.get_collection(\"ubi_queries\")\n",
    "    create_view_from_collection(ubi_queries_collection, \"ubi_queries\")\n",
    "    queries = from_sql(\"SELECT * FROM ubi_queries\")\n",
    "    queries_transformed = queries.rdd.map(lambda r: \n",
    "        (r[\"timestamp\"], r[\"query_id\"], r[\"client_id\"],\n",
    "         r[\"user_query\"], \"query\"))\n",
    "    return queries_transformed.toDF([\"signal_time\", \"query_id\", \"user\", \"target\", \"type\"])\n",
    "\n",
    "def create_signals_collection_with_ubi_data():\n",
    "    signals_collection = engine.create_collection(\"signals\")\n",
    "    events = load_ubi_events_as_aips_dataframe()\n",
    "    queries = load_ubi_queries_as_aips_dataframe()\n",
    "    signals_collection.write(queries)\n",
    "    signals_collection.write(events, overwrite=False)\n",
    "    return signals_collection\n",
    "\n",
    "signals_collection = create_signals_collection_with_ubi_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and viewing the UBI Dashboard\n",
    "\n",
    "The following code will import the default Dashboard objects. The dashboard can be viewed here\n",
    "\n",
    "http://opensearch-aips:5601/app/dashboards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'successCount': 6,\n",
       " 'success': True,\n",
       " 'successResults': [{'type': 'index-pattern',\n",
       "   'id': '7d14f3e4-c873-4ff0-ba62-c5b741d2ac6b',\n",
       "   'meta': {'title': 'ubi_*', 'icon': 'indexPatternApp'},\n",
       "   'destinationId': '8920b2d7-957b-4a15-b179-3e82fa5a3fca'},\n",
       "  {'type': 'visualization',\n",
       "   'id': '1391fd2c-18f3-4b9f-85e7-799da34bcf1d',\n",
       "   'meta': {'title': 'all ubi messages', 'icon': 'visualizeApp'},\n",
       "   'destinationId': '4690f32b-797c-4e36-af08-ae1f5a146120'},\n",
       "  {'type': 'visualization',\n",
       "   'id': '789b6480-d667-11ef-96b9-a3e177a902a3',\n",
       "   'meta': {'title': 'Searches', 'icon': 'visualizeApp'},\n",
       "   'destinationId': '5b0ae97d-a9d0-4700-bd6b-16837f35bc00'},\n",
       "  {'type': 'index-pattern',\n",
       "   'id': 'b8544e15-0471-497e-a4c8-7696a83fcd84',\n",
       "   'meta': {'title': 'ubi_events', 'icon': 'indexPatternApp'},\n",
       "   'destinationId': 'f60c6c43-4ecb-4971-bf4d-715fa3673b7c'},\n",
       "  {'type': 'visualization',\n",
       "   'id': 'f2e2cc60-d667-11ef-96b9-a3e177a902a3',\n",
       "   'meta': {'title': 'Event types', 'icon': 'visualizeApp'},\n",
       "   'destinationId': 'bed6be39-8366-4fe1-8b6b-f449b30aa707'},\n",
       "  {'type': 'dashboard',\n",
       "   'id': 'c1d6447f-aa85-42f9-a340-6d4d089a5030',\n",
       "   'meta': {'title': 'User Behavior Insights', 'icon': 'dashboardApp'},\n",
       "   'destinationId': 'e4be7a2d-25cd-4460-bf85-038a0f209529'}]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def import_ubi_dashboard():\n",
    "    with open(\"./engines/opensearch/build/ubi-dashboard-objects.ndjson\", \"rb\") as f: \n",
    "        dashboard_ndjson = f.read()\n",
    "    response = requests.post(f\"http://opensearch-dashboards:5601/api/saved_objects/_import?createNewCopies=true\",\n",
    "                            files={\"file\": (\"request.ndjson\", dashboard_ndjson)},\n",
    "                            headers={\"kbn-xsrf\": \"true\",\n",
    "                                     \"osd-version\": \"2.14.0\",\n",
    "                                     \"osd-xsrf\": \"osd-fetch\"})\n",
    "    display(response.json())\n",
    "\n",
    "import_ubi_dashboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
