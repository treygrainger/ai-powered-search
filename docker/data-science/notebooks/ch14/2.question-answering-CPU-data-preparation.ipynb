{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import html\n",
    "import pandas\n",
    "import pickle\n",
    "import json\n",
    "sys.path.append('..')\n",
    "from aips import *\n",
    "path = \"../data/outdoors/\"\n",
    "engine = get_engine()\n",
    "outdoors_collection = engine.get_collection(\"outdoors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: This notebook depends upon the Outdoors dataset. If you have any issues, please rerun the [Setting up the Outdoors Dataset](../ch13/1.setting-up-the-outdoors-dataset.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 14.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_questions():\n",
    "    question_types = [\"who\", \"what\", \"when\", \"where\", \"why\", \"how\"]\n",
    "    questions = []\n",
    "    for type in question_types:\n",
    "        request = {\n",
    "            \"query\": type,\n",
    "            \"fields\": [\"id\", \"url\", \"owner_user_id\", \"title\", \"accepted_answer_id\"],\n",
    "            \"params\": {\n",
    "              \"qf\": [\"title\"],\n",
    "              \"fq\": [\"accepted_answer_id:[* TO *]\"],\n",
    "              \"defType\": \"edismax\",\n",
    "              \"rows\": 10000\n",
    "            }\n",
    "        }\n",
    "        docs = engine.docs_from_response(outdoors_collection.search(request))\n",
    "        questions += [document for document in docs\n",
    "                      if document[\"title\"].lower().startswith(type)]  #Only titles starting with a question type\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 14.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_dataframe(questions):\n",
    "    contexts = {\"id\": [], \"question\":[], \"context\":[], \"url\": []}\n",
    "    for question in questions:\n",
    "        request = {\n",
    "            \"query\": \"*:*\",\n",
    "            \"fields\": [\"body\"],\n",
    "            \"params\": {\n",
    "              \"fq\": [\"id:\" + str(question[\"accepted_answer_id\"])],\n",
    "              \"defType\": \"edismax\",\n",
    "              \"rows\": 1,\n",
    "              \"sort\": \"score desc\"\n",
    "            }\n",
    "        }\n",
    "        docs = engine.docs_from_response(outdoors_collection.search(request))\n",
    "        contexts[\"id\"].append(question[\"id\"])\n",
    "        contexts[\"url\"].append(question[\"url\"])\n",
    "        contexts[\"question\"].append(question[\"title\"]),\n",
    "        contexts[\"context\"].append(docs[0][\"body\"])\n",
    "    return pandas.DataFrame(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/models.py:971\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomplexjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    973\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m questions \u001b[38;5;241m=\u001b[39m \u001b[43mget_questions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m contexts \u001b[38;5;241m=\u001b[39m get_context_dataframe(questions)\n\u001b[1;32m      3\u001b[0m contexts[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m10\u001b[39m]\n",
      "Cell \u001b[0;32mIn[2], line 15\u001b[0m, in \u001b[0;36mget_questions\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;129;01min\u001b[39;00m question_types:\n\u001b[1;32m      5\u001b[0m     request \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m,\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfields\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mowner_user_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccepted_answer_id\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m         }\n\u001b[1;32m     14\u001b[0m     }\n\u001b[0;32m---> 15\u001b[0m     docs \u001b[38;5;241m=\u001b[39m engine\u001b[38;5;241m.\u001b[39mdocs_from_response(\u001b[43moutdoors_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     16\u001b[0m     questions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [document \u001b[38;5;28;01mfor\u001b[39;00m document \u001b[38;5;129;01min\u001b[39;00m docs\n\u001b[1;32m     17\u001b[0m                   \u001b[38;5;28;01mif\u001b[39;00m document[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;28mtype\u001b[39m)]  \u001b[38;5;66;03m#Only titles starting with a question type\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m questions\n",
      "File \u001b[0;32m~/notebooks/solr_collection.py:66\u001b[0m, in \u001b[0;36mSolrCollection.search\u001b[0;34m(self, request, data)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch\u001b[39m(\u001b[38;5;28mself\u001b[39m, request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mSOLR_URL\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/select\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/models.py:975\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    972\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    973\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[0;32m--> 975\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "questions = get_questions()\n",
    "contexts = get_context_dataframe(questions)\n",
    "contexts[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mkdir ../data/outdoors\n",
    "contexts.to_csv(\"../data/outdoors/question-answer-seed-contexts.csv\", index=False)\n",
    "#contexts = pandas.read_csv(path+'data/question-answer-seed-contexts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 14.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processor_type():\n",
    "    return 0 if torch.cuda.is_available() else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import tqdm\n",
    "\n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "pipeline_type = \"question-answering\"\n",
    "\n",
    "device = get_processor_type()\n",
    "\n",
    "def answer_questions(contexts, k=10):\n",
    "    nlp = pipeline(pipeline_type, model=model_name,tokenizer=model_name, device=device)\n",
    "    guesses = []\n",
    "    for _, row in tqdm.tqdm(contexts[0:k].iterrows(), total=k):\n",
    "        result = nlp({\"question\": row[\"question\"], \"context\": row[\"context\"]})\n",
    "        guesses.append(result)\n",
    "    return guesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1663/1663 [09:29<00:00,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.2789272964000702, 'start': 474, 'end': 516, 'answer': 'a local enthusiast or group of enthusiasts'}, {'score': 0.20084792375564575, 'start': 81, 'end': 117, 'answer': 'the person who is creating the climb'}, {'score': 0.018631743267178535, 'start': 14, 'end': 24, 'answer': 'the victim'}, {'score': 0.22231647372245789, 'start': 29, 'end': 38, 'answer': 'slip knot'}, {'score': 0.0005512409843504429, 'start': 1255, 'end': 1262, 'answer': 'aquatic'}, {'score': 0.3749975562095642, 'start': 15, 'end': 40, 'answer': 'a high-tech treasure hunt'}, {'score': 0.5637544989585876, 'start': 192, 'end': 232, 'answer': 'a tube of lightweight, stretchy material'}, {'score': 0.11091530323028564, 'start': 125, 'end': 154, 'answer': 'the cheapest one of the three'}, {'score': 0.805173933506012, 'start': 68, 'end': 76, 'answer': 'blocking'}, {'score': 0.24700817465782166, 'start': 227, 'end': 265, 'answer': 'the traditional longbow made from wood'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nSome weights of RobertaModel were not initialized from the model checkpoint at deepset/roberta-base-squad2 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\\n100%|██████████| 1662/1662 [43:19<00:00,  1.56s/it] \\n\\n[{'score': 0.2776225209236145, 'start': 474, 'end': 517, 'answer': 'a local enthusiast or group of enthusiasts.'}, {'score': 0.1954791247844696, 'start': 81, 'end': 118, 'answer': 'the person who is creating the climb.'}, {'score': 0.024139929562807083, 'start': 14, 'end': 24, 'answer': 'the victim'}, {'score': 0.3299234211444855, 'start': 29, 'end': 38, 'answer': 'slip knot'}, {'score': 0.0005422658286988735, 'start': 1255, 'end': 1262, 'answer': 'aquatic'}, {'score': 0.37733304500579834, 'start': 15, 'end': 41, 'answer': 'a high-tech treasure hunt.'}, {'score': 0.5653401613235474, 'start': 192, 'end': 233, 'answer': 'a tube of lightweight, stretchy material.'}, {'score': 0.10057392716407776, 'start': 125, 'end': 155, 'answer': 'the cheapest one of the three,'}, {'score': 0.7781326174736023, 'start': 68, 'end': 77, 'answer': 'blocking.'}, {'score': 0.2520507276058197, 'start': 227, 'end': 266, 'answer': 'the traditional longbow made from wood,'}]\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guesses = answer_questions(contexts, k=len(contexts))\n",
    "contexts[\"answers\"] = guesses\n",
    "print(guesses[0:10])\n",
    "\"\"\"\n",
    "Some weights of RobertaModel were not initialized from the model checkpoint at deepset/roberta-base-squad2 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "100%|██████████| 1662/1662 [43:19<00:00,  1.56s/it] \n",
    "\n",
    "[{'score': 0.2776225209236145, 'start': 474, 'end': 517, 'answer': 'a local enthusiast or group of enthusiasts.'}, {'score': 0.1954791247844696, 'start': 81, 'end': 118, 'answer': 'the person who is creating the climb.'}, {'score': 0.024139929562807083, 'start': 14, 'end': 24, 'answer': 'the victim'}, {'score': 0.3299234211444855, 'start': 29, 'end': 38, 'answer': 'slip knot'}, {'score': 0.0005422658286988735, 'start': 1255, 'end': 1262, 'answer': 'aquatic'}, {'score': 0.37733304500579834, 'start': 15, 'end': 41, 'answer': 'a high-tech treasure hunt.'}, {'score': 0.5653401613235474, 'start': 192, 'end': 233, 'answer': 'a tube of lightweight, stretchy material.'}, {'score': 0.10057392716407776, 'start': 125, 'end': 155, 'answer': 'the cheapest one of the three,'}, {'score': 0.7781326174736023, 'start': 68, 'end': 77, 'answer': 'blocking.'}, {'score': 0.2520507276058197, 'start': 227, 'end': 266, 'answer': 'the traditional longbow made from wood,'}]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts.to_csv(\"../data/outdoors/question-answer-squad2-guesses.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 14.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Manually labeling data**\n",
    "*The above csv file (../data/question-answer-squad2-guesses.csv) is used as a raw first pass at attempting to answer the questions.  This is then used with human-in-the-loop manual correction and labelling of the data.  There is no python code that can do this for you.  The data MUST be labelled by an intelligent person with an understanding of the domain.  All further listings will use the 'golden set' - the manually corrected answer file, and not the guesses that were generated above.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'url', 'title', 'question', 'context', 'answers', '__index_level_0__'],\n",
       "        num_rows: 125\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'url', 'title', 'question', 'context', 'answers', '__index_level_0__'],\n",
       "        num_rows: 32\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'url', 'title', 'question', 'context', 'answers', '__index_level_0__'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "def get_training_data(filename):\n",
    "    golden_answers = pandas.read_csv(filename)\n",
    "    golden_answers[\"class\"] = golden_answers[\"class\"].fillna(-2).astype(int)\n",
    "    validated = golden_answers[golden_answers[\"class\"] > -1]    \n",
    "    table = {\"id\": [], \"url\": [], \"title\": [], \"question\": [], \"context\": [], \"answers\": []}\n",
    "    \n",
    "    for _, row in validated.iterrows():\n",
    "        answers = row[\"gold\"].split(\"|\")\n",
    "        starts = []\n",
    "        missing = False\n",
    "        for answer in answers:\n",
    "            result = row[\"context\"].find(answer)\n",
    "            starts.append(result)\n",
    "            if(result < 0):\n",
    "                missing = True\n",
    "        if not missing:\n",
    "            table[\"id\"].append(row[\"id\"])\n",
    "            table[\"url\"].append(row[\"url\"])\n",
    "            table[\"title\"].append(row[\"question\"])\n",
    "            table[\"question\"].append(row[\"question\"])\n",
    "            table[\"context\"].append(row[\"context\"])\n",
    "            table[\"answers\"].append({\"text\": answers, \"answer_start\": starts})\n",
    "            \n",
    "    df = pandas.DataFrame(table).sample(frac=1)\n",
    "    train_split = int(len(df) * 0.75)\n",
    "    eval_split = int((len(df) - train_split) / 1.25) + train_split - 1\n",
    "    train_dataset = Dataset.from_pandas(df[:train_split])\n",
    "    test_dataset = Dataset.from_pandas(df[train_split:eval_split])\n",
    "    validation_dataset = Dataset.from_pandas(df[eval_split:])\n",
    "    return DatasetDict({\"train\": train_dataset, \"test\": test_dataset, \"validation\": validation_dataset})\n",
    "\n",
    "#This golden answers file was labeled by me (Max Irwin).\n",
    "#It took about 2-3 hours to label 200 question/answer rows\n",
    "#Doing so will give you a deeper appreciation for the difficulty of the NLP task.\n",
    "#I *highly* encourage you to label even more documents, and re-run the fine-tuning tasks coming up.\n",
    "data = get_training_data(path + \"outdoors_golden_answers_20210130.csv\")\n",
    "data.save_to_disk(path + \"question-answering-training-set\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up next: [Question Answering LLM Fine-tuning](3.question-answering-GPU-fine-tuning.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
