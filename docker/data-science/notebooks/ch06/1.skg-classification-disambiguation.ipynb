{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ Chapter 6 - Using Content to Learn Domain-specific Language ]\n",
    "# Query Classification and Disambiguation with Semantic Knowledge Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "NOTE: This notebook depends upon the the Stack Exchange datasets. If you have any issues, please rerun the [Setting up the Stack Exchange Dataset](../ch05/2.index-datasets.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from aips import *\n",
    "import os\n",
    "import json\n",
    "from IPython.display import display,HTML\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"AIPS\").getOrCreate()\n",
    "engine = get_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackexchange_collection = engine.get_collection(\"stackexchange\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: docker\n",
      "  Classifications: \n",
      "    devops  0.87978\n",
      "\n",
      "\n",
      "Query: airplane\n",
      "  Classifications: \n",
      "    travel  0.33334\n",
      "\n",
      "\n",
      "Query: airplane AND crash\n",
      "  Classifications: \n",
      "    scifi  0.02149\n",
      "    travel  0.00475\n",
      "\n",
      "\n",
      "Query: vitamins\n",
      "  Classifications: \n",
      "    health  0.48681\n",
      "    cooking  0.09441\n",
      "\n",
      "\n",
      "Query: alien\n",
      "  Classifications: \n",
      "    scifi  0.62541\n",
      "\n",
      "\n",
      "Query: passport\n",
      "  Classifications: \n",
      "    travel  0.82883\n",
      "\n",
      "\n",
      "Query: driver\n",
      "  Classifications: \n",
      "    travel  0.38996\n",
      "    devops  0.08917\n",
      "\n",
      "\n",
      "Query: driver AND taxi\n",
      "  Classifications: \n",
      "    travel  0.24184\n",
      "    scifi  -0.13757\n",
      "\n",
      "\n",
      "Query: driver AND install\n",
      "  Classifications: \n",
      "    devops  0.22277\n",
      "    travel  -0.00675\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def run_query_classification(query, classification_field=\"category\", \n",
    "                             classification_limit=5, keywords_field=\"body\", min_occurrences=5):\n",
    "        \n",
    "    classification_query = {\n",
    "        \"params\": {\n",
    "            \"qf\": keywords_field,\n",
    "            \"fore\": \"{!type=$defType qf=$qf v=$q}\",\n",
    "            \"back\": \"*:*\",\n",
    "            \"defType\": \"edismax\"\n",
    "        },\n",
    "        \"query\": query,\n",
    "        \"facet\": {\n",
    "            \"classification\":{\n",
    "                \"type\": \"terms\",\n",
    "                \"field\": classification_field,\n",
    "                \"sort\": { \"classification_relatedness\": \"desc\"},\n",
    "                \"mincount\": min_occurrences, \n",
    "                \"limit\": classification_limit,\n",
    "                \"facet\": {\n",
    "                    \"classification_relatedness\": {\n",
    "                        \"type\": \"func\",\n",
    "                        \"func\": \"relatedness($fore,$back)\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "     \n",
    "    response = stackexchange_collection.search(classification_query)\n",
    "    print(f\"Query: {query}\") \n",
    "    print(\"  Classifications: \")\n",
    "    for bucket in response[\"facets\"][\"classification\"][\"buckets\"]:\n",
    "        print(f\"    {bucket['val']}  {bucket['classification_relatedness']['relatedness']}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "run_query_classification(\"docker\", classification_limit=3 )\n",
    "run_query_classification(\"airplane\", classification_limit=1 )\n",
    "run_query_classification(\"airplane AND crash\", classification_limit=2 )\n",
    "run_query_classification(\"vitamins\", classification_limit=2 )\n",
    "run_query_classification(\"alien\", classification_limit=1 )\n",
    "run_query_classification(\"passport\", classification_limit=1 )\n",
    "run_query_classification(\"driver\", classification_limit=2 )\n",
    "run_query_classification(\"driver AND taxi\", classification_limit=2 )\n",
    "run_query_classification(\"driver AND install\", classification_limit=2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disambiguation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_disambiguation_query(query, context_field=\"category\", context_limit=5,\n",
    "                             keywords_field=\"body\", keywords_limit=10, min_occurrences=5):      \n",
    "    \n",
    "    disambiguation_query = {\n",
    "        \"params\": {\n",
    "            \"qf\": keywords_field,\n",
    "            \"fore\": \"{!type=$defType qf=$qf v=$q}\",\n",
    "            \"back\": \"*:*\",\n",
    "            \"defType\": \"edismax\",\n",
    "            \"rows\": 0,\n",
    "            \"echoParams\": \"none\",\n",
    "            \"omitHeader\": \"true\"\n",
    "        },\n",
    "        \"query\": query,\n",
    "        \"facet\": {\n",
    "            \"context\":{\n",
    "                \"type\": \"terms\",\n",
    "                \"field\": context_field,\n",
    "                \"sort\": { \"context_relatedness\": \"desc\"},\n",
    "                \"mincount\": min_occurrences, \n",
    "                \"limit\": context_limit,\n",
    "                \"facet\": {\n",
    "                    \"context_relatedness\": {\n",
    "                        \"type\": \"func\",\n",
    "                        \"func\": \"relatedness($fore,$back)\"\n",
    "                    },        \n",
    "                    \"keywords\": {\n",
    "                        \"type\": \"terms\",\n",
    "                        \"field\": keywords_field,\n",
    "                        \"mincount\": min_occurrences,\n",
    "                        \"limit\": keywords_limit,\n",
    "                        \"sort\": { \"keywords_relatedness\": \"desc\"},\n",
    "                        \"facet\": {\n",
    "                            \"keywords_relatedness\": {\n",
    "                                \"type\": \"func\",\n",
    "                                \"func\": \"relatedness($fore,$back)\"\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }    \n",
    "        \n",
    "    response = stackexchange_collection.search(disambiguation_query)    \n",
    "    print(f\"Query: {query}\") \n",
    "    for ctx_bucket in response[\"facets\"][\"context\"][\"buckets\"]:\n",
    "        print(f\"  Context: {ctx_bucket['val']}  {ctx_bucket['context_relatedness']['relatedness']}\")\n",
    "        print(\"    Keywords: \")\n",
    "        for kw_bucket in ctx_bucket[\"keywords\"][\"buckets\"]:\n",
    "            print(f\"      {kw_bucket['val']}  {kw_bucket['keywords_relatedness']['relatedness']}\")\n",
    "        print (\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 6.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: server\n",
      "  Context: devops  0.83796\n",
      "    Keywords: \n",
      "      server  0.93698\n",
      "      servers  0.76818\n",
      "      docker  0.75955\n",
      "      code  0.72832\n",
      "      configuration  0.70686\n",
      "      deploy  0.70634\n",
      "      nginx  0.70366\n",
      "      jenkins  0.69934\n",
      "      git  0.68932\n",
      "      ssh  0.6836\n",
      "\n",
      "\n",
      "  Context: cooking  -0.1574\n",
      "    Keywords: \n",
      "      server  0.66363\n",
      "      restaurant  0.16482\n",
      "      pie  0.12882\n",
      "      served  0.12098\n",
      "      restaurants  0.11679\n",
      "      knife  0.10788\n",
      "      pieces  0.10135\n",
      "      serve  0.08934\n",
      "      staff  0.0886\n",
      "      dish  0.08553\n",
      "\n",
      "\n",
      "  Context: travel  -0.15959\n",
      "    Keywords: \n",
      "      server  0.81226\n",
      "      tipping  0.54391\n",
      "      vpn  0.45352\n",
      "      tip  0.41117\n",
      "      servers  0.39053\n",
      "      firewall  0.33092\n",
      "      restaurant  0.21698\n",
      "      tips  0.19524\n",
      "      bill  0.18951\n",
      "      cash  0.18485\n",
      "\n",
      "\n",
      "  Context: scifi  -0.28208\n",
      "    Keywords: \n",
      "      server  0.78173\n",
      "      flynn's  0.53341\n",
      "      computer  0.28075\n",
      "      computers  0.2593\n",
      "      flynn  0.24963\n",
      "      servers  0.24778\n",
      "      grid  0.23889\n",
      "      networking  0.2178\n",
      "      shutdown  0.21121\n",
      "      hacker  0.19444\n",
      "\n",
      "\n",
      "Query: driver\n",
      "  Context: travel  0.38996\n",
      "    Keywords: \n",
      "      driver  0.93417\n",
      "      drivers  0.76932\n",
      "      taxi  0.71977\n",
      "      car  0.65572\n",
      "      license  0.61319\n",
      "      driving  0.60849\n",
      "      taxis  0.57708\n",
      "      traffic  0.52823\n",
      "      bus  0.52306\n",
      "      driver's  0.51043\n",
      "\n",
      "\n",
      "  Context: devops  0.08917\n",
      "    Keywords: \n",
      "      ipam  0.78219\n",
      "      driver  0.77583\n",
      "      aufs  0.73758\n",
      "      overlayfs  0.73758\n",
      "      container_name  0.73483\n",
      "      overlay2  0.69079\n",
      "      cgroup  0.68438\n",
      "      docker  0.67529\n",
      "      compose.yml  0.65012\n",
      "      compose  0.55631\n",
      "\n",
      "\n",
      "Query: chef\n",
      "  Context: cooking  0.37731\n",
      "    Keywords: \n",
      "      chef  0.93239\n",
      "      chefs  0.5151\n",
      "      www.pamperedchef.com  0.41292\n",
      "      kitchen  0.39127\n",
      "      restaurant  0.38975\n",
      "      cooking  0.38332\n",
      "      chef's  0.37392\n",
      "      professional  0.36688\n",
      "      nakiri  0.36599\n",
      "      pampered  0.34736\n",
      "\n",
      "\n",
      "  Context: devops  0.34959\n",
      "    Keywords: \n",
      "      chef  0.87653\n",
      "      puppet  0.79142\n",
      "      docs.chef.io  0.7865\n",
      "      ansible  0.73888\n",
      "      www.chef.io  0.72073\n",
      "      learn.chef.io  0.71902\n",
      "      default.rb  0.70194\n",
      "      configuration  0.68296\n",
      "      inspec  0.65237\n",
      "      cookbooks  0.61503\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_disambiguation_query(\"server\")\n",
    "run_disambiguation_query(\"driver\", context_limit=2)\n",
    "run_disambiguation_query(\"chef\", context_limit=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Examples: Job Skills (not included in chapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jobs_collection=engine.get_collection(\"jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_related_job_terms(query):\n",
    "    request = {\n",
    "        \"params\": {\n",
    "            \"qf\": \"job_description job_title\",\n",
    "            \"fore\": \"{!type=$defType qf=$qf v=$q}\",\n",
    "            \"back\": \"*:*\",\n",
    "            \"defType\": \"edismax\",\n",
    "            \"rows\": 0,\n",
    "            \"echoParams\": \"none\",\n",
    "            \"omitHeader\": \"true\"\n",
    "        },\n",
    "        \"query\": f\"\\\"{query}\\\"\",\n",
    "        \"facet\": {\n",
    "            \"job_description_keywords\": {\n",
    "                \"type\": \"terms\",\n",
    "                \"field\": \"job_description\",\n",
    "                \"sort\": { \"relatedness\": \"desc\"},\n",
    "                \"facet\": {\n",
    "                    \"relatedness\": {\n",
    "                        \"type\": \"func\",\n",
    "                        \"func\": \"relatedness($fore,$back)\",\n",
    "                        \"min_popularity\": 0.0005\n",
    "                    }\n",
    "                }            \n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = jobs_collection.search(request)\n",
    "\n",
    "    print(f\"{query}\\n------------\")\n",
    "    for bucket in response[\"facets\"][\"job_description_keywords\"][\"buckets\"]:\n",
    "        print(str(bucket[\"val\"]) + \"  \" + str(bucket[\"relatedness\"][\"relatedness\"]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark\n",
      "------------\n",
      "spark  0.80665\n",
      "hadoop  0.59424\n",
      "hive  0.52983\n",
      "kafka  0.51552\n",
      "impala  0.45309\n",
      "streamsets  0.39341\n",
      "scala  0.38564\n",
      "flume  0.38401\n",
      "mapreduce  0.36195\n",
      "nosql  0.35116\n",
      "\n",
      "\n",
      "registered nurse\n",
      "------------\n",
      "nurse  0.64418\n",
      "rn  0.59936\n",
      "registered  0.42736\n",
      "nursing  0.33399\n",
      "clinical  0.23799\n",
      "hospital  0.20062\n",
      "families  0.14216\n",
      "patients  0.13847\n",
      "practice  0.13157\n",
      "care  0.12564\n",
      "\n",
      "\n",
      "docker\n",
      "------------\n",
      "docker  0.80683\n",
      "kubernetes  0.69719\n",
      "jenkins  0.49458\n",
      "container  0.49226\n",
      "openshift  0.45521\n",
      "aws  0.41473\n",
      "microservices  0.39713\n",
      "cd  0.39409\n",
      "ci  0.38633\n",
      "devops  0.38085\n",
      "\n",
      "\n",
      "personal trainer\n",
      "------------\n",
      "nsca  0.74439\n",
      "nasm  0.71632\n",
      "acsm  0.66348\n",
      "nfpt  0.66004\n",
      "trainer  0.63695\n",
      "issa  0.62555\n",
      "aerobic  0.62328\n",
      "cpr  0.53834\n",
      "ace  0.50328\n",
      "distant  0.46272\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_related_job_terms(\"spark\")\n",
    "print_related_job_terms(\"registered nurse\")\n",
    "print_related_job_terms(\"docker\")\n",
    "print_related_job_terms(\"personal trainer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success!\n",
    "\n",
    "You've leveraged a semantic knowledge graph to find related terms for a query, performed query expansion based upon semantically-similar terms, explored multiple different way to impact precision and recall of queries through integrating semantically-augmented queries, generated content-based recommendations leveraging a semantic knowledge graph, explored arbitrary relationship types by traversing a semantic knowledge graph, and performed both query classification and query disambiguration using a semantic knowledge graph.\n",
    "\n",
    "Semantic knowledge graphs can be a powerful tool for understaning user intent and interpreting both queries and content based upon meaning instead of just text kewords.\n",
    "\n",
    "Up next: [Related Keyword Detection from Signals](../ch06/2.related-keywords-from-signals.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
