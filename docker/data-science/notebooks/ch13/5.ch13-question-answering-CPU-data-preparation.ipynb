{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import html\n",
    "import pandas\n",
    "import pickle\n",
    "import json\n",
    "sys.path.append('..')\n",
    "from aips import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') #Some operations warn inside a loop, we'll only need to see the first warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/sbert/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "outdoors_collection=\"outdoors\"\n",
    "path = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/sbert/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def getQuestions():\n",
    "    qtypes = ['Who','What','When','Where','Why','How']\n",
    "    questions = []\n",
    "    for qt in qtypes:\n",
    "        lq = len(qt)\n",
    "        request = {\n",
    "            \"query\": qt,\n",
    "            \"fields\": [\"id\", \"url\", \"owner_user_id\", \"title\", \"accepted_answer_id\"],\n",
    "            \"params\": {\n",
    "              \"qf\": [\"title\"],\n",
    "              \"fq\": [\"accepted_answer_id:[* TO *]\"],\n",
    "              \"defType\": \"edismax\",\n",
    "              \"rows\":10000\n",
    "            }\n",
    "        }\n",
    "        docs = requests.post(solr_url + outdoors_collection + \"/select\", json=request).json()[\"response\"][\"docs\"]\n",
    "        questions += [doc for doc in docs if doc['title'][0:lq]==qt]             \n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getContextDataFrame(questions):\n",
    "    contexts={\"id\":[],\"question\":[],\"context\":[],\"url\":[]}\n",
    "    for question in questions:\n",
    "        request = {\n",
    "            \"query\": \"*:*\",\n",
    "            \"fields\": [\"body\"],\n",
    "            \"params\": {\n",
    "              \"fq\": [\"id:\"+str(question[\"accepted_answer_id\"])],\n",
    "              \"defType\": \"edismax\",\n",
    "              \"rows\":1,\n",
    "              \"sort\":\"score desc\"\n",
    "            }\n",
    "        }\n",
    "        docs = requests.post(solr_url + outdoors_collection + \"/select\", json=request).json()[\"response\"][\"docs\"]\n",
    "        contexts[\"id\"].append(question[\"id\"])\n",
    "        contexts[\"url\"].append(question[\"url\"])\n",
    "        contexts[\"question\"].append(question[\"title\"]),\n",
    "        contexts[\"context\"].append(docs[0][\"body\"])\n",
    "    return pandas.DataFrame(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4410</td>\n",
       "      <td>Who places the anchors that rock climbers use?</td>\n",
       "      <td>There are two distinct styles of free rock cli...</td>\n",
       "      <td>https://outdoors.stackexchange.com/questions/4410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5347</td>\n",
       "      <td>Who places the bolts on rock climbing routes, ...</td>\n",
       "      <td>What you're talking about is Sport climbing. G...</td>\n",
       "      <td>https://outdoors.stackexchange.com/questions/5347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20662</td>\n",
       "      <td>Who gets the bill if you activate a PLB to hel...</td>\n",
       "      <td>Almost always the victim gets the bill, but as...</td>\n",
       "      <td>https://outdoors.stackexchange.com/questions/2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7623</td>\n",
       "      <td>What knot is this one? What are its purposes?</td>\n",
       "      <td>Slip knot It's undoubtably a slip knot that's ...</td>\n",
       "      <td>https://outdoors.stackexchange.com/questions/7623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11587</td>\n",
       "      <td>What sort of crane, and what sort of snake?</td>\n",
       "      <td>To answer the snake part of it, looking at som...</td>\n",
       "      <td>https://outdoors.stackexchange.com/questions/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1660</td>\n",
       "      <td>What is Geocaching?</td>\n",
       "      <td>In short, it's a high-tech treasure hunt. geoc...</td>\n",
       "      <td>https://outdoors.stackexchange.com/questions/1660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>913</td>\n",
       "      <td>What is a buff?</td>\n",
       "      <td>To be honest I was dubious about getting somet...</td>\n",
       "      <td>https://outdoors.stackexchange.com/questions/913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4904</td>\n",
       "      <td>What Rope to purchase?</td>\n",
       "      <td>Short answer: For your first rope, none of the...</td>\n",
       "      <td>https://outdoors.stackexchange.com/questions/4904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6397</td>\n",
       "      <td>What is a bloquers?</td>\n",
       "      <td>I can only assume, that it derives from bloque...</td>\n",
       "      <td>https://outdoors.stackexchange.com/questions/6397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10173</td>\n",
       "      <td>What is a longbow?</td>\n",
       "      <td>The problem with the longbow discussion is tha...</td>\n",
       "      <td>https://outdoors.stackexchange.com/questions/1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                           question  \\\n",
       "0   4410     Who places the anchors that rock climbers use?   \n",
       "1   5347  Who places the bolts on rock climbing routes, ...   \n",
       "2  20662  Who gets the bill if you activate a PLB to hel...   \n",
       "3   7623      What knot is this one? What are its purposes?   \n",
       "4  11587        What sort of crane, and what sort of snake?   \n",
       "5   1660                                What is Geocaching?   \n",
       "6    913                                    What is a buff?   \n",
       "7   4904                             What Rope to purchase?   \n",
       "8   6397                                What is a bloquers?   \n",
       "9  10173                                 What is a longbow?   \n",
       "\n",
       "                                             context  \\\n",
       "0  There are two distinct styles of free rock cli...   \n",
       "1  What you're talking about is Sport climbing. G...   \n",
       "2  Almost always the victim gets the bill, but as...   \n",
       "3  Slip knot It's undoubtably a slip knot that's ...   \n",
       "4  To answer the snake part of it, looking at som...   \n",
       "5  In short, it's a high-tech treasure hunt. geoc...   \n",
       "6  To be honest I was dubious about getting somet...   \n",
       "7  Short answer: For your first rope, none of the...   \n",
       "8  I can only assume, that it derives from bloque...   \n",
       "9  The problem with the longbow discussion is tha...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://outdoors.stackexchange.com/questions/4410  \n",
       "1  https://outdoors.stackexchange.com/questions/5347  \n",
       "2  https://outdoors.stackexchange.com/questions/2...  \n",
       "3  https://outdoors.stackexchange.com/questions/7623  \n",
       "4  https://outdoors.stackexchange.com/questions/1...  \n",
       "5  https://outdoors.stackexchange.com/questions/1660  \n",
       "6   https://outdoors.stackexchange.com/questions/913  \n",
       "7  https://outdoors.stackexchange.com/questions/4904  \n",
       "8  https://outdoors.stackexchange.com/questions/6397  \n",
       "9  https://outdoors.stackexchange.com/questions/1...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = getQuestions()\n",
    "contexts = getContextDataFrame(questions)\n",
    "contexts[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contexts.to_csv(path+'data/question-answer-seed-contexts.csv',index=False)\n",
    "contexts = pandas.read_csv(path+'data/question-answer-seed-contexts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "import tqdm\n",
    "\n",
    "model_name = 'deepset/roberta-base-squad2'\n",
    "pipeline_type = 'question-answering'\n",
    "\n",
    "device=-1 #CPU\n",
    "#device=0 #<-- Uncomment to use GPU, if you are running in Google Colab\n",
    "\n",
    "def answerQuestions(contexts,k=10):\n",
    "    nlp = pipeline(pipeline_type, model=model_name, tokenizer=model_name, device=device)\n",
    "    guesses = []\n",
    "    for idx,row in tqdm.tqdm(contexts[0:k].iterrows(),total=k):\n",
    "        result = nlp({\"question\":row[\"question\"],\"context\":row[\"context\"]})\n",
    "        guesses.append(result)\n",
    "    return guesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at deepset/roberta-base-squad2 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1662/1662 [43:19<00:00,  1.56s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.2776225209236145, 'start': 474, 'end': 517, 'answer': 'a local enthusiast or group of enthusiasts.'}, {'score': 0.1954791247844696, 'start': 81, 'end': 118, 'answer': 'the person who is creating the climb.'}, {'score': 0.024139929562807083, 'start': 14, 'end': 24, 'answer': 'the victim'}, {'score': 0.3299234211444855, 'start': 29, 'end': 38, 'answer': 'slip knot'}, {'score': 0.0005422658286988735, 'start': 1255, 'end': 1262, 'answer': 'aquatic'}, {'score': 0.37733304500579834, 'start': 15, 'end': 41, 'answer': 'a high-tech treasure hunt.'}, {'score': 0.5653401613235474, 'start': 192, 'end': 233, 'answer': 'a tube of lightweight, stretchy material.'}, {'score': 0.10057392716407776, 'start': 125, 'end': 155, 'answer': 'the cheapest one of the three,'}, {'score': 0.7781326174736023, 'start': 68, 'end': 77, 'answer': 'blocking.'}, {'score': 0.2520507276058197, 'start': 227, 'end': 266, 'answer': 'the traditional longbow made from wood,'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "guesses = answerQuestions(contexts,k=len(contexts))\n",
    "contexts[\"answers\"] = guesses\n",
    "print(guesses[0:10])\n",
    "\"\"\"\n",
    "Some weights of RobertaModel were not initialized from the model checkpoint at deepset/roberta-base-squad2 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1662/1662 [43:19<00:00,  1.56s/it] \n",
    "\n",
    "[{'score': 0.2776225209236145, 'start': 474, 'end': 517, 'answer': 'a local enthusiast or group of enthusiasts.'}, {'score': 0.1954791247844696, 'start': 81, 'end': 118, 'answer': 'the person who is creating the climb.'}, {'score': 0.024139929562807083, 'start': 14, 'end': 24, 'answer': 'the victim'}, {'score': 0.3299234211444855, 'start': 29, 'end': 38, 'answer': 'slip knot'}, {'score': 0.0005422658286988735, 'start': 1255, 'end': 1262, 'answer': 'aquatic'}, {'score': 0.37733304500579834, 'start': 15, 'end': 41, 'answer': 'a high-tech treasure hunt.'}, {'score': 0.5653401613235474, 'start': 192, 'end': 233, 'answer': 'a tube of lightweight, stretchy material.'}, {'score': 0.10057392716407776, 'start': 125, 'end': 155, 'answer': 'the cheapest one of the three,'}, {'score': 0.7781326174736023, 'start': 68, 'end': 77, 'answer': 'blocking.'}, {'score': 0.2520507276058197, 'start': 227, 'end': 266, 'answer': 'the traditional longbow made from wood,'}]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts.to_csv(path+'data/question-answer-squad2-guesses.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1662\n",
      "{\n",
      "  \"question\": \"Who places the anchors that rock climbers use?\",\n",
      "  \"context\": \"There are two distinct styles of free rock climbing (with, as always, some grey areas). In \\\"trad\\\" (= traditional) climbing, the team carries their own removable gear. The leader places protection points whilst climbing and the second climber removes them again. There might be the odd piece of permanent gear - pitons or bolts - but the climbing team is basically responsible for arranging their own protection. In sport climbing, bolts are pre-placed by somebody , usually a local enthusiast or group of enthusiasts. How stuff is funded/maintained varies a lot from area to area, but it is almost always voluntary efforts of some kind. Formal standards don't exist, although most areas have best practices that are generally understood and adhered to by people putting in or maintaining fixed equipment. Exactly what these best practices are vary a lot from area to area depending on type of rock, proximity to salt water etc. IANAL, but from what I've read there might be some question of legal liability if people putting in fixed gear blatantly violate local best practices. Bolt spacing norms in particular vary a lot from area to area. One of my local areas is the Frankenjura. There the locals generally bolt the routes so you won't hit the ground, but they don't care how scared you are. So you might be looking at doing hard climbing much further above a - solid - bolt than you are used to or comfortable with. A couple of hours in the opposite direction is Nassereith in Austria, where the bolt spacing makes some climbing gyms feel terrifying. (Sadly the climbing and scenery there are nowhere near as good as in the Frankenjura)\",\n",
      "  \"result\": {\n",
      "    \"score\": 0.322016179561615,\n",
      "    \"start\": 474,\n",
      "    \"end\": 516,\n",
      "    \"answer\": \"a local enthusiast or group of enthusiasts\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "with open('data/guesses_all.json','r') as fd:\n",
    "    guesses_all = json.load(fd)\n",
    "print(len(guesses_all))\n",
    "print(json.dumps(guesses_all[0],indent=2))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>score</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Who places the anchors that rock climbers use?</td>\n",
       "      <td>There are two distinct styles of free rock cli...</td>\n",
       "      <td>0.322016</td>\n",
       "      <td>474</td>\n",
       "      <td>516</td>\n",
       "      <td>a local enthusiast or group of enthusiasts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Who places the bolts on rock climbing routes, ...</td>\n",
       "      <td>What you're talking about is Sport climbing. G...</td>\n",
       "      <td>0.217747</td>\n",
       "      <td>81</td>\n",
       "      <td>117</td>\n",
       "      <td>the person who is creating the climb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Who gets the bill if you activate a PLB to hel...</td>\n",
       "      <td>Almost always the victim gets the bill, but as...</td>\n",
       "      <td>0.518512</td>\n",
       "      <td>2347</td>\n",
       "      <td>2357</td>\n",
       "      <td>the victim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>What knot is this one? What are its purposes?</td>\n",
       "      <td>Slip knot It's undoubtably a slip knot that's ...</td>\n",
       "      <td>0.229166</td>\n",
       "      <td>29</td>\n",
       "      <td>38</td>\n",
       "      <td>slip knot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>What sort of crane, and what sort of snake?</td>\n",
       "      <td>To answer the snake part of it, looking at som...</td>\n",
       "      <td>0.124369</td>\n",
       "      <td>1255</td>\n",
       "      <td>1262</td>\n",
       "      <td>aquatic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1657</th>\n",
       "      <td>1657</td>\n",
       "      <td>How much margin due to charging inefficiency s...</td>\n",
       "      <td>The overall efficiency from a power bank to a ...</td>\n",
       "      <td>0.490509</td>\n",
       "      <td>887</td>\n",
       "      <td>890</td>\n",
       "      <td>50%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1658</th>\n",
       "      <td>1658</td>\n",
       "      <td>How do you dry wet socks when camping/backpack...</td>\n",
       "      <td>Sleeping with the socks on your torso is the m...</td>\n",
       "      <td>0.063256</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>Sleeping with the socks on your torso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1659</th>\n",
       "      <td>1659</td>\n",
       "      <td>How do simple compass needles remain level ins...</td>\n",
       "      <td>The short answer is that the manufacturers kno...</td>\n",
       "      <td>0.013619</td>\n",
       "      <td>223</td>\n",
       "      <td>262</td>\n",
       "      <td>balancing a compass for a specific zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1660</th>\n",
       "      <td>1660</td>\n",
       "      <td>How do I apply for a permit for the Zona de Re...</td>\n",
       "      <td>The phone number is right: + 34 956 70 97 33 (...</td>\n",
       "      <td>0.182854</td>\n",
       "      <td>99</td>\n",
       "      <td>112</td>\n",
       "      <td>send an email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1661</th>\n",
       "      <td>1661</td>\n",
       "      <td>How do you protect yourself against crocodiles...</td>\n",
       "      <td>If I were in such a situation, and not alone, ...</td>\n",
       "      <td>0.200273</td>\n",
       "      <td>67</td>\n",
       "      <td>92</td>\n",
       "      <td>rotating shifts of sentry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1662 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                           question  \\\n",
       "0        0     Who places the anchors that rock climbers use?   \n",
       "1        1  Who places the bolts on rock climbing routes, ...   \n",
       "2        2  Who gets the bill if you activate a PLB to hel...   \n",
       "3        3      What knot is this one? What are its purposes?   \n",
       "4        4        What sort of crane, and what sort of snake?   \n",
       "...    ...                                                ...   \n",
       "1657  1657  How much margin due to charging inefficiency s...   \n",
       "1658  1658  How do you dry wet socks when camping/backpack...   \n",
       "1659  1659  How do simple compass needles remain level ins...   \n",
       "1660  1660  How do I apply for a permit for the Zona de Re...   \n",
       "1661  1661  How do you protect yourself against crocodiles...   \n",
       "\n",
       "                                                context     score  start  \\\n",
       "0     There are two distinct styles of free rock cli...  0.322016    474   \n",
       "1     What you're talking about is Sport climbing. G...  0.217747     81   \n",
       "2     Almost always the victim gets the bill, but as...  0.518512   2347   \n",
       "3     Slip knot It's undoubtably a slip knot that's ...  0.229166     29   \n",
       "4     To answer the snake part of it, looking at som...  0.124369   1255   \n",
       "...                                                 ...       ...    ...   \n",
       "1657  The overall efficiency from a power bank to a ...  0.490509    887   \n",
       "1658  Sleeping with the socks on your torso is the m...  0.063256      0   \n",
       "1659  The short answer is that the manufacturers kno...  0.013619    223   \n",
       "1660  The phone number is right: + 34 956 70 97 33 (...  0.182854     99   \n",
       "1661  If I were in such a situation, and not alone, ...  0.200273     67   \n",
       "\n",
       "       end                                      answer  \n",
       "0      516  a local enthusiast or group of enthusiasts  \n",
       "1      117        the person who is creating the climb  \n",
       "2     2357                                  the victim  \n",
       "3       38                                   slip knot  \n",
       "4     1262                                     aquatic  \n",
       "...    ...                                         ...  \n",
       "1657   890                                         50%  \n",
       "1658    37       Sleeping with the socks on your torso  \n",
       "1659   262     balancing a compass for a specific zone  \n",
       "1660   112                               send an email  \n",
       "1661    92                   rotating shifts of sentry  \n",
       "\n",
       "[1662 rows x 7 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def guessesToDataFrame(guesses):\n",
    "    table={\n",
    "        \"id\":[],\n",
    "        \"question\":[],\n",
    "        \"context\":[],\n",
    "        #\"result\":[],\n",
    "        \"score\":[],\n",
    "        \"start\":[],\n",
    "        \"end\":[],\n",
    "        \"answer\":[]\n",
    "    }\n",
    "    i=0\n",
    "    for guess in guesses:\n",
    "        table[\"id\"].append(i)\n",
    "        table[\"question\"].append(guess[\"question\"])\n",
    "        table[\"context\"].append(guess[\"context\"])\n",
    "        #table[\"result\"].append(guess[\"result\"])\n",
    "        table[\"score\"].append(guess[\"result\"][\"score\"])\n",
    "        table[\"start\"].append(guess[\"result\"][\"start\"])\n",
    "        table[\"end\"].append(guess[\"result\"][\"end\"])\n",
    "        table[\"answer\"].append(guess[\"result\"][\"answer\"])\n",
    "        i+=1\n",
    "\n",
    "    return pandas.DataFrame(table)\n",
    "guesses_df = guessesToDataFrame(guesses_all)\n",
    "guesses_df\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#guesses_df.to_csv('data/guesses.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/sbert/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "def getTrainingData(filename):\n",
    "    golden_answers = pandas.read_csv(filename)\n",
    "    golden_answers[\"class\"] = golden_answers[\"class\"].fillna(-2).astype(int)\n",
    "    validated=golden_answers[golden_answers[\"class\"]>-1]\n",
    "    \n",
    "    table={\n",
    "        \"id\":[],\n",
    "        \"url\":[],\n",
    "        \"title\":[],\n",
    "        \"question\":[],\n",
    "        \"context\":[],\n",
    "        \"answers\":[],\n",
    "        \"owner_user_id\":[]\n",
    "    }\n",
    "    \n",
    "    for idx,row in validated.iterrows():\n",
    "        answers = row[\"gold\"].split('|')\n",
    "        starts = []\n",
    "        notfound = False\n",
    "        for i in range(len(answers)):\n",
    "            found = row[\"context\"].find(answers[i])\n",
    "            starts.append(found)\n",
    "            if(found<0):\n",
    "                notfound = True\n",
    "        if not notfound:\n",
    "            table[\"id\"].append(row[\"id\"])\n",
    "            table[\"url\"].append(row[\"url\"])\n",
    "            table[\"title\"].append(row[\"question\"])\n",
    "            table[\"question\"].append(row[\"question\"])\n",
    "            table[\"context\"].append(row[\"context\"])\n",
    "            table[\"answers\"].append({\n",
    "                \"text\":answers,\n",
    "                \"answer_start\":starts\n",
    "            })\n",
    "            table[\"owner_user_id\"].append(row[\"owner_user_id\"])\n",
    "    df = pandas.DataFrame(table).sample(frac=1)\n",
    "    train_split = int(len(df)*0.8)\n",
    "    train_dataset = datasets.Dataset.from_pandas(df[:train_split])\n",
    "    test_dataset = datasets.Dataset.from_pandas(df[train_split:])\n",
    "    datadict = datasets.DatasetDict({'train':train_dataset,'test':test_dataset})\n",
    "    return datadict\n",
    "\n",
    "#This golden answers file was labeled by me (Max Irwin).\n",
    "#It took about 2-3 hours to label 200 question/answer rows\n",
    "#Doing so will give you a deeper appreciation for the difficulty of the NLP task.\n",
    "#I *highly* encourage you to label even more documents, and re-run the fine-tuning tasks coming up.\n",
    "datadict = getTrainingData(path+'data/outdoors_golden_answers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/sbert/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'question', 'context', 'answers', '__index_level_0__'],\n",
       "    num_rows: 86\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "tokenizer = transformers.RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 4), (5, 10), (11, 17), (18, 19), (20, 25), (26, 30), (31, 35), (35, 42), (42, 43), (0, 0), (0, 0), (0, 1), (2, 5), (6, 8), (9, 14), (15, 18), (19, 28), (28, 29), (30, 33), (33, 38), (39, 44), (45, 46), (46, 47), (47, 48), (48, 49), (49, 51), (52, 56), (57, 61), (62, 65), (66, 70), (71, 73), (74, 79), (80, 82), (83, 85), (85, 87), (88, 91), (92, 101), (102, 104), (105, 110), (111, 117), (118, 121), (122, 132), (132, 133), (134, 139), (140, 143), (144, 149), (150, 153), (154, 160), (161, 163), (164, 166), (167, 174), (174, 175), (176, 179), (179, 180), (180, 184), (185, 190), (190, 195), (196, 200), (200, 207), (208, 212), (212, 213), (214, 218), (218, 224), (225, 228), (229, 231), (231, 234), (234, 236), (236, 241), (241, 242), (243, 246), (247, 250), (251, 256), (257, 259), (260, 264), (265, 270), (271, 273), (274, 277), (278, 282), (282, 289), (290, 297), (297, 299), (300, 306), (307, 313), (314, 317), (318, 321), (322, 327), (328, 333), (334, 340), (340, 341), (342, 345), (346, 350), (351, 355), (356, 360), (361, 364), (365, 369), (370, 372), (373, 377), (377, 378), (379, 382)]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/envs/sbert/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sequence_ids'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-ccc447e7cefb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_example\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"offset_mapping\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0msequence_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenized_example\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/sbert/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "example = training_data[0]\n",
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=384,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    stride=127\n",
    ")\n",
    "\n",
    "assert isinstance(tokenized_example, transformers.BatchEncoding)\n",
    "\n",
    "print(tokenized_example[\"offset_mapping\"][0][:100])\n",
    "sequence_ids = tokenized_example.sequence_ids(1)\n",
    "print(sequence_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=384, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/envs/sbert/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sequence_ids'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3340431c4d0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenized_examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-3340431c4d0d>\u001b[0m in \u001b[0;36mprepare_dataset\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Grab the sequence corresponding to that example (to know what is the context and what is the question).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0msequence_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenized_examples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# One example can give several spans, this is the index of the example containing this span of text.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/sbert/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def prepare_dataset(examples):\n",
    "\n",
    "    maximum_tokens = 384 # This will be the number of tokens in BOTH the question and context\n",
    "    document_overlap = 128 # Sometimes we need to split the context into smaller chunks, so we will overlap with this window\n",
    "    pad_on_right = tokenizer.padding_side == \"right\"\n",
    "    \n",
    "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=maximum_tokens,\n",
    "        stride=document_overlap,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    print(tokenized_examples[0])\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples\n",
    "\n",
    "train_dataset = prepare_dataset(training_data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForQuestionAnswering, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "model = RobertaForQuestionAnswering.from_pretrained('deepset/roberta-base-squad2')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='data/questionanswering/results',          # output directory\n",
    "    num_train_epochs=3,                                   # total # of training epochs\n",
    "    per_device_train_batch_size=16,                       # batch size per device during training\n",
    "    per_device_eval_batch_size=64,                        # batch size for evaluation\n",
    "    warmup_steps=500,                                     # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,                                    # strength of weight decay\n",
    "    logging_dir='data/questionanswering/logs',            # directory for storing logs\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=test_dataset            # evaluation dataset\n",
    ")\n",
    "\n",
    "\n",
    "#question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n",
    "#inputs = tokenizer(question, text, return_tensors='pt')\n",
    "#start_positions = torch.tensor([1])\n",
    "#end_positions = torch.tensor([3])\n",
    "\n",
    "#outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)\n",
    "#loss = outputs.loss\n",
    "#start_scores = outputs.start_logits\n",
    "#end_scores = outputs.end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You have to specify either input_ids or inputs_embeds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/sbert/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model_path, trial)\u001b[0m\n\u001b[1;32m    773\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    776\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_total_flos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/sbert/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1110\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/sbert/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0mSubclass\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0moverride\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcustom\u001b[0m \u001b[0mbehavior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \"\"\"\n\u001b[0;32m-> 1136\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1137\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpast_index\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/sbert/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/sbert/lib/python3.7/site-packages/transformers/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1297\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m         )\n\u001b[1;32m   1301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/sbert/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/sbert/lib/python3.7/site-packages/transformers/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You have to specify either input_ids or inputs_embeds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You have to specify either input_ids or inputs_embeds"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
