{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ Chapter 7 - Interpreting Query Intent through Semantic Search ]\n",
    "# Setting up the Reviews Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from aips import *\n",
    "import os\n",
    "from IPython.display import display,HTML\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, col\n",
    "spark = SparkSession.builder.appName(\"AIPS\").getOrCreate()\n",
    "engine = get_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n",
      "._reviews.csv\n",
      "reviews.csv\n",
      "entities.csv\n",
      "._cities.csv\n",
      "cities.csv\n"
     ]
    }
   ],
   "source": [
    "#Get datasets\n",
    "![ ! -d 'reviews' ] && git clone --depth 1 https://github.com/ai-powered-search/reviews.git\n",
    "! cd reviews && git pull\n",
    "! cd reviews && mkdir -p '../../data/reviews/' && tar -xvf reviews.tgz -C '../../data/reviews/' && tar -xvf entities.tgz -C '../../data/reviews/' && tar -xvf cities.tgz -C '../../data/reviews/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviews Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing 7.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_reviews_collection(reviews_collection):\n",
    "    print(\"\\nLoading Reviews...\")\n",
    "    csvFile = \"../data/reviews/reviews.csv\"\n",
    "    reviews_update_opts={\"zkhost\": \"aips-zk\", \"collection\": reviews_collection.name, \n",
    "                        \"gen_uniq_key\": \"true\", \"commit_within\": \"5000\"}\n",
    "    csvDF = spark.read.csv(csvFile, inferSchema=True, header=True, multiLine=True, escape=\"\\\"\") \\\n",
    "        .withColumn(\"poplarity_i\", col(\"stars_i\") * 20) \\\n",
    "        .select(\n",
    "          \"id\", \"name_t\", \"city_t\", \"state_t\", \"text_t\", \"stars_i\", \n",
    "          \"categories_t\",  \"location_pt_s\", \"type_ss\", \"latitude_d\", \"longitude_d\")\n",
    "    csvDF.write.format(\"solr\").options(**reviews_update_opts).mode(\"overwrite\").save()\n",
    "    print(\"Reviews Schema: \")\n",
    "    csvDF.printSchema()\n",
    "    print(\"Status: Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index the Reviews Dataset into the Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing 7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiping \"reviews\" collection\n",
      "Creating \"reviews\" collection\n",
      "Status: Success\n",
      "\n",
      "Loading Reviews...\n",
      "Reviews Schema: \n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name_t: string (nullable = true)\n",
      " |-- city_t: string (nullable = true)\n",
      " |-- state_t: string (nullable = true)\n",
      " |-- text_t: string (nullable = true)\n",
      " |-- stars_i: integer (nullable = true)\n",
      " |-- categories_t: string (nullable = true)\n",
      " |-- location_pt_s: string (nullable = true)\n",
      " |-- type_ss: string (nullable = true)\n",
      " |-- latitude_d: double (nullable = true)\n",
      " |-- longitude_d: double (nullable = true)\n",
      "\n",
      "Status: Success\n"
     ]
    }
   ],
   "source": [
    "reviews_collection = engine.create_collection(\"reviews\")\n",
    "index_reviews_collection(reviews_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enities Dataset (Manually-specified Knowledge Graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing 7.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load -n SolrEngine.add_tag_field_type\n",
    "def add_tag_field_type(self, collection):\n",
    "    request = {\n",
    "        \"add-field-type\": {\n",
    "            \"name\": \"tag\",\n",
    "            \"class\": \"solr.TextField\",\n",
    "            \"postingsFormat\": \"FST50\",\n",
    "            \"omitNorms\": \"true\",\n",
    "            \"omitTermFreqAndPositions\": \"true\",\n",
    "            \"indexAnalyzer\": {\n",
    "                \"tokenizer\": {\n",
    "                    \"class\": \"solr.StandardTokenizerFactory\"},\n",
    "                \"filters\": [\n",
    "                    {\"class\": \"solr.EnglishPossessiveFilterFactory\"},\n",
    "                    {\"class\": \"solr.ASCIIFoldingFilterFactory\"},\n",
    "                    {\"class\": \"solr.LowerCaseFilterFactory\"},\n",
    "                    {\"class\": \"solr.ConcatenateGraphFilterFactory\", \"preservePositionIncrements\": \"false\"}\n",
    "                ]},\n",
    "            \"queryAnalyzer\": {\n",
    "                \"tokenizer\": {\n",
    "                    \"class\": \"solr.StandardTokenizerFactory\"},\n",
    "                \"filters\": [\n",
    "                    {\"class\": \"solr.EnglishPossessiveFilterFactory\"},\n",
    "                    {\"class\": \"solr.ASCIIFoldingFilterFactory\"},\n",
    "                    {\"class\": \"solr.LowerCaseFilterFactory\"}\n",
    "                ]}\n",
    "            }\n",
    "        }\n",
    "    print(f\"{SOLR_URL}/{collection.name}/schema\")\n",
    "    print(requests.post(f\"{SOLR_URL}/{collection.name}/schema\", json=request).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load -n SolrEngine.add_tag_request_handler\n",
    "def add_tag_request_handler(self, collection, request_name, field):\n",
    "    request = {\n",
    "        \"add-requesthandler\" : {\n",
    "            \"name\": request_name,\n",
    "            \"class\": \"solr.TaggerRequestHandler\",\n",
    "            \"defaults\": {\"field\": field}\n",
    "        }\n",
    "    }\n",
    "    return requests.post(f\"{SOLR_URL}/{collection.name}/config\", json=request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cities Dataset (Geonames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modify Schema to make some fields explicitly searchable by keyword\n",
    "#upsert_text_field(jobs_collection, \"company_country\")\n",
    "#upsert_text_field(jobs_collection, \"job_description\")\n",
    "#upsert_text_field(jobs_collection, \"company_description\")\n",
    "#upsert_text_field(products_collection, \"longDescription\")\n",
    "#upsert_text_field(products_collection, \"manufacturer\")\n",
    "\n",
    "def index_cities(collection):\n",
    "    print(\"Loading Geonames...\")\n",
    "    csvFile = \"../data/reviews/cities.csv\"\n",
    "    entities_update_opts={\"zkhost\": \"aips-zk\", \"collection\": collection.name, \n",
    "                          \"gen_uniq_key\": \"true\", \"commit_within\": \"5000\"}\n",
    "\n",
    "    from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "    from pyspark.sql.functions import concat_ws\n",
    "\n",
    "    schema = StructType() \\\n",
    "          .add(\"id\",StringType(),True) \\\n",
    "          .add(\"name\",StringType(),True) \\\n",
    "          .add(\"ascii_name_s\",StringType(),True) \\\n",
    "          .add(\"alternative_names_s\",StringType(),True) \\\n",
    "          .add(\"latitude_s\",StringType(),True) \\\n",
    "          .add(\"longitude_s\",StringType(),True) \\\n",
    "          .add(\"feature_class_s\",StringType(),True) \\\n",
    "          .add(\"feature_code_s\",StringType(),True) \\\n",
    "          .add(\"StringType\",StringType(),True) \\\n",
    "          .add(\"cc2_s\",StringType(),True) \\\n",
    "          .add(\"admin_code_1_s\",StringType(),True) \\\n",
    "          .add(\"admin_code_2_s\",StringType(),True) \\\n",
    "          .add(\"admin_code_3_s\",StringType(),True) \\\n",
    "          .add(\"admin_code_4_s\",StringType(),True) \\\n",
    "          .add(\"population_i\",IntegerType(),True) \\\n",
    "          .add(\"elevation_s\",StringType(),True) \\\n",
    "          .add(\"dem_s\",StringType(),True) \\\n",
    "          .add(\"timezone_s\",StringType(),True) \\\n",
    "          .add(\"modification_date_s\",StringType(),True)\n",
    "\n",
    "    csvDF = spark.read.csv(csvFile, schema=schema, multiLine=True, escape=\"\\\\\", sep=\"\\t\") \\\n",
    "        .withColumn(\"type\", lit(\"city\")) \\\n",
    "        .withColumn(\"location_p\", concat_ws(\",\", \"latitude_s\", \"longitude_s\"))\n",
    "        #.show()\n",
    "\n",
    "    csvDF.write.format(\"solr\").options(**entities_update_opts).mode(\"overwrite\").save()\n",
    "    #print(\"Entities Schema: \")\n",
    "    #csvDF.printSchema()\n",
    "    print(\"Status: Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing 7.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiping \"entities\" collection\n",
      "Creating \"entities\" collection\n",
      "http://aips-solr:8983/solr/entities/schema\n",
      "{\n",
      "  \"responseHeader\":{\n",
      "    \"status\":0,\n",
      "    \"QTime\":584}}\n",
      "\n",
      "Status: Success\n",
      "Loading entities\n",
      "entities Schema: \n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- surface_form: string (nullable = true)\n",
      " |-- canonical_form: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- popularity: integer (nullable = true)\n",
      " |-- semantic_function: string (nullable = true)\n",
      "\n",
      "Status: Success\n",
      "Loading Geonames...\n",
      "Status: Success\n"
     ]
    }
   ],
   "source": [
    "entitites_collection = engine.create_collection(\"entities\")\n",
    "entitites_collection.write_from_csv(\"../data/reviews/entities.csv\")\n",
    "index_cities(entitites_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success!\n",
    "\n",
    "Now that you've indexed the Reviews Dataset and semantic data, it's time to test our end to end semantic search example!\n",
    "\n",
    "Up next: [Semantic search](2.semantic-search.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
