{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the Knowledge Graph Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from aips import *\n",
    "import os\n",
    "from IPython.core.display import display,HTML\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "spark = SparkSession.builder.appName(\"ch5\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jobs\n",
    "![ ! -d 'jobs' ] && git clone https://github.com/ai-powered-search/jobs.git\n",
    "! cd jobs && git pull\n",
    "! cd jobs && mkdir -p '../../data/jobs/' && tar -xvf jobs.tgz -C '../../data/jobs/'    \n",
    "\n",
    "#health\n",
    "![ ! -d 'health' ] && git clone https://github.com/ai-powered-search/health.git\n",
    "! cd health && git pull\n",
    "! cd health && mkdir -p '../../data/health/' && tar -xvf health.tgz -C '../../data/health/'\n",
    "\n",
    "#scifi\n",
    "![ ! -d 'scifi' ] && git clone https://github.com/ai-powered-search/scifi.git\n",
    "! cd scifi && git pull\n",
    "! cd scifi && mkdir -p '../../data/scifi/' && tar -xvf scifi.tgz -C '../../data/scifi/' \n",
    "\n",
    "#cooking\n",
    "![ ! -d 'cooking' ] && git clone https://github.com/ai-powered-search/cooking.git\n",
    "! cd cooking && git pull\n",
    "! cd cooking && mkdir -p '../../data/cooking/' && tar -xvf cooking.tgz -C '../../data/cooking/'\n",
    "\n",
    "#outdoors\n",
    "![ ! -d 'outdoors' ] && git clone https://github.com/ai-powered-search/outdoors.git\n",
    "! cd outdoors && git pull\n",
    "! cd outdoors && mkdir -p '../../data/outdoors/' && tar -xvf outdoors.tgz -C '../../data/outdoors/'\n",
    "\n",
    "#outdoors\n",
    "![ ! -d 'travel' ] && git clone https://github.com/ai-powered-search/travel.git\n",
    "! cd travel && git pull\n",
    "! cd travel && mkdir -p '../../data/travel/' && tar -xvf travel.tgz -C '../../data/travel/'\n",
    "\n",
    "#devops\n",
    "![ ! -d 'devops' ] && git clone https://github.com/ai-powered-search/devops.git\n",
    "! cd travel && git pull\n",
    "! cd travel && mkdir -p '../../data/devops/' && tar -xvf travel.tgz -C '../../data/devops/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index the Jobs Dataset into the Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiping 'jobs' collection\n",
      "Creating jobs' collection\n",
      "Status: Success\n",
      "Adding 'company_country' field to collection\n",
      "Status: Success\n",
      "Adding 'job_description' field to collection\n",
      "Status: Success\n",
      "Adding 'company_description' field to collection\n",
      "Status: Success\n",
      "Loading Jobs...\n",
      "Jobs Schema: \n",
      "root\n",
      " |-- job_title: string (nullable = true)\n",
      " |-- job_description: string (nullable = true)\n",
      " |-- job_type: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- job_location: string (nullable = true)\n",
      " |-- job_city: string (nullable = true)\n",
      " |-- job_state: string (nullable = true)\n",
      " |-- job_country: string (nullable = true)\n",
      " |-- job_zip_code: string (nullable = true)\n",
      " |-- job_address: string (nullable = true)\n",
      " |-- min_salary: string (nullable = true)\n",
      " |-- max_salary: string (nullable = true)\n",
      " |-- salary_period: string (nullable = true)\n",
      " |-- apply_url: string (nullable = true)\n",
      " |-- apply_email: string (nullable = true)\n",
      " |-- num_employees: string (nullable = true)\n",
      " |-- industry: string (nullable = true)\n",
      " |-- company_name: string (nullable = true)\n",
      " |-- company_email: string (nullable = true)\n",
      " |-- company_website: string (nullable = true)\n",
      " |-- company_phone: string (nullable = true)\n",
      " |-- company_logo: string (nullable = true)\n",
      " |-- company_description: string (nullable = true)\n",
      " |-- company_location: string (nullable = true)\n",
      " |-- company_city: string (nullable = true)\n",
      " |-- company_state: string (nullable = true)\n",
      " |-- company_country: string (nullable = true)\n",
      " |-- company_zip_code: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- job_date: string (nullable = true)\n",
      "\n",
      "Status: Success\n"
     ]
    }
   ],
   "source": [
    "#Create Jobs Collection\n",
    "jobs_collection=\"jobs\"\n",
    "create_collection(jobs_collection)\n",
    "\n",
    "#Modify Schema to make some fields explicitly searchable by keyword\n",
    "upsert_text_field(jobs_collection, \"company_country\")\n",
    "upsert_text_field(jobs_collection, \"job_description\")\n",
    "upsert_text_field(jobs_collection, \"company_description\")\n",
    "#upsert_text_field(products_collection, \"longDescription\")\n",
    "#upsert_text_field(products_collection, \"manufacturer\")\n",
    "\n",
    "print(\"Loading Jobs...\")\n",
    "csvFile = \"../data/jobs/jobs.csv\"\n",
    "jobs_update_opts={\"zkhost\": \"aips-zk\", \"collection\": jobs_collection, \"gen_uniq_key\": \"true\", \"commit_within\": \"5000\"}\n",
    "csvDF = spark.read.format(\"com.databricks.spark.csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"charset\", \"utf-8\") \\\n",
    "    .option(\"quote\", \"\\\"\") \\\n",
    "    .option(\"escape\", \"\\\"\") \\\n",
    "    .option(\"multiLine\",\"true\") \\\n",
    "    .option(\"delimiter\", \",\").load(csvFile)\n",
    "csvDF.write.format(\"solr\").options(**jobs_update_opts).mode(\"overwrite\").save()\n",
    "print(\"Jobs Schema: \")\n",
    "csvDF.printSchema()\n",
    "print(\"Status: Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index StackExchange datasets: health, scifi, cooking, outdoors, travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_stack_exchange_dataset(collection,dataset):\n",
    "    print(\"Loading '\" + dataset + \"' Dataset...\")\n",
    "    csvFile = \"../data/\" + dataset + \"/posts.csv\"\n",
    "    update_opts={\"zkhost\": \"aips-zk\", \"collection\": collection, \"gen_uniq_key\": \"true\", \"commit_within\": \"5000\"}\n",
    "    csvDF = spark.read.format(\"com.databricks.spark.csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .option(\"charset\", \"utf-8\") \\\n",
    "        .option(\"quote\", \"\\\"\") \\\n",
    "        .option(\"escape\", \"\\\"\") \\\n",
    "        .option(\"multiLine\",\"true\") \\\n",
    "        .option(\"delimiter\", \",\").load(csvFile)\n",
    "        \n",
    "    csvWithCategoryDF = csvDF.withColumn(\"category\", lit(dataset))\n",
    "    \n",
    "    csvWithCategoryDF.write.format(\"solr\").options(**update_opts).mode(\"overwrite\").save()\n",
    "    print(\"Status: Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiping 'stackexchange' collection\n",
      "Creating stackexchange' collection\n",
      "Status: Success\n",
      "Adding 'title' field to collection\n",
      "Status: Success\n",
      "Adding 'body' field to collection\n",
      "Status: Success\n",
      "Loading 'health' Dataset...\n",
      "Status: Success\n",
      "Loading 'cooking' Dataset...\n",
      "Status: Success\n",
      "Loading 'scifi' Dataset...\n",
      "Status: Success\n",
      "Loading 'outdoors' Dataset...\n",
      "Status: Success\n",
      "Loading 'travel' Dataset...\n",
      "Status: Success\n",
      "Loading 'devops' Dataset...\n",
      "Status: Success\n",
      "Wiping 'health' collection\n",
      "Creating health' collection\n",
      "Status: Success\n",
      "Adding 'title' field to collection\n",
      "Status: Success\n",
      "Adding 'body' field to collection\n",
      "Status: Success\n",
      "Loading 'health' Dataset...\n",
      "Status: Success\n",
      "Wiping 'cooking' collection\n",
      "Creating cooking' collection\n",
      "Status: Success\n",
      "Adding 'title' field to collection\n",
      "Status: Success\n",
      "Adding 'body' field to collection\n",
      "Status: Success\n",
      "Loading 'cooking' Dataset...\n",
      "Status: Success\n",
      "Wiping 'scifi' collection\n",
      "Creating scifi' collection\n",
      "Status: Success\n",
      "Adding 'title' field to collection\n",
      "Status: Success\n",
      "Adding 'body' field to collection\n",
      "Status: Success\n",
      "Loading 'scifi' Dataset...\n",
      "Status: Success\n",
      "Wiping 'outdoors' collection\n",
      "Creating outdoors' collection\n",
      "Status: Success\n",
      "Adding 'title' field to collection\n",
      "Status: Success\n",
      "Adding 'body' field to collection\n",
      "Status: Success\n",
      "Loading 'outdoors' Dataset...\n",
      "Status: Success\n",
      "Wiping 'travel' collection\n",
      "Creating travel' collection\n",
      "Status: Success\n",
      "Adding 'title' field to collection\n",
      "Status: Success\n",
      "Adding 'body' field to collection\n",
      "Status: Success\n",
      "Loading 'travel' Dataset...\n",
      "Status: Success\n",
      "Wiping 'devops' collection\n",
      "Creating devops' collection\n",
      "Status: Success\n",
      "Adding 'title' field to collection\n",
      "Status: Success\n",
      "Adding 'body' field to collection\n",
      "Status: Success\n",
      "Loading 'devops' Dataset...\n",
      "Status: Success\n"
     ]
    }
   ],
   "source": [
    "collection=\"stackexchange\"\n",
    "create_collection(collection)\n",
    "\n",
    "#Modify Schema to make some fields explicitly searchable by keyword\n",
    "upsert_text_field(collection, \"title\")\n",
    "upsert_text_field(collection, \"body\")\n",
    "    \n",
    "index_stack_exchange_dataset(collection,\"health\")\n",
    "index_stack_exchange_dataset(collection,\"cooking\")\n",
    "index_stack_exchange_dataset(collection,\"scifi\")\n",
    "index_stack_exchange_dataset(collection,\"outdoors\")\n",
    "index_stack_exchange_dataset(collection,\"travel\")\n",
    "index_stack_exchange_dataset(collection,\"devops\")\n",
    "\n",
    "\n",
    "create_collection(\"health\")\n",
    "upsert_text_field(\"health\", \"title\")\n",
    "upsert_text_field(\"health\", \"body\")\n",
    "index_stack_exchange_dataset(\"health\",\"health\")\n",
    "\n",
    "create_collection(\"cooking\")\n",
    "upsert_text_field(\"cooking\", \"title\")\n",
    "upsert_text_field(\"cooking\", \"body\")\n",
    "index_stack_exchange_dataset(\"cooking\",\"cooking\")\n",
    "\n",
    "create_collection(\"scifi\")\n",
    "upsert_text_field(\"scifi\", \"title\")\n",
    "upsert_text_field(\"scifi\", \"body\")\n",
    "index_stack_exchange_dataset(\"scifi\",\"scifi\")\n",
    "\n",
    "create_collection(\"outdoors\")\n",
    "upsert_text_field(\"outdoors\", \"title\")\n",
    "upsert_text_field(\"outdoors\", \"body\")\n",
    "index_stack_exchange_dataset(\"outdoors\",\"outdoors\")\n",
    "\n",
    "create_collection(\"travel\")\n",
    "upsert_text_field(\"travel\", \"title\")\n",
    "upsert_text_field(\"travel\", \"body\")\n",
    "index_stack_exchange_dataset(\"travel\",\"travel\")\n",
    "\n",
    "create_collection(\"devops\")\n",
    "upsert_text_field(\"devops\", \"title\")\n",
    "upsert_text_field(\"devops\", \"body\")\n",
    "index_stack_exchange_dataset(\"devops\",\"devops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success!\n",
    "\n",
    "Now that you've indexed several large text datasets, in the next notebook we will explore the rich graph of semantic relationships embedded within those documents by leveraging Semantic Knowledge Graphs for real-time traversal and ranking of arbitrary relationships within the domains of our datasets.\n",
    "\n",
    "Up next: [Working with Semantic Knowledge Graphs](3.semantic-knowledge-graph.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
