{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the Knowledge Graph Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from aips import *\n",
    "import os\n",
    "from IPython.core.display import display,HTML\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, col, concat\n",
    "spark = SparkSession.builder.appName(\"ch5\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'jobs'...\n",
      "remote: Enumerating objects: 6, done.\u001b[K\n",
      "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
      "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
      "remote: Total 6 (delta 0), reused 3 (delta 0), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (6/6), done.\n",
      "Already up to date.\n",
      "._jobs.csv\n",
      "jobs.csv\n",
      "Cloning into 'health'...\n",
      "remote: Enumerating objects: 6, done.\u001b[K\n",
      "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
      "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
      "remote: Total 6 (delta 0), reused 3 (delta 0), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (6/6), done.\n",
      "Already up to date.\n",
      "._posts.csv\n",
      "posts.csv\n",
      "Cloning into 'scifi'...\n",
      "remote: Enumerating objects: 6, done.\u001b[K\n",
      "remote: Total 6 (delta 0), reused 0 (delta 0), pack-reused 6\u001b[K\n",
      "Unpacking objects: 100% (6/6), done.\n",
      "Already up to date.\n",
      "._posts.csv\n",
      "posts.csv\n",
      "Cloning into 'cooking'...\n",
      "remote: Enumerating objects: 6, done.\u001b[K\n",
      "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
      "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
      "remote: Total 6 (delta 0), reused 3 (delta 0), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (6/6), done.\n",
      "Already up to date.\n",
      "._posts.csv\n",
      "posts.csv\n",
      "Cloning into 'outdoors'...\n",
      "remote: Enumerating objects: 67, done.\u001b[K\n",
      "remote: Counting objects: 100% (1/1), done.\u001b[K\n",
      "remote: Total 67 (delta 0), reused 0 (delta 0), pack-reused 66\u001b[K\n",
      "Unpacking objects: 100% (67/67), done.\n",
      "Already up to date.\n",
      "README.md\n",
      "concepts.pickle\n",
      "._guesses.csv\n",
      "guesses.csv\n",
      "._guesses_all.json\n",
      "guesses_all.json\n",
      "outdoors_concepts.pickle\n",
      "outdoors_embeddings.pickle\n",
      "._outdoors_golden_answers.csv\n",
      "outdoors_golden_answers.csv\n",
      "._outdoors_golden_answers.xlsx\n",
      "outdoors_golden_answers.xlsx\n",
      "._outdoors_golden_answers_20210130.csv\n",
      "outdoors_golden_answers_20210130.csv\n",
      "outdoors_labels.pickle\n",
      "outdoors_question_answering_contexts.json\n",
      "outdoors_questionanswering_test_set.json\n",
      "outdoors_questionanswering_train_set.json\n",
      "._posts.csv\n",
      "posts.csv\n",
      "predicates.pickle\n",
      "pull_aips_dependency.py\n",
      "._question-answer-seed-contexts.csv\n",
      "question-answer-seed-contexts.csv\n",
      "question-answer-squad2-guesses.csv\n",
      "._roberta-base-squad2-outdoors\n",
      "roberta-base-squad2-outdoors/\n",
      "roberta-base-squad2-outdoors/._tokenizer_config.json\n",
      "roberta-base-squad2-outdoors/tokenizer_config.json\n",
      "roberta-base-squad2-outdoors/._special_tokens_map.json\n",
      "roberta-base-squad2-outdoors/special_tokens_map.json\n",
      "roberta-base-squad2-outdoors/._config.json\n",
      "roberta-base-squad2-outdoors/config.json\n",
      "roberta-base-squad2-outdoors/._merges.txt\n",
      "roberta-base-squad2-outdoors/merges.txt\n",
      "roberta-base-squad2-outdoors/._training_args.bin\n",
      "roberta-base-squad2-outdoors/training_args.bin\n",
      "roberta-base-squad2-outdoors/._pytorch_model.bin\n",
      "roberta-base-squad2-outdoors/pytorch_model.bin\n",
      "roberta-base-squad2-outdoors/._vocab.json\n",
      "roberta-base-squad2-outdoors/vocab.json\n",
      "Cloning into 'travel'...\n",
      "remote: Enumerating objects: 6, done.\u001b[K\n",
      "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
      "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
      "remote: Total 6 (delta 0), reused 3 (delta 0), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (6/6), done.\n",
      "Already up to date.\n",
      "._posts.csv\n",
      "posts.csv\n",
      "Already up to date.\n",
      "._posts.csv\n",
      "posts.csv\n"
     ]
    }
   ],
   "source": [
    "#jobs\n",
    "![ ! -d 'jobs' ] && git clone --depth=1 https://github.com/ai-powered-search/jobs.git\n",
    "! cd jobs && git pull\n",
    "! cd jobs && mkdir -p '../../data/jobs/' && tar -xvf jobs.tgz -C '../../data/jobs/'    \n",
    "\n",
    "#health\n",
    "![ ! -d 'health' ] && git clone --depth=1 https://github.com/ai-powered-search/health.git\n",
    "! cd health && git pull\n",
    "! cd health && mkdir -p '../../data/health/' && tar -xvf health.tgz -C '../../data/health/'\n",
    "\n",
    "#scifi\n",
    "![ ! -d 'scifi' ] && git clone --depth=1 https://github.com/ai-powered-search/scifi.git\n",
    "! cd scifi && git pull\n",
    "! cd scifi && mkdir -p '../../data/scifi/' && tar -xvf scifi.tgz -C '../../data/scifi/' \n",
    "\n",
    "#cooking\n",
    "![ ! -d 'cooking' ] && git clone --depth=1 https://github.com/ai-powered-search/cooking.git\n",
    "! cd cooking && git pull\n",
    "! cd cooking && mkdir -p '../../data/cooking/' && tar -xvf cooking.tgz -C '../../data/cooking/'\n",
    "\n",
    "#outdoors\n",
    "![ ! -d 'outdoors' ] && git clone --depth=1 https://github.com/ai-powered-search/outdoors.git\n",
    "! cd outdoors && git pull\n",
    "! cd outdoors && cat outdoors.tgz.part* > outdoors.tgz\n",
    "! cd outdoors && mkdir -p '../../data/outdoors/' && tar -xvf outdoors.tgz -C '../../data/outdoors/'\n",
    "\n",
    "#outdoors\n",
    "![ ! -d 'travel' ] && git clone --depth=1 https://github.com/ai-powered-search/travel.git\n",
    "! cd travel && git pull\n",
    "! cd travel && mkdir -p '../../data/travel/' && tar -xvf travel.tgz -C '../../data/travel/'\n",
    "\n",
    "#devops\n",
    "![ ! -d 'devops' ] && git clone --depth=1 https://github.com/ai-powered-search/devops.git\n",
    "! cd travel && git pull\n",
    "! cd travel && mkdir -p '../../data/devops/' && tar -xvf travel.tgz -C '../../data/devops/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index the Jobs Dataset into the Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiping 'jobs' collection\n",
      "[('action', 'CREATE'), ('name', 'jobs'), ('numShards', 1), ('replicationFactor', 1)]\n",
      "Creating 'jobs' collection\n",
      "Status: Success\n",
      "Adding 'company_country' field to collection\n",
      "Status: Success\n",
      "Adding 'job_description' field to collection\n",
      "Status: Success\n",
      "Adding 'company_description' field to collection\n",
      "Status: Success\n",
      "Loading Jobs...\n",
      "Jobs Schema: \n",
      "root\n",
      " |-- job_title: string (nullable = true)\n",
      " |-- job_description: string (nullable = true)\n",
      " |-- job_type: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- job_location: string (nullable = true)\n",
      " |-- job_city: string (nullable = true)\n",
      " |-- job_state: string (nullable = true)\n",
      " |-- job_country: string (nullable = true)\n",
      " |-- job_zip_code: string (nullable = true)\n",
      " |-- job_address: string (nullable = true)\n",
      " |-- min_salary: string (nullable = true)\n",
      " |-- max_salary: string (nullable = true)\n",
      " |-- salary_period: string (nullable = true)\n",
      " |-- apply_url: string (nullable = true)\n",
      " |-- apply_email: string (nullable = true)\n",
      " |-- num_employees: string (nullable = true)\n",
      " |-- industry: string (nullable = true)\n",
      " |-- company_name: string (nullable = true)\n",
      " |-- company_email: string (nullable = true)\n",
      " |-- company_website: string (nullable = true)\n",
      " |-- company_phone: string (nullable = true)\n",
      " |-- company_logo: string (nullable = true)\n",
      " |-- company_description: string (nullable = true)\n",
      " |-- company_location: string (nullable = true)\n",
      " |-- company_city: string (nullable = true)\n",
      " |-- company_state: string (nullable = true)\n",
      " |-- company_country: string (nullable = true)\n",
      " |-- company_zip_code: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- job_date: string (nullable = true)\n",
      "\n",
      "Status: Success\n"
     ]
    }
   ],
   "source": [
    "#Create Jobs Collection\n",
    "jobs_collection=\"jobs\"\n",
    "create_collection(jobs_collection)\n",
    "\n",
    "#Modify Schema to make some fields explicitly searchable by keyword\n",
    "upsert_text_field(jobs_collection, \"company_country\")\n",
    "upsert_text_field(jobs_collection, \"job_description\")\n",
    "upsert_text_field(jobs_collection, \"company_description\")\n",
    "#upsert_text_field(products_collection, \"longDescription\")\n",
    "#upsert_text_field(products_collection, \"manufacturer\")\n",
    "\n",
    "print(\"Loading Jobs...\")\n",
    "csvFile = \"../data/jobs/jobs.csv\"\n",
    "csvDF = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"charset\", \"utf-8\") \\\n",
    "    .option(\"quote\", \"\\\"\") \\\n",
    "    .option(\"escape\", \"\\\"\") \\\n",
    "    .option(\"multiLine\",\"true\") \\\n",
    "    .option(\"delimiter\", \",\").load(csvFile)\n",
    "\n",
    "jobs_update_opts={\"zkhost\": \"aips-zk\", \"collection\": jobs_collection, \n",
    "                  \"gen_uniq_key\": \"true\", \"commit_within\": \"5000\"}\n",
    "csvDF.write.format(\"solr\").options(**jobs_update_opts).mode(\"overwrite\").save()\n",
    "print(\"Jobs Schema: \")\n",
    "csvDF.printSchema()\n",
    "print(\"Status: Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index StackExchange datasets: health, scifi, cooking, outdoors, travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_stack_exchange_dataset(collection, dataset):\n",
    "    print(f\"Loading '{dataset}' dataset into collection '{collection}'...\")\n",
    "    csvFile = \"../data/\" + dataset + \"/posts.csv\"\n",
    "    csvDF = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .option(\"charset\", \"utf-8\") \\\n",
    "        .option(\"quote\", \"\\\"\") \\\n",
    "        .option(\"escape\", \"\\\"\") \\\n",
    "        .option(\"multiLine\",\"true\") \\\n",
    "        .option(\"delimiter\", \",\").load(csvFile)\n",
    "        \n",
    "    csvWithCategoryDF = csvDF.withColumn(\"category\", lit(dataset))\\\n",
    "        .drop(\"id\")\n",
    "    # We can rely on automatic generation of IDs, or we can create them ourselves. \n",
    "    # If we do it, comment out previous line\n",
    "    # .withColumn(\"id\", concat(col(\"category\"), lit(\"_\") col(\"id\")))\n",
    "    \n",
    "    update_opts={\"zkhost\": \"aips-zk\", \"collection\": collection, \"gen_uniq_key\": \"true\", \"commit_within\": \"5000\"}\n",
    "    csvWithCategoryDF.write.format(\"solr\").options(**update_opts).mode(\"overwrite\").save()\n",
    "    print(\"Status: Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiping 'stackexchange' collection\n",
      "[('action', 'CREATE'), ('name', 'stackexchange'), ('numShards', 1), ('replicationFactor', 1)]\n",
      "Creating 'stackexchange' collection\n",
      "Status: Success\n",
      "Adding 'title' field to collection\n",
      "Status: Success\n",
      "Adding 'body' field to collection\n",
      "Status: Success\n",
      "Loading 'health' dataset into collection 'stackexchange'...\n",
      "Status: Success\n",
      "Loading 'cooking' dataset into collection 'stackexchange'...\n",
      "Status: Success\n",
      "Loading 'scifi' dataset into collection 'stackexchange'...\n",
      "Status: Success\n",
      "Loading 'outdoors' dataset into collection 'stackexchange'...\n",
      "Status: Success\n",
      "Loading 'travel' dataset into collection 'stackexchange'...\n",
      "Status: Success\n",
      "Loading 'devops' dataset into collection 'stackexchange'...\n",
      "Status: Success\n",
      "Wiping 'health' collection\n",
      "[('action', 'CREATE'), ('name', 'health'), ('numShards', 1), ('replicationFactor', 1)]\n",
      "Creating 'health' collection\n",
      "Status: Success\n",
      "Adding 'title' field to collection\n",
      "Status: Success\n",
      "Adding 'body' field to collection\n",
      "Status: Success\n",
      "Loading 'health' dataset into collection 'health'...\n",
      "Status: Success\n",
      "Wiping 'cooking' collection\n",
      "[('action', 'CREATE'), ('name', 'cooking'), ('numShards', 1), ('replicationFactor', 1)]\n",
      "Creating 'cooking' collection\n",
      "Status: Success\n",
      "Adding 'title' field to collection\n",
      "Status: Success\n",
      "Adding 'body' field to collection\n",
      "Status: Success\n",
      "Loading 'cooking' dataset into collection 'cooking'...\n",
      "Status: Success\n",
      "Wiping 'scifi' collection\n",
      "[('action', 'CREATE'), ('name', 'scifi'), ('numShards', 1), ('replicationFactor', 1)]\n",
      "Creating 'scifi' collection\n",
      "Status: Success\n",
      "Adding 'title' field to collection\n",
      "Status: Success\n",
      "Adding 'body' field to collection\n",
      "Status: Success\n",
      "Loading 'scifi' dataset into collection 'scifi'...\n",
      "Status: Success\n",
      "Wiping 'outdoors' collection\n",
      "[('action', 'CREATE'), ('name', 'outdoors'), ('numShards', 1), ('replicationFactor', 1)]\n",
      "Creating 'outdoors' collection\n",
      "Status: Success\n",
      "Adding 'title' field to collection\n",
      "Status: Success\n",
      "Adding 'body' field to collection\n",
      "Status: Success\n",
      "Loading 'outdoors' dataset into collection 'outdoors'...\n",
      "Status: Success\n",
      "Wiping 'travel' collection\n",
      "[('action', 'CREATE'), ('name', 'travel'), ('numShards', 1), ('replicationFactor', 1)]\n",
      "Creating 'travel' collection\n",
      "Status: Success\n",
      "Adding 'title' field to collection\n",
      "Status: Success\n",
      "Adding 'body' field to collection\n",
      "Status: Success\n",
      "Loading 'travel' dataset into collection 'travel'...\n",
      "Status: Success\n",
      "Wiping 'devops' collection\n",
      "[('action', 'CREATE'), ('name', 'devops'), ('numShards', 1), ('replicationFactor', 1)]\n",
      "Creating 'devops' collection\n",
      "Status: Success\n",
      "Adding 'title' field to collection\n",
      "Status: Success\n",
      "Adding 'body' field to collection\n",
      "Status: Success\n",
      "Loading 'devops' dataset into collection 'devops'...\n",
      "Status: Success\n"
     ]
    }
   ],
   "source": [
    "collection=\"stackexchange\"\n",
    "create_collection(collection)\n",
    "\n",
    "#Modify Schema to make some fields explicitly searchable by keyword\n",
    "upsert_text_field(collection, \"title\")\n",
    "upsert_text_field(collection, \"body\")\n",
    "    \n",
    "index_stack_exchange_dataset(collection,\"health\")\n",
    "index_stack_exchange_dataset(collection,\"cooking\")\n",
    "index_stack_exchange_dataset(collection,\"scifi\")\n",
    "index_stack_exchange_dataset(collection,\"outdoors\")\n",
    "index_stack_exchange_dataset(collection,\"travel\")\n",
    "index_stack_exchange_dataset(collection,\"devops\")\n",
    "\n",
    "\n",
    "create_collection(\"health\")\n",
    "upsert_text_field(\"health\", \"title\")\n",
    "upsert_text_field(\"health\", \"body\")\n",
    "index_stack_exchange_dataset(\"health\",\"health\")\n",
    "\n",
    "create_collection(\"cooking\")\n",
    "upsert_text_field(\"cooking\", \"title\")\n",
    "upsert_text_field(\"cooking\", \"body\")\n",
    "index_stack_exchange_dataset(\"cooking\",\"cooking\")\n",
    "\n",
    "create_collection(\"scifi\")\n",
    "upsert_text_field(\"scifi\", \"title\")\n",
    "upsert_text_field(\"scifi\", \"body\")\n",
    "index_stack_exchange_dataset(\"scifi\",\"scifi\")\n",
    "\n",
    "create_collection(\"outdoors\")\n",
    "upsert_text_field(\"outdoors\", \"title\")\n",
    "upsert_text_field(\"outdoors\", \"body\")\n",
    "index_stack_exchange_dataset(\"outdoors\",\"outdoors\")\n",
    "\n",
    "create_collection(\"travel\")\n",
    "upsert_text_field(\"travel\", \"title\")\n",
    "upsert_text_field(\"travel\", \"body\")\n",
    "index_stack_exchange_dataset(\"travel\",\"travel\")\n",
    "\n",
    "create_collection(\"devops\")\n",
    "upsert_text_field(\"devops\", \"title\")\n",
    "upsert_text_field(\"devops\", \"body\")\n",
    "index_stack_exchange_dataset(\"devops\",\"devops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success!\n",
    "\n",
    "Now that you've indexed several large text datasets, in the next notebook we will explore the rich graph of semantic relationships embedded within those documents by leveraging Semantic Knowledge Graphs for real-time traversal and ranking of arbitrary relationships within the domains of our datasets.\n",
    "\n",
    "Up next: [Working with Semantic Knowledge Graphs](3.semantic-knowledge-graph.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
