{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the Knowledge Graph Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from aips import *\n",
    "import os\n",
    "from IPython.core.display import display,HTML\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, col, concat\n",
    "spark = SparkSession.builder.appName(\"ch5\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n",
      "._jobs.csv\n",
      "jobs.csv\n",
      "Already up to date.\n",
      "._posts.csv\n",
      "posts.csv\n",
      "Already up to date.\n",
      "._posts.csv\n",
      "posts.csv\n",
      "Already up to date.\n",
      "._posts.csv\n",
      "posts.csv\n",
      "Already up to date.\n",
      "._posts.csv\n",
      "posts.csv\n",
      "Already up to date.\n",
      "._posts.csv\n",
      "posts.csv\n",
      "Already up to date.\n",
      "._posts.csv\n",
      "posts.csv\n"
     ]
    }
   ],
   "source": [
    "#jobs\n",
    "![ ! -d 'jobs' ] && git clone https://github.com/ai-powered-search/jobs.git\n",
    "! cd jobs && git pull\n",
    "! cd jobs && mkdir -p '../../data/jobs/' && tar -xvf jobs.tgz -C '../../data/jobs/'    \n",
    "\n",
    "#health\n",
    "![ ! -d 'health' ] && git clone https://github.com/ai-powered-search/health.git\n",
    "! cd health && git pull\n",
    "! cd health && mkdir -p '../../data/health/' && tar -xvf health.tgz -C '../../data/health/'\n",
    "\n",
    "#scifi\n",
    "![ ! -d 'scifi' ] && git clone https://github.com/ai-powered-search/scifi.git\n",
    "! cd scifi && git pull\n",
    "! cd scifi && mkdir -p '../../data/scifi/' && tar -xvf scifi.tgz -C '../../data/scifi/' \n",
    "\n",
    "#cooking\n",
    "![ ! -d 'cooking' ] && git clone https://github.com/ai-powered-search/cooking.git\n",
    "! cd cooking && git pull\n",
    "! cd cooking && mkdir -p '../../data/cooking/' && tar -xvf cooking.tgz -C '../../data/cooking/'\n",
    "\n",
    "#outdoors\n",
    "![ ! -d 'outdoors' ] && git clone https://github.com/ai-powered-search/outdoors.git\n",
    "! cd outdoors && git pull\n",
    "! cd outdoors && mkdir -p '../../data/outdoors/' && tar -xvf outdoors.tgz -C '../../data/outdoors/'\n",
    "\n",
    "#outdoors\n",
    "![ ! -d 'travel' ] && git clone https://github.com/ai-powered-search/travel.git\n",
    "! cd travel && git pull\n",
    "! cd travel && mkdir -p '../../data/travel/' && tar -xvf travel.tgz -C '../../data/travel/'\n",
    "\n",
    "#devops\n",
    "![ ! -d 'devops' ] && git clone https://github.com/ai-powered-search/devops.git\n",
    "! cd travel && git pull\n",
    "! cd travel && mkdir -p '../../data/devops/' && tar -xvf travel.tgz -C '../../data/devops/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index the Jobs Dataset into the Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiping 'jobs' collection\n",
      "[('action', 'CREATE'), ('name', 'jobs'), ('numShards', 1), ('replicationFactor', 1)]\n",
      "Creating jobs' collection\n",
      "Status: Success\n",
      "Adding 'company_country' field to collection\n",
      "Status: Success\n",
      "Adding 'job_description' field to collection\n",
      "Status: Success\n",
      "Adding 'company_description' field to collection\n",
      "Status: Success\n",
      "Loading Jobs...\n",
      "Jobs Schema: \n",
      "root\n",
      " |-- job_title: string (nullable = true)\n",
      " |-- job_description: string (nullable = true)\n",
      " |-- job_type: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- job_location: string (nullable = true)\n",
      " |-- job_city: string (nullable = true)\n",
      " |-- job_state: string (nullable = true)\n",
      " |-- job_country: string (nullable = true)\n",
      " |-- job_zip_code: string (nullable = true)\n",
      " |-- job_address: string (nullable = true)\n",
      " |-- min_salary: string (nullable = true)\n",
      " |-- max_salary: string (nullable = true)\n",
      " |-- salary_period: string (nullable = true)\n",
      " |-- apply_url: string (nullable = true)\n",
      " |-- apply_email: string (nullable = true)\n",
      " |-- num_employees: string (nullable = true)\n",
      " |-- industry: string (nullable = true)\n",
      " |-- company_name: string (nullable = true)\n",
      " |-- company_email: string (nullable = true)\n",
      " |-- company_website: string (nullable = true)\n",
      " |-- company_phone: string (nullable = true)\n",
      " |-- company_logo: string (nullable = true)\n",
      " |-- company_description: string (nullable = true)\n",
      " |-- company_location: string (nullable = true)\n",
      " |-- company_city: string (nullable = true)\n",
      " |-- company_state: string (nullable = true)\n",
      " |-- company_country: string (nullable = true)\n",
      " |-- company_zip_code: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- job_date: string (nullable = true)\n",
      "\n",
      "Status: Success\n"
     ]
    }
   ],
   "source": [
    "#Create Jobs Collection\n",
    "jobs_collection=\"jobs\"\n",
    "create_collection(jobs_collection)\n",
    "\n",
    "#Modify Schema to make some fields explicitly searchable by keyword\n",
    "upsert_text_field(jobs_collection, \"company_country\")\n",
    "upsert_text_field(jobs_collection, \"job_description\")\n",
    "upsert_text_field(jobs_collection, \"company_description\")\n",
    "#upsert_text_field(products_collection, \"longDescription\")\n",
    "#upsert_text_field(products_collection, \"manufacturer\")\n",
    "\n",
    "print(\"Loading Jobs...\")\n",
    "csvFile = \"../data/jobs/jobs.csv\"\n",
    "csvDF = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"charset\", \"utf-8\") \\\n",
    "    .option(\"quote\", \"\\\"\") \\\n",
    "    .option(\"escape\", \"\\\"\") \\\n",
    "    .option(\"multiLine\",\"true\") \\\n",
    "    .option(\"delimiter\", \",\").load(csvFile)\n",
    "print(\"Jobs Schema: \")\n",
    "csvDF.printSchema()\n",
    "\n",
    "jobs_update_opts={\"zkhost\": \"aips-zk\", \"collection\": jobs_collection, \n",
    "                  \"gen_uniq_key\": \"true\", \"commit_within\": \"5000\"}\n",
    "csvDF.write.format(\"solr\").options(**jobs_update_opts).mode(\"overwrite\").save()\n",
    "print(\"Status: Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index StackExchange datasets: health, scifi, cooking, outdoors, travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_stack_exchange_dataset(collection, dataset):\n",
    "    print(f\"Loading '{dataset}' dataset into '{collection}'...\")\n",
    "    csvFile = \"../data/\" + dataset + \"/posts.csv\"\n",
    "    csvDF = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .option(\"charset\", \"utf-8\") \\\n",
    "        .option(\"quote\", \"\\\"\") \\\n",
    "        .option(\"escape\", \"\\\"\") \\\n",
    "        .option(\"multiLine\",\"true\") \\\n",
    "        .option(\"delimiter\", \",\").load(csvFile)\n",
    "        \n",
    "    csvWithCategoryDF = csvDF.withColumn(\"category\", lit(dataset))\\\n",
    "        .drop(\"id\") # gen_uniq_key generates it only if the `id` column doesn't exist\n",
    "# We can rely on automatic generation of IDs, or we can create them ourselves. If we do it, comment out previous line\n",
    "#       .withColumn(\"id\", concat(col(\"category\"), lit(\"_\") col(\"id\")))\n",
    "    \n",
    "    update_opts={\"zkhost\": \"aips-zk\", \"collection\": collection, \"gen_uniq_key\": \"true\", \"commit_within\": \"5000\"}\n",
    "    csvWithCategoryDF.write.format(\"solr\").options(**update_opts).mode(\"overwrite\").save()\n",
    "    print(\"Status: Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiping 'stackexchange' collection\n",
      "[('action', 'CREATE'), ('name', 'stackexchange'), ('numShards', 1), ('replicationFactor', 1)]\n",
      "Creating stackexchange' collection\n",
      "Status: Success\n",
      "Adding 'title' field to collection\n",
      "Status: Success\n",
      "Adding 'body' field to collection\n",
      "Status: Success\n",
      "Loading 'health' dataset into 'stackexchange'...\n",
      "Status: Success\n",
      "Loading 'cooking' dataset into 'stackexchange'...\n",
      "Status: Success\n",
      "Loading 'scifi' dataset into 'stackexchange'...\n",
      "Status: Success\n",
      "Loading 'outdoors' dataset into 'stackexchange'...\n",
      "Status: Success\n",
      "Loading 'travel' dataset into 'stackexchange'...\n",
      "Status: Success\n",
      "Loading 'devops' dataset into 'stackexchange'...\n",
      "Status: Success\n",
      "Wiping 'health' collection\n",
      "[('action', 'CREATE'), ('name', 'health'), ('numShards', 1), ('replicationFactor', 1)]\n",
      "Creating health' collection\n",
      "Status: Success\n",
      "Adding 'title' field to collection\n",
      "Status: Success\n",
      "Adding 'body' field to collection\n",
      "Status: Success\n",
      "Loading 'health' dataset into 'health'...\n",
      "Status: Success\n",
      "Wiping 'cooking' collection\n",
      "[('action', 'CREATE'), ('name', 'cooking'), ('numShards', 1), ('replicationFactor', 1)]\n",
      "Creating cooking' collection\n",
      "Status: Success\n",
      "Adding 'title' field to collection\n",
      "Status: Success\n",
      "Adding 'body' field to collection\n",
      "Status: Success\n",
      "Loading 'cooking' dataset into 'cooking'...\n",
      "Status: Success\n",
      "Wiping 'scifi' collection\n",
      "[('action', 'CREATE'), ('name', 'scifi'), ('numShards', 1), ('replicationFactor', 1)]\n",
      "Creating scifi' collection\n",
      "Status: Success\n",
      "Adding 'title' field to collection\n",
      "Status: Success\n",
      "Adding 'body' field to collection\n",
      "Status: Success\n",
      "Loading 'scifi' dataset into 'scifi'...\n",
      "Status: Success\n",
      "Wiping 'outdoors' collection\n",
      "[('action', 'CREATE'), ('name', 'outdoors'), ('numShards', 1), ('replicationFactor', 1)]\n",
      "Creating outdoors' collection\n",
      "Status: Success\n",
      "Adding 'title' field to collection\n",
      "Status: Success\n",
      "Adding 'body' field to collection\n",
      "Status: Success\n",
      "Loading 'outdoors' dataset into 'outdoors'...\n",
      "Status: Success\n",
      "Wiping 'travel' collection\n",
      "[('action', 'CREATE'), ('name', 'travel'), ('numShards', 1), ('replicationFactor', 1)]\n",
      "Creating travel' collection\n",
      "Status: Success\n",
      "Adding 'title' field to collection\n",
      "Status: Success\n",
      "Adding 'body' field to collection\n",
      "Status: Success\n",
      "Loading 'travel' dataset into 'travel'...\n",
      "Status: Success\n",
      "Wiping 'devops' collection\n",
      "[('action', 'CREATE'), ('name', 'devops'), ('numShards', 1), ('replicationFactor', 1)]\n",
      "Creating devops' collection\n",
      "Status: Success\n",
      "Adding 'title' field to collection\n",
      "Status: Success\n",
      "Adding 'body' field to collection\n",
      "Status: Success\n",
      "Loading 'devops' dataset into 'devops'...\n",
      "Status: Success\n"
     ]
    }
   ],
   "source": [
    "collection=\"stackexchange\"\n",
    "create_collection(collection)\n",
    "\n",
    "#Modify Schema to make some fields explicitly searchable by keyword\n",
    "upsert_text_field(collection, \"title\")\n",
    "upsert_text_field(collection, \"body\")\n",
    "\n",
    "index_stack_exchange_dataset(collection, \"health\")\n",
    "index_stack_exchange_dataset(collection, \"cooking\")\n",
    "index_stack_exchange_dataset(collection, \"scifi\")\n",
    "index_stack_exchange_dataset(collection, \"outdoors\")\n",
    "index_stack_exchange_dataset(collection, \"travel\")\n",
    "index_stack_exchange_dataset(collection, \"devops\")\n",
    "\n",
    "for dataset in [\"health\", \"cooking\", \"scifi\", \"outdoors\", \"travel\", \"devops\"]: \n",
    "    create_collection(dataset)\n",
    "    upsert_text_field(dataset, \"title\")\n",
    "    upsert_text_field(dataset, \"body\")\n",
    "    index_stack_exchange_dataset(dataset, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success!\n",
    "\n",
    "Now that you've indexed several large text datasets, in the next notebook we will explore the rich graph of semantic relationships embedded within those documents by leveraging Semantic Knowledge Graphs for real-time traversal and ranking of arbitrary relationships within the domains of our datasets.\n",
    "\n",
    "Up next: [Working with Semantic Knowledge Graphs](3.semantic-knowledge-graph.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
