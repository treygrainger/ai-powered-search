{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the Knowledge Graph Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from aips import *\n",
    "import os\n",
    "from IPython.core.display import display,HTML\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, col, concat\n",
    "spark = SparkSession.builder.appName(\"ch5\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jobs\n",
    "![ ! -d 'jobs' ] && git clone --depth=1 https://github.com/ai-powered-search/jobs.git\n",
    "! cd jobs && git pull\n",
    "! cd jobs && mkdir -p '../../data/jobs/' && tar -xvf jobs.tgz -C '../../data/jobs/'    \n",
    "\n",
    "#health\n",
    "![ ! -d 'health' ] && git clone --depth=1 https://github.com/ai-powered-search/health.git\n",
    "! cd health && git pull\n",
    "! cd health && mkdir -p '../../data/health/' && tar -xvf health.tgz -C '../../data/health/'\n",
    "\n",
    "#scifi\n",
    "![ ! -d 'scifi' ] && git clone --depth=1 https://github.com/ai-powered-search/scifi.git\n",
    "! cd scifi && git pull\n",
    "! cd scifi && mkdir -p '../../data/scifi/' && tar -xvf scifi.tgz -C '../../data/scifi/' \n",
    "\n",
    "#cooking\n",
    "![ ! -d 'cooking' ] && git clone --depth=1 https://github.com/ai-powered-search/cooking.git\n",
    "! cd cooking && git pull\n",
    "! cd cooking && mkdir -p '../../data/cooking/' && tar -xvf cooking.tgz -C '../../data/cooking/'\n",
    "\n",
    "#outdoors\n",
    "![ ! -d 'outdoors' ] && git clone --depth=1 https://github.com/ai-powered-search/outdoors.git\n",
    "! cd outdoors && git pull\n",
    "! cd outdoors && cat outdoors.tgz.part* > outdoors.tgz\n",
    "! cd outdoors && mkdir -p '../../data/outdoors/' && tar -xvf outdoors.tgz -C '../../data/outdoors/'\n",
    "\n",
    "#outdoors\n",
    "![ ! -d 'travel' ] && git clone --depth=1 https://github.com/ai-powered-search/travel.git\n",
    "! cd travel && git pull\n",
    "! cd travel && mkdir -p '../../data/travel/' && tar -xvf travel.tgz -C '../../data/travel/'\n",
    "\n",
    "#devops\n",
    "![ ! -d 'devops' ] && git clone --depth=1 https://github.com/ai-powered-search/devops.git\n",
    "! cd travel && git pull\n",
    "! cd travel && mkdir -p '../../data/devops/' && tar -xvf travel.tgz -C '../../data/devops/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index the Jobs Dataset into the Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Jobs Collection\n",
    "jobs_collection=\"jobs\"\n",
    "create_collection(jobs_collection)\n",
    "\n",
    "#Modify Schema to make some fields explicitly searchable by keyword\n",
    "upsert_text_field(jobs_collection, \"company_country\")\n",
    "upsert_text_field(jobs_collection, \"job_description\")\n",
    "upsert_text_field(jobs_collection, \"company_description\")\n",
    "#upsert_text_field(products_collection, \"longDescription\")\n",
    "#upsert_text_field(products_collection, \"manufacturer\")\n",
    "\n",
    "print(\"Loading Jobs...\")\n",
    "csvFile = \"../data/jobs/jobs.csv\"\n",
    "csvDF = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"charset\", \"utf-8\") \\\n",
    "    .option(\"quote\", \"\\\"\") \\\n",
    "    .option(\"escape\", \"\\\"\") \\\n",
    "    .option(\"multiLine\",\"true\") \\\n",
    "    .option(\"delimiter\", \",\").load(csvFile)\n",
    "\n",
    "jobs_update_opts={\"zkhost\": \"aips-zk\", \"collection\": jobs_collection, \n",
    "                  \"gen_uniq_key\": \"true\", \"commit_within\": \"5000\"}\n",
    "csvDF.write.format(\"solr\").options(**jobs_update_opts).mode(\"overwrite\").save()\n",
    "print(\"Jobs Schema: \")\n",
    "csvDF.printSchema()\n",
    "print(\"Status: Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index StackExchange datasets: health, scifi, cooking, outdoors, travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_stack_exchange_dataset(collection, dataset):\n",
    "    print(f\"Loading '{dataset}' dataset into collection '{collection}'...\")\n",
    "    csvFile = \"../data/\" + dataset + \"/posts.csv\"\n",
    "    csvDF = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .option(\"charset\", \"utf-8\") \\\n",
    "        .option(\"quote\", \"\\\"\") \\\n",
    "        .option(\"escape\", \"\\\"\") \\\n",
    "        .option(\"multiLine\",\"true\") \\\n",
    "        .option(\"delimiter\", \",\").load(csvFile)\n",
    "        \n",
    "    csvWithCategoryDF = csvDF.withColumn(\"category\", lit(dataset))\\\n",
    "        .drop(\"id\")\n",
    "    # We can rely on automatic generation of IDs, or we can create them ourselves. \n",
    "    # If we do it, comment out previous line\n",
    "    # .withColumn(\"id\", concat(col(\"category\"), lit(\"_\") col(\"id\")))\n",
    "    \n",
    "    update_opts={\"zkhost\": \"aips-zk\", \"collection\": collection, \"gen_uniq_key\": \"true\", \"commit_within\": \"5000\"}\n",
    "    csvWithCategoryDF.write.format(\"solr\").options(**update_opts).mode(\"overwrite\").save()\n",
    "    print(\"Status: Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection=\"stackexchange\"\n",
    "create_collection(collection)\n",
    "\n",
    "#Modify Schema to make some fields explicitly searchable by keyword\n",
    "upsert_text_field(collection, \"title\")\n",
    "upsert_text_field(collection, \"body\")\n",
    "    \n",
    "index_stack_exchange_dataset(collection,\"health\")\n",
    "index_stack_exchange_dataset(collection,\"cooking\")\n",
    "index_stack_exchange_dataset(collection,\"scifi\")\n",
    "index_stack_exchange_dataset(collection,\"outdoors\")\n",
    "index_stack_exchange_dataset(collection,\"travel\")\n",
    "index_stack_exchange_dataset(collection,\"devops\")\n",
    "\n",
    "\n",
    "create_collection(\"health\")\n",
    "upsert_text_field(\"health\", \"title\")\n",
    "upsert_text_field(\"health\", \"body\")\n",
    "index_stack_exchange_dataset(\"health\",\"health\")\n",
    "\n",
    "create_collection(\"cooking\")\n",
    "upsert_text_field(\"cooking\", \"title\")\n",
    "upsert_text_field(\"cooking\", \"body\")\n",
    "index_stack_exchange_dataset(\"cooking\",\"cooking\")\n",
    "\n",
    "create_collection(\"scifi\")\n",
    "upsert_text_field(\"scifi\", \"title\")\n",
    "upsert_text_field(\"scifi\", \"body\")\n",
    "index_stack_exchange_dataset(\"scifi\",\"scifi\")\n",
    "\n",
    "create_collection(\"outdoors\")\n",
    "upsert_text_field(\"outdoors\", \"title\")\n",
    "upsert_text_field(\"outdoors\", \"body\")\n",
    "index_stack_exchange_dataset(\"outdoors\",\"outdoors\")\n",
    "\n",
    "create_collection(\"travel\")\n",
    "upsert_text_field(\"travel\", \"title\")\n",
    "upsert_text_field(\"travel\", \"body\")\n",
    "index_stack_exchange_dataset(\"travel\",\"travel\")\n",
    "\n",
    "create_collection(\"devops\")\n",
    "upsert_text_field(\"devops\", \"title\")\n",
    "upsert_text_field(\"devops\", \"body\")\n",
    "index_stack_exchange_dataset(\"devops\",\"devops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success!\n",
    "\n",
    "Now that you've indexed several large text datasets, in the next notebook we will explore the rich graph of semantic relationships embedded within those documents by leveraging Semantic Knowledge Graphs for real-time traversal and ranking of arbitrary relationships within the domains of our datasets.\n",
    "\n",
    "Up next: [Working with Semantic Knowledge Graphs](3.semantic-knowledge-graph.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
