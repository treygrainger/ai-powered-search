{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Chapter 3] Vectors and Text Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('..')\n",
    "from aips import num2str, vec2str, tokenize\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing 3.1\n",
    "In this example, we explore Ranking two documents for the query \"Apple Juice\". We present the query as a feature vector, as well as the documents.\n",
    "\n",
    "#### Text Content\n",
    "*Query*: \"Apple Juice\"\n",
    "\n",
    "*Document 1*: \n",
    "```Lynn: ham and cheese sandwich, chocolate cookie, ice water.\n",
    "Brian: turkey avocado sandwich, plain potato chips, apple juice\n",
    "Mohammed: grilled chicken salad, fruit cup, lemonade```\n",
    "\n",
    "*Document 2*: ```Orchard Farms apple juice is premium, organic apple juice made from the freshest apples and never from concentrate. Its juice has received the regional award for best apple juice three years in a row.```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense Vectors\n",
    "If we consider a vector with each keyword as a feature (48 terms total):\n",
    "```[a, and, apple, apples, avocado, award, best, brian, cheese, chicken, chips, chocolate, concentrate, cookie, cup, farms, for, freshest, from, fruit, grilled, ham, has, ice, in, is, its, juice, lemonade, lynn, made, mohammed, never, orchard, organic, plain, potato, premium, received, regional, row, salad, sandwich, the, three, turkey, water, years]```\n",
    "\n",
    "\n",
    "Then our query becomes the 48-feature vector, where the `apple` and `juice` features both exist:\n",
    "Query:      ```[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding vectors for our documents are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1_vector = np.array([0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0])\n",
    "doc2_vector = np.array([1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity\n",
    "To rank our documents, we then just need to calculate the cosine between each document and the query, \n",
    "which will become the relevance score for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(vector1,vector2):\n",
    "  return dot(vector1, vector2)/(norm(vector1)*norm(vector2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance Scores:\n",
      " doc1: 0.2828\n",
      " doc2: 0.2828\n"
     ]
    }
   ],
   "source": [
    "doc1_score = cos_sim(query_vector, doc1_vector)\n",
    "doc2_score = cos_sim(query_vector, doc2_vector)\n",
    "\n",
    "print(\"Relevance Scores:\\n doc1: \" + num2str(doc1_score) + \"\\n doc2: \" + num2str(doc2_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting... Both documents received exactly the same relevance score, even though the documents contain lengthy vectors with very different content. It might not be immediately obvious, but let's simplify the calculation by focusing only on the features that matter.\n",
    "\n",
    "#### Sparse Vectors\n",
    "The key to understanding the calculation is understanding that the only features that matter are the ones shared between the query and a document. All other features (words appearing in documents that don't match the query) have zero impact on whether one document is ranked higher than another. As such, we can simplify our calculations significantly by creating sparse vectors that only include the terms present in the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance Scores:\n",
      " doc1: 1.0\n",
      " doc2: 1.0\n"
     ]
    }
   ],
   "source": [
    "sparse_query_vector = [1, 1] #[apple, juice]\n",
    "sparse_doc1_vector = [1, 1]\n",
    "sparse_doc2_vector = [1, 1]\n",
    "\n",
    "doc1_score = cos_sim(sparse_query_vector, sparse_doc1_vector)\n",
    "doc2_score = cos_sim(sparse_query_vector, sparse_doc2_vector)\n",
    "\n",
    "print(\"Relevance Scores:\\n doc1: \" + num2str(doc1_score) + \"\\n doc2: \" + num2str(doc2_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, you'll notice several very interesting things:\n",
    "1. This simplified sparse vector calculation still shows both `doc1` and `doc2` returning equivalent relevance scores, since they both match all the words in the query.\n",
    "2. Even though the absolute score between the dense vector similarity (0.2828) and the sparse vector similarity (1.0) are different due to normalization, the scores are still the same relative to each other (equal to each other in this case).\n",
    "3. The feature weights for the two query terms (`apple`, `juice`) are exactly the same between the query and each of the documents, resulting in a cosine score of 1.0.\n",
    "\n",
    "The problem here, of course, is that the features in the vector only signifies IF the word `apple` or `juice` exists, not how well each document actually represents either of the terms. We'll correct for by introducing the concept of \"term frequency\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term Frequency\n",
    "In Section 3.1.4 we learn about Term Frequency (TF). If we count up the number of times each term appears in our documents, we will get a better understanding of \"how well\" the document represents those terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_term_occurrences: [1, 1]\n",
      "doc1_term_occurrences: [1, 1]\n",
      "doc2_term_occurrences: [3, 4]\n"
     ]
    }
   ],
   "source": [
    "query = \"apple juice\"\n",
    "doc1 = \"Lynn: ham and cheese sandwhich, chocolate cookie, ice water.\\nBrian: turkey avocado sandwhich, plain potato chips, apple juice\\nMohammed: grilled chicken salad, fruit cup, lemonade\"\n",
    "doc2 = \"Orchard Farms apple juice is premium, organic apple juice  made from the freshest apples and never from concentrate. Its juice has received the regional award for best apple juice three years in a row.\"\n",
    "\n",
    "query_term_occurrences = [tokenize(query).count(\"apple\"), tokenize(query).count(\"juice\")] #[apple:1, juice:1]\n",
    "doc1_term_occurrences = [tokenize(doc1).count(\"apple\"), tokenize(doc1).count(\"juice\")] #[apple:1, juice:1]\n",
    "doc2_term_occurrences = [tokenize(doc2).count(\"apple\"), tokenize(doc2).count(\"juice\")] #[apple:3, juice:4]\n",
    "\n",
    "print(\"query_term_occurrences: \" + vec2str(query_term_occurrences))\n",
    "print(\"doc1_term_occurrences: \" + vec2str(doc1_term_occurrences))\n",
    "print(\"doc2_term_occurrences: \" + vec2str(doc2_term_occurrences))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the feature values for the terms `apple` and `juice` are now weighted in each vector based upon the number of occurrences in each document. Unfortunately we can't just do a cosine similarity on the raw count of term occurrences, however, because the query only contains one occurrence of each term, whereas we would consider documents with multiple occurrences of each term to likely be more similar. Let's test it out and see the problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance Scores:\n",
      " doc1: 1.0\n",
      " doc2: 0.9899\n"
     ]
    }
   ],
   "source": [
    "doc1_tf_vector = [1, 1] #[apple:1, juice:1]\n",
    "doc2_tf_vector = [3, 4] #[apple:3, juice:4]\n",
    "\n",
    "query_vector = [1, 1] #[apple:1, juice:1]\n",
    "\n",
    "doc1_score = num2str(cos_sim(query_vector, doc1_tf_vector))\n",
    "doc2_score = num2str(cos_sim(query_vector, doc2_tf_vector))\n",
    "\n",
    "print(\"Relevance Scores:\\n doc1: \" + str(doc1_score) + \"\\n doc2: \" + str(doc2_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, document 1 is considered a better cosine similarity match because the terms `apple` and `juice` both occur the same number of times (just once) in the query and document one. Since our goal is for documents with higher term frequency to score higher, we can overcome this by either:\n",
    "1. Modify the query features to represent the \"best\" possible score for each query term, or\n",
    "2. Making our score be the sum of the feature weights in each sparse vector.\n",
    "\n",
    "Let's try option 1 for now (we'll visit option 2 in later examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance Scores:\n",
      " doc1: 0.9899\n",
      " doc2: 1.0\n"
     ]
    }
   ],
   "source": [
    "doc1_tf_vector = [1, 1] #[apple:1, juice:1]\n",
    "doc2_tf_vector = [3, 4] #[apple:3, juice:4]\n",
    "\n",
    "#query should represent the \"best possible\" match, so we include the \"top possible score\" for each term in the query vector.\n",
    "query_vector = np.maximum.reduce([doc1_tf_vector, doc2_tf_vector]) #[apple:3, juice:4]\n",
    "\n",
    "doc1_score = cos_sim(query_vector, doc1_tf_vector)\n",
    "doc2_score = cos_sim(query_vector, doc2_tf_vector)\n",
    "\n",
    "print(\"Relevance Scores:\\n doc1: \" + num2str(doc1_score) + \"\\n doc2: \" + num2str(doc2_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result rankings now look more inline with our expectations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great - the result rankings now look more inline with our expectations (for this simple example, at least)!\n",
    "\n",
    "As our feature-weighting calculation are getting more sophisticated, let's move on beyond our initial `\"apple juice\"` example toward a query and documents with more interesting statistics in terms of intersections and overlaps between terms in the query and terms in the documents.\n",
    "\n",
    "The following example demonstrates some more useful characteristics that will better help us understand how term frequency helps with our text-based sparse vector similarity scoring.\n",
    "\n",
    "*Document 1:* ```In light of the big reveal in the interview, the interesting thing is that the person in wrong probably made the right decision in the end.```\n",
    "\n",
    "*Document 2:* ```My favorite book is the cat in the hat, which is about a crazy cat in a hat who breaks into a house and creates a crazy afternoon for two kids.```\n",
    "\n",
    "*Document 3:* ```My careless neighbors apparently let a stray cat stay in their garage unsupervised, which resulted in my favorite hat that I let them borrow being ruined.```\n",
    "\n",
    "Let's map these into their corresponding (sparse) vector representations and calculate a similarity score:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"In light of the big reveal in the interview, the interesting thing is that the person in wrong probably made the right decision in the end.\"\n",
    "doc2 = \"My favorite book is the cat in the hat, which is about a crazy cat in a hat who breaks into a house and creates a crazy afternoon for two kids.\"\n",
    "doc3 = \"My careless neighbors apparently let a stray cat stay in their garage unsupervised, which resulted in my favorite hat that I let them borrow being ruined.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(content, term):\n",
    "    tokenized_content = tokenize(content)\n",
    "    term_count = tokenized_content.count(term.lower())\n",
    "    return float(term_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: [the, cat, in, the, hat]\n",
      "doc1_vector: [6.0, 0.0, 4.0, 6.0, 0.0]\n",
      "doc2_vector: [2.0, 2.0, 2.0, 2.0, 2.0]\n",
      "doc3_vector: [0.0, 1.0, 2.0, 0.0, 1.0]\n",
      "\n",
      "query_vector: [6.0, 2.0, 4.0, 6.0, 2.0]\n",
      "\n",
      "Relevance Scores:\n",
      " doc1: 0.9574\n",
      " doc2: 0.9129\n",
      " doc3: 0.5\n"
     ]
    }
   ],
   "source": [
    "doc1_tf_vector = [ tf(doc1,\"the\"), tf(doc1,\"cat\"), tf(doc1,\"in\"), tf(doc1,\"the\"), tf(doc1,\"hat\") ]\n",
    "doc2_tf_vector = [ tf(doc2,\"the\"), tf(doc2,\"cat\"), tf(doc2,\"in\"), tf(doc2,\"the\"), tf(doc2,\"hat\") ]\n",
    "doc3_tf_vector = [ tf(doc3,\"the\"), tf(doc3,\"cat\"), tf(doc3,\"in\"), tf(doc3,\"the\"), tf(doc3,\"hat\") ]\n",
    "\n",
    "print (\"labels: [the, cat, in, the, hat]\")\n",
    "print (\"doc1_vector: [\" + \", \".join(map(num2str,doc1_tf_vector)) + \"]\")\n",
    "print (\"doc2_vector: [\" + \", \".join(map(num2str,doc2_tf_vector)) + \"]\")\n",
    "print (\"doc3_vector: [\" + \", \".join(map(num2str,doc3_tf_vector)) + \"]\\n\")\n",
    "                   \n",
    "query = \"the cat in the hat\"\n",
    "query_vector = np.maximum.reduce([doc1_tf_vector, doc2_tf_vector, doc3_tf_vector])\n",
    "print (\"query_vector: [\" + \", \".join(map(num2str,query_vector)) + \"]\\n\")\n",
    "\n",
    "doc1_score = cos_sim(query_vector, doc1_tf_vector)\n",
    "doc2_score = cos_sim(query_vector, doc2_tf_vector)\n",
    "doc3_score = cos_sim(query_vector, doc3_tf_vector)\n",
    "\n",
    "print(\"Relevance Scores:\\n doc1: \" + num2str(doc1_score) + \"\\n doc2: \" + num2str(doc2_score)+ \"\\n doc3: \" + num2str(doc3_score))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, those results don't necessarily match our intuition about which documents are the best matches. Intuitively, we would instead expect the following ordering:\n",
    "1. doc2 (is about the book _The Cat in the Hat_ )\n",
    "2. doc3 (matches all of the words `the`, `cat`, `in`, and `hat`\n",
    "3. doc1 (only matches the words `the` and `in`, even though it contains them many times).\n",
    "\n",
    "The problem here, of course, is that since every occurrence of any word is considered just as important, the more times ANY term appears, the more relevant that document becomes. In this case, *doc1* is getting the highest score, because it contains 12 total term matches (`the` ten times, `in` two times), which more total term matches than any other document.\n",
    "\n",
    "To overcome these issues, \"term frequency\" calculations will typically both normalize for document length (take the total term count divided by document length) and also dampen the effect of additional term occurrences (take the square root of term occurrences).\n",
    "\n",
    "This gives us the following term frequency calculations:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing 3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(content, term):\n",
    "    tokenized_content = tokenize(content)\n",
    "    term_count = tokenized_content.count(term.lower())\n",
    "    vector_length = len(tokenized_content)\n",
    "    return float(np.sqrt(term_count)) / float(vector_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our updated TF calculation in place, let's calculate our relevance ranking again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: [the, cat, in, the, hat]\n",
      "doc1_vector: [0.0942, 0.0, 0.0769, 0.0942, 0.0]\n",
      "doc2_vector: [0.0456, 0.0456, 0.0456, 0.0456, 0.0456]\n",
      "doc3_vector: [0.0, 0.0385, 0.0544, 0.0, 0.0385]\n",
      "\n",
      "query_vector: [0.0942, 0.0456, 0.0769, 0.0942, 0.0456]\n",
      "\n",
      "Relevance Scores:\n",
      " doc1: 0.9222\n",
      " doc2: 0.9559\n",
      " doc3: 0.5995\n"
     ]
    }
   ],
   "source": [
    "doc1_tf_vector = [ tf(doc1,\"the\"), tf(doc1,\"cat\"), tf(doc1,\"in\"), tf(doc1,\"the\"), tf(doc1,\"hat\") ]\n",
    "doc2_tf_vector = [ tf(doc2,\"the\"), tf(doc2,\"cat\"), tf(doc2,\"in\"), tf(doc2,\"the\"), tf(doc2,\"hat\") ]\n",
    "doc3_tf_vector = [ tf(doc3,\"the\"), tf(doc3,\"cat\"), tf(doc3,\"in\"), tf(doc3,\"the\"), tf(doc3,\"hat\") ]\n",
    "\n",
    "print (\"labels: [the, cat, in, the, hat]\")\n",
    "print (\"doc1_vector: [\" + \", \".join(map(num2str,doc1_tf_vector)) + \"]\")\n",
    "print (\"doc2_vector: [\" + \", \".join(map(num2str,doc2_tf_vector)) + \"]\")\n",
    "print (\"doc3_vector: [\" + \", \".join(map(num2str,doc3_tf_vector)) + \"]\\n\")\n",
    "                   \n",
    "query = \"the cat in the hat\"\n",
    "query_vector = np.maximum.reduce([doc1_tf_vector, doc2_tf_vector, doc3_tf_vector])\n",
    "print (\"query_vector: [\" + \", \".join(map(num2str,query_vector)) + \"]\\n\")\n",
    "\n",
    "doc1_score = cos_sim(query_vector, doc1_tf_vector)\n",
    "doc2_score = cos_sim(query_vector, doc2_tf_vector)\n",
    "doc3_score = cos_sim(query_vector, doc3_tf_vector)\n",
    "\n",
    "print(\"Relevance Scores:\\n doc1: \" + num2str(doc1_score) + \"\\n doc2: \" + num2str(doc2_score)+ \"\\n doc3: \" + num2str(doc3_score))              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normalized TF clearly helped, as `doc2` is now ranked the highest, as we would expect. This is mostly because of the dampening effect on number of term occurrences so that each additional term (in `doc1`, which matched `the` and `in` so many times) so that each additional occurrrence contributed less to the feature weight than prior occurrences. Unfortunately, `doc1` is still ranked second highest, so even that wasn't enough to get the better matching `doc3` to the top.\n",
    "\n",
    "Your intuition is probably screaming right \"Yeah, but nobody really cares about the words `the` and `in`. It's obvious that the words `cat` and `hat` should be given the most weight here!\"\n",
    "\n",
    "And you would be right. Let's modify our scoring calculation to fix this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency (IDF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Document Frequency (DF)* for a term is defined as the total number of document in the search engine that contain the term, and it serve as a good measure for how important a term is. The intuition here is that more specific or rare words (like `cat` and `hat`) tend to be more important than more common words (like `the` and `in`).\n",
    "\n",
    "$$DF(t\\ in\\ d)=\\sum_{d\\ in\\ c} d.contains(t)\\ ?\\ 1\\ :\\ 0$$\n",
    "\n",
    "Since we would like words which are more important to get a higher score, we take an inverse of the document frequency (IDF), typically defined through the following function:\n",
    "\n",
    "$$IDF(t\\ in\\ d)=1 + log (\\ totalDocs\\ /\\ (\\ DF(t)\\ +\\ 1\\ )\\ )$$\n",
    "\n",
    "In our query for `the cat in the hat`, a vector of IDFs would thus look as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing 3.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: [the, cat, in, the, hat]\n",
      "idf_vector: [1.0512, 5.5952, 1.1052, 1.0512, 6.2785]\n"
     ]
    }
   ],
   "source": [
    "#Pretending like we have a representative sample of docs with meaningful real-world statistics\n",
    "df_map = {\"the\": 9500, \"cat\": 100, \"in\":9000, \"hat\":50}\n",
    "totalDocs = 10000\n",
    "\n",
    "def idf(term):\n",
    "    return 1 + np.log(totalDocs / (df_map[term] + 1) )\n",
    "\n",
    "#same for both queries and documents; IDF is term-dependent, not document dependent\n",
    "idf_vector = np.array([idf(\"the\"), idf(\"cat\"), idf(\"in\"), idf(\"the\"), idf(\"hat\")])\n",
    "\n",
    "print (\"labels: [the, cat, in, the, hat]\\nidf_vector: \" + vec2str(idf_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "We now have the two principle components of text-based relevance ranking:\n",
    "- TF (measures how well a term describes a document)\n",
    "- IDF (measures how important each term is)\n",
    "\n",
    "Most search engines, and many other data science applications, leverage a combination of each of these factors as the basis for textual similarity scoring, using the following function:\n",
    "\n",
    "$$TF\\_IDF = TF * IDF^2$$\n",
    "\n",
    "With this formula in place, we can finally calculate a relevance score (that weights both number of occurrences and usefulness of terms) for how well each of our documents match our query:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing 3.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(tf,idf):\n",
    "    return tf * idf**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: [the, cat, in, the, hat]\n",
      "doc2_tfidf: [0.1041, 0.0, 0.094, 0.1041, 0.0]\n",
      "doc2_tfidf: [0.0504, 1.4282, 0.0557, 0.0504, 1.7983]\n",
      "doc3_tfidf: [0.0, 1.2041, 0.0664, 0.0, 1.5161]\n",
      "\n",
      "Relevance Scores:\n",
      " doc1: 0.0758\n",
      " doc2: 0.9993\n",
      " doc3: 0.9979\n"
     ]
    }
   ],
   "source": [
    "query = \"the cat in the hat\"\n",
    "\n",
    "print (\"labels: [the, cat, in, the, hat]\")\n",
    "doc1_tfidf = [\n",
    "               tf_idf(tf(doc1, \"the\"), idf(\"the\")), \n",
    "               tf_idf(tf(doc1, \"cat\"), idf(\"cat\")),\n",
    "               tf_idf(tf(doc1, \"in\"), idf(\"in\")),\n",
    "               tf_idf(tf(doc1, \"the\"), idf(\"the\")),\n",
    "               tf_idf(tf(doc1, \"hat\"), idf(\"hat\"))\n",
    "             ]\n",
    "print(\"doc2_tfidf: \" + vec2str(doc1_tfidf))\n",
    "\n",
    "doc2_tfidf = [\n",
    "               tf_idf(tf(doc2, \"the\"), idf(\"the\")), \n",
    "               tf_idf(tf(doc2, \"cat\"), idf(\"cat\")),\n",
    "               tf_idf(tf(doc2, \"in\"), idf(\"in\")),\n",
    "               tf_idf(tf(doc2, \"the\"), idf(\"the\")),\n",
    "               tf_idf(tf(doc2, \"hat\"), idf(\"hat\"))\n",
    "             ]\n",
    "print(\"doc2_tfidf: \" + vec2str(doc2_tfidf))\n",
    "\n",
    "doc3_tfidf = [\n",
    "               tf_idf(tf(doc3, \"the\"), idf(\"the\")), \n",
    "               tf_idf(tf(doc3, \"cat\"), idf(\"cat\")),\n",
    "               tf_idf(tf(doc3, \"in\"), idf(\"in\")),\n",
    "               tf_idf(tf(doc3, \"the\"), idf(\"the\")),\n",
    "               tf_idf(tf(doc3, \"hat\"), idf(\"hat\"))\n",
    "             ]\n",
    "print(\"doc3_tfidf: \" + vec2str(doc3_tfidf))\n",
    "\n",
    "query_tfidf = np.maximum.reduce([doc1_tfidf, doc2_tfidf, doc3_tfidf])\n",
    "\n",
    "doc1_relevance = cos_sim(query_tfidf,doc1_tfidf)\n",
    "doc2_relevance = cos_sim(query_tfidf,doc2_tfidf)\n",
    "doc3_relevance = cos_sim(query_tfidf,doc3_tfidf)\n",
    "\n",
    "print(\"\\nRelevance Scores:\\n doc1: \" + num2str(doc1_relevance) + \"\\n doc2: \" + num2str(doc2_relevance) + \"\\n doc3: \" + num2str(doc3_relevance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally!\n",
    "Finally our search results make intuitive sense! `doc2` gets the highest score, since it matches the most important words the most, followed by `doc3`, which contains all the words, but not as many times, followed by `doc1`, which only contains an abundance of insignificant words.\n",
    "\n",
    "This TF-IDF calculation is at the heart of many search engine relevance calculations, including the default algorithms - called BM25 - used by both Apache Solr and Elasticsearch. In addition, it is possible to match on much more than just text keywords - modern search engines enable dynamically specifying boosts of fields, terms, and functions, which enables full control over the relevance scoring calculation.\n",
    "\n",
    "We'll introduce each of these in the next workbook: [Controlling Relevance](2.ch3-controlling-relevance.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
