{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the Reviews Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from aips import *\n",
    "import os\n",
    "from IPython.display import display,HTML\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, col\n",
    "spark = SparkSession.builder.appName(\"ch7\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n",
      "._reviews.csv\n",
      "reviews.csv\n",
      "._entities.csv\n",
      "entities.csv\n",
      "._cities.csv\n",
      "cities.csv\n"
     ]
    }
   ],
   "source": [
    "#Get datasets\n",
    "![ ! -d 'reviews' ] && git clone --depth 1 https://github.com/ai-powered-search/reviews.git\n",
    "! cd reviews && git pull\n",
    "! cd reviews && mkdir -p '../../data/reviews/' && tar -xvf reviews.tgz -C '../../data/reviews/' && tar -xvf entities.tgz -C '../../data/reviews/' && tar -xvf cities.tgz -C '../../data/reviews/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviews Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collection Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def create_reviews_collection():\n",
    "    #Create Reviews Collection\n",
    "    reviews_collection=\"reviews\"\n",
    "\n",
    "    #delete collection\n",
    "    requests.get(f\"{SOLR_URL}/admin/collections?action=DELETE&name={reviews_collection}\")\n",
    "    #delete configSet to start from scratch\n",
    "    requests.get(f\"{SOLR_URL}/admin/configs?action=DELETE&name={reviews_collection}.AUTOCREATED\")\n",
    "\n",
    "    create_collection(reviews_collection)\n",
    "    #add_text_tagger_fields(reviews_collection)\n",
    "\n",
    "    headers={\"Content-type\": \"application/json\"}\n",
    "\n",
    "    schemaCommands = [\n",
    "            \"\"\"{\n",
    "              \"add-field-type\":{\n",
    "                \"name\":\"commaDelimited\",\n",
    "                \"class\":\"solr.TextField\",\n",
    "                \"positionIncrementGap\":100,\n",
    "                \"omitTermFreqAndPositions\":true,\n",
    "                \"indexAnalyzer\":{\n",
    "                  \"tokenizer\":{\n",
    "                     \"class\":\"solr.PatternTokenizerFactory\",\n",
    "                     \"pattern\": \",\\\\\\s*\"\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "            }\"\"\",\n",
    "             \"\"\"{\n",
    "              \"add-field-type\":{\n",
    "                \"name\":\"pipeDelimited\",\n",
    "                \"class\":\"solr.TextField\",\n",
    "                \"positionIncrementGap\":100,\n",
    "                \"omitTermFreqAndPositions\":true,\n",
    "                \"indexAnalyzer\":{\n",
    "                  \"tokenizer\":{\n",
    "                     \"class\":\"solr.PatternTokenizerFactory\",\n",
    "                     \"pattern\": \"\\\\|\\\\\\s*\"\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "            }\"\"\",\n",
    "            \"\"\"{\n",
    "              \"add-field\":{\"name\":\"doc_type\", \"type\":\"commaDelimited\", \"stored\":true, \"multiValued\": true}\n",
    "            }\"\"\",\n",
    "            \"\"\"{\n",
    "              \"add-copy-field\":{\"source\":\"categories_t\", \"dest\":[\"doc_type\"]}\n",
    "            }\"\"\",\n",
    "            \"\"\"{\n",
    "              \"add-field\":{\"name\":\"location_p\", \"type\":\"location\", \"stored\":true}\n",
    "            }\"\"\",\n",
    "            \"\"\"{\n",
    "              \"add-copy-field\":{\"source\":\"location_pt_s\", \"dest\":[\"location_p\"]}\n",
    "            }\"\"\"\n",
    "    ]\n",
    "\n",
    "    for schemaCommand in schemaCommands:\n",
    "        response = requests.post(f\"{SOLR_URL}/{reviews_collection}/schema\", \n",
    "                                 headers=headers, data=schemaCommand)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing 7.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enable_text_tagger(collection):\n",
    "    headers={\"Content-type\": \"application/json\"}\n",
    "    \n",
    "    schemaCommands = [\n",
    "        \"\"\"{\n",
    "          \"add-field-type\":{\n",
    "            \"name\":\"tag\",\n",
    "            \"class\":\"solr.TextField\",\n",
    "            \"postingsFormat\":\"FST50\",\n",
    "            \"omitNorms\":true,\n",
    "            \"omitTermFreqAndPositions\":true,\n",
    "            \"indexAnalyzer\":{\n",
    "              \"tokenizer\":{\n",
    "                 \"class\":\"solr.StandardTokenizerFactory\" },\n",
    "              \"filters\":[\n",
    "                {\"class\":\"solr.EnglishPossessiveFilterFactory\"},\n",
    "                {\"class\":\"solr.ASCIIFoldingFilterFactory\"},\n",
    "                {\"class\":\"solr.LowerCaseFilterFactory\"},\n",
    "                {\"class\":\"solr.ConcatenateGraphFilterFactory\", \"preservePositionIncrements\":false }\n",
    "              ]},\n",
    "            \"queryAnalyzer\":{\n",
    "              \"tokenizer\":{\n",
    "                 \"class\":\"solr.StandardTokenizerFactory\" },\n",
    "              \"filters\":[\n",
    "                {\"class\":\"solr.EnglishPossessiveFilterFactory\"},\n",
    "                {\"class\":\"solr.ASCIIFoldingFilterFactory\"},\n",
    "                {\"class\":\"solr.LowerCaseFilterFactory\"}\n",
    "              ]}\n",
    "            }\n",
    "        }\"\"\",\n",
    "        \"\"\"{\n",
    "          \"add-field\":{\"name\":\"surface_form\", \"type\":\"string\", \"stored\":true}\n",
    "        }\"\"\",\n",
    "            \"\"\"{\n",
    "          \"add-field\":{\"name\":\"canonical_form\", \"type\":\"string\", \"stored\":true}\n",
    "        }\"\"\",\n",
    "            \"\"\"{\n",
    "          \"add-field\":{\"name\":\"name\", \"type\":\"text_general\"}\n",
    "        }\"\"\",\n",
    "            \"\"\"{\n",
    "          \"add-field\":{\"name\":\"popularity\", \"type\":\"pint\", \"stored\":true}\n",
    "        }\"\"\",\n",
    "            \"\"\"{\n",
    "          \"add-field\":{\"name\":\"name_tag\", \"type\":\"tag\", \"stored\":false}\n",
    "        }\"\"\",\n",
    "            \"\"\"{\n",
    "          \"add-copy-field\":{\"source\":\"name\", \"dest\":[\"surface_form\", \"name_tag\", \"canonical_form\"]}\n",
    "        }\"\"\",\n",
    "            \"\"\"{\n",
    "          \"add-copy-field\":{\"source\":\"population_i\", \"dest\":[\"popularity\"]}\n",
    "        }\"\"\",\n",
    "            \"\"\"{\n",
    "          \"add-copy-field\":{\"source\":\"surface_form\", \"dest\":[\"name_tag\"]}\n",
    "        }\"\"\"\n",
    "    ]\n",
    "    \n",
    "    for schemaCommand in schemaCommands:\n",
    "        response = requests.post(f\"{SOLR_URL}/{collection}/schema\", headers=headers, data=schemaCommand)\n",
    "        #print(response)    \n",
    "    \n",
    "    \n",
    "    response = requests.post(f\"{SOLR_URL}/{collection}/config\", headers=headers, data=\"\"\"{\n",
    "      \"add-requesthandler\" : {\n",
    "        \"name\": \"/tag\",\n",
    "        \"class\":\"solr.TaggerRequestHandler\",\n",
    "        \"defaults\":{\"field\":\"name_tag\"}\n",
    "      }\n",
    "    }\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_reviews_collection():\n",
    "    print(\"\\nLoading Reviews...\")\n",
    "    csvFile = \"../data/reviews/reviews.csv\"\n",
    "    reviews_collection = \"reviews\"\n",
    "    reviews_update_opts={\"zkhost\": \"aips-zk\", \"collection\": reviews_collection, \n",
    "                        \"gen_uniq_key\": \"true\", \"commit_within\": \"5000\"}\n",
    "    csvDF = spark.read.csv(csvFile, inferSchema=True, header=True, multiLine=True, escape=\"\\\"\") \\\n",
    "        .withColumn(\"poplarity_i\", col(\"stars_i\") * 20) \\\n",
    "        .select(\n",
    "          \"id\", \"name_t\", \"city_t\", \"state_t\", \"text_t\", \"stars_i\", \n",
    "          \"categories_t\",  \"location_pt_s\", \"type_ss\", \"latitude_d\", \"longitude_d\")\n",
    "    csvDF.write.format(\"solr\").options(**reviews_update_opts).mode(\"overwrite\").save()\n",
    "    print(\"Reviews Schema: \")\n",
    "    csvDF.printSchema()\n",
    "    print(\"Status: Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: is it correct one?\n",
    "def index_reviews_collection():\n",
    "    print(\"\\nLoading Reviews...\")\n",
    "    csvFile = \"../data/reviews/reviews2.csv\"\n",
    "    reviews_collection = \"reviews\"\n",
    "    reviews_update_opts={\"zkhost\": \"aips-zk\", \"collection\": reviews_collection, \n",
    "                        \"gen_uniq_key\": \"true\", \"commit_within\": \"5000\"}\n",
    "    csvDF = spark.read.csv(csvFile, inferSchema=True, header=True, multiLine=True, escape=\"\\\"\") \\\n",
    "        .withColumn(\"popularity_i\", col(\"aggregatedRating\") * 20) \\\n",
    "        .withColumn(\"name_t\", col(\"name\")) \\\n",
    "        .withColumn(\"city_t\", col(\"address_city\")) \\\n",
    "        .withColumn(\"state_t\", col(\"address_regionCode\")) \\\n",
    "        .withColumn(\"text_t\", col(\"review_text\")) \\\n",
    "        .withColumn(\"stars_i\", col(\"aggregatedRating\")) \\\n",
    "        .withColumn(\"categories_t\", col(\"categories\")) \\\n",
    "        .withColumn(\"type_ss\", col(\"type\")) \\\n",
    "        .withColumn(\"latitude_d\", col(\"address_lat\")) \\\n",
    "        .withColumn(\"longitude_d\", col(\"address_long\")) \\\n",
    "        .select(\"*\")\n",
    "    csvDF.write.format(\"solr\").options(**reviews_update_opts).mode(\"overwrite\").save()\n",
    "    print(\"Reviews Schema: \")\n",
    "    csvDF.printSchema()\n",
    "    print(\"Status: Success\")\n",
    "    #        .withColumn(\"location_pt_s\", concat(col(\"address_lat\"), lit(\",\"), col(\"address_long\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index the Reviews Dataset into the Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing 7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiping 'reviews' collection\n",
      "[('action', 'CREATE'), ('name', 'reviews'), ('numShards', 1), ('replicationFactor', 1)]\n",
      "Creating 'reviews' collection\n",
      "Status: Success\n",
      "\n",
      "Loading Reviews...\n",
      "Reviews Schema: \n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name_t: string (nullable = true)\n",
      " |-- city_t: string (nullable = true)\n",
      " |-- state_t: string (nullable = true)\n",
      " |-- text_t: string (nullable = true)\n",
      " |-- stars_i: integer (nullable = true)\n",
      " |-- categories_t: string (nullable = true)\n",
      " |-- location_pt_s: string (nullable = true)\n",
      " |-- type_ss: string (nullable = true)\n",
      " |-- latitude_d: double (nullable = true)\n",
      " |-- longitude_d: double (nullable = true)\n",
      "\n",
      "Status: Success\n"
     ]
    }
   ],
   "source": [
    "create_reviews_collection()\n",
    "index_reviews_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enities Dataset (Manually-specified Knowledge Graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collection Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_entities_collection():\n",
    "    #Create Entities Collection\n",
    "    entities_collection=\"entities\"\n",
    "    #delete collection\n",
    "    requests.get(f\"{SOLR_URL}/admin/collections?action=DELETE&name={entities_collection}\")\n",
    "    #delete configSet to start from scratch\n",
    "    requests.get(f\"{SOLR_URL}/admin/configs?action=DELETE&name={entities_collection}.AUTOCREATED\")\n",
    "\n",
    "    create_collection(entities_collection)\n",
    "    enable_text_tagger(entities_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_entities():\n",
    "    entities_collection=\"entities\"\n",
    "    print(\"Loading Entities...\")\n",
    "    csvFile = \"../data/reviews/entities.csv\"\n",
    "    entities_update_opts={\"zkhost\": \"aips-zk\", \"collection\": entities_collection, \n",
    "                          \"gen_uniq_key\": \"true\", \"commit_within\": \"5000\"}\n",
    "    csvDF = spark.read.csv(csvFile, inferSchema=True, header=True, multiLine=True, escape=\"\\\"\")\n",
    "    csvDF.write.format(\"solr\").options(**entities_update_opts).mode(\"overwrite\").save()\n",
    "    print(\"Entities Schema: \")\n",
    "    csvDF.printSchema()\n",
    "    print(\"Status: Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cities Dataset (Geonames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modify Schema to make some fields explicitly searchable by keyword\n",
    "#upsert_text_field(jobs_collection, \"company_country\")\n",
    "#upsert_text_field(jobs_collection, \"job_description\")\n",
    "#upsert_text_field(jobs_collection, \"company_description\")\n",
    "#upsert_text_field(products_collection, \"longDescription\")\n",
    "#upsert_text_field(products_collection, \"manufacturer\")\n",
    "\n",
    "def index_cities():\n",
    "    entities_collection=\"entities\"\n",
    "    print(\"Loading Geonames...\")\n",
    "    csvFile = \"../data/reviews/cities.csv\"\n",
    "    entities_update_opts={\"zkhost\": \"aips-zk\", \"collection\": entities_collection, \n",
    "                          \"gen_uniq_key\": \"true\", \"commit_within\": \"5000\"}\n",
    "\n",
    "    from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "    from pyspark.sql.functions import concat_ws\n",
    "\n",
    "    schema = StructType() \\\n",
    "          .add(\"id\",StringType(),True) \\\n",
    "          .add(\"name\",StringType(),True) \\\n",
    "          .add(\"ascii_name_s\",StringType(),True) \\\n",
    "          .add(\"alternative_names_s\",StringType(),True) \\\n",
    "          .add(\"latitude_s\",StringType(),True) \\\n",
    "          .add(\"longitude_s\",StringType(),True) \\\n",
    "          .add(\"feature_class_s\",StringType(),True) \\\n",
    "          .add(\"feature_code_s\",StringType(),True) \\\n",
    "          .add(\"StringType\",StringType(),True) \\\n",
    "          .add(\"cc2_s\",StringType(),True) \\\n",
    "          .add(\"admin_code_1_s\",StringType(),True) \\\n",
    "          .add(\"admin_code_2_s\",StringType(),True) \\\n",
    "          .add(\"admin_code_3_s\",StringType(),True) \\\n",
    "          .add(\"admin_code_4_s\",StringType(),True) \\\n",
    "          .add(\"population_i\",IntegerType(),True) \\\n",
    "          .add(\"elevation_s\",StringType(),True) \\\n",
    "          .add(\"dem_s\",StringType(),True) \\\n",
    "          .add(\"timezone_s\",StringType(),True) \\\n",
    "          .add(\"modification_date_s\",StringType(),True)\n",
    "\n",
    "    csvDF = spark.read.csv(csvFile, schema=schema,multiLine=True, escape=\"\\\\\", sep=\"\\t\") \\\n",
    "        .withColumn(\"type\", lit(\"city\")) \\\n",
    "        .withColumn(\"location_p\", concat_ws(\",\", \"latitude_s\", \"longitude_s\"))\n",
    "        #.show()\n",
    "\n",
    "    csvDF.write.format(\"solr\").options(**entities_update_opts).mode(\"overwrite\").save()\n",
    "    #print(\"Entities Schema: \")\n",
    "    #csvDF.printSchema()\n",
    "    print(\"Status: Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing 17.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiping 'entities' collection\n",
      "[('action', 'CREATE'), ('name', 'entities'), ('numShards', 1), ('replicationFactor', 1)]\n",
      "Creating 'entities' collection\n",
      "Status: Success\n",
      "Loading Entities...\n",
      "Entities Schema: \n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- surface_form: string (nullable = true)\n",
      " |-- canonical_form: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- popularity: integer (nullable = true)\n",
      " |-- command_function: string (nullable = true)\n",
      "\n",
      "Status: Success\n",
      "Loading Geonames...\n",
      "Status: Success\n"
     ]
    }
   ],
   "source": [
    "create_entities_collection()\n",
    "index_entities()\n",
    "index_cities()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success!\n",
    "\n",
    "Now that you've indexed the Reviews Dataset and semantic data, it's time to test our end to end semantic search example!\n",
    "\n",
    "Up next: [Semantic search](2.semantic-search.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
