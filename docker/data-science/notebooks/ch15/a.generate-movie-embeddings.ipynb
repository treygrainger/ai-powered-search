{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "import json\n",
    "import pickle\n",
    "import sys\n",
    "import warnings\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import pandas\n",
    "import spacy\n",
    "from aips import *\n",
    "from aips.spark import create_view_from_collection\n",
    "from aips.spark.dataframe import from_csv\n",
    "from IPython.display import HTML, display\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import UserDefinedFunction, col\n",
    "import pyspark.sql.types as pys\n",
    "from pyspark.sql.types import ArrayType, StructField, StructType, StringType, FloatType\n",
    "import numpy\n",
    "from transformers import CLIPProcessor, CLIPTextModel, CLIPModel\n",
    "import PIL\n",
    "import os\n",
    "from itertools import groupby\n",
    "import aips.data_loaders.movies as movies\n",
    "import imageio as iio\n",
    "import requests\n",
    "import shutil\n",
    "\n",
    "warnings.filterwarnings(\"ignore\") #Some operations warn inside a loop, we'll only need to see the first warning\n",
    "\n",
    "engine = get_engine()\n",
    "outdoors_collection = engine.get_collection(\"outdoors\")\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.driver.memory\", \"8g\")\n",
    "conf.set(\"spark.executor.memory\", \"8g\")\n",
    "conf.set(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "conf.set(\"spark.dynamicAllocation.executorMemoryOverhead\", \"8g\")\n",
    "spark = SparkSession.builder.appName(\"AIPS\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(file_name, remote_url=\"\", log=False):\n",
    "    full_path = f\"../data/tmdb/large_movie_images/{file_name}.jpg\"    \n",
    "    try:\n",
    "        exists = os.path.exists(full_path)\n",
    "        #print(f\"Exists {exists} file {file_name} remote {remote_url}\")\n",
    "        if not exists and remote_url:\n",
    "            response = requests.get(remote_url, stream=True)\n",
    "            with open(full_path, 'wb') as out_file:\n",
    "                shutil.copyfileobj(response.raw, out_file)\n",
    "            del response\n",
    "            if log: print(f\"Wrote {full_path}\")\n",
    "        image = iio.imread(full_path)\n",
    "        if log: print(\"File Found\")\n",
    "        return image\n",
    "    except:\n",
    "        if log: print(f\"No Image Available {full_path}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def float_conversion(flist):\n",
    "    return list(map(lambda f: f.item(), flist))\n",
    "\n",
    "def normalize_embedding(embedding):\n",
    "    return numpy.divide(embedding,\n",
    "                        numpy.linalg.norm(embedding,axis=0))\n",
    "\n",
    "def compute_image_embedding(image):\n",
    "    try:\n",
    "        inputs = processor(images=[image], return_tensors=\"pt\", padding=True)\n",
    "        embedding = model.get_image_features(**inputs).tolist()[0]\n",
    "        return normalize_embedding(embedding)\n",
    "    except:\n",
    "        print(\"Exception in image processing\")\n",
    "        return []\n",
    "\n",
    "def calculate_embeddings(image_id, remote_url=\"\"):\n",
    "    image = load_image(image_id, remote_url, log=False)\n",
    "    if len(image):\n",
    "        return compute_image_embedding(image)\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def load_movie_images(dataframe):    \n",
    "    poster_paths = dataframe.rdd.map(lambda x: x.path).collect()\n",
    "    for p in poster_paths:\n",
    "        load_image(p.split(\"/\")[-1], p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump(data, cache_name=\"tmdb_movies\", ignore_cache=False):\n",
    "    cache_file_name = f\"../data/tmdb/{cache_name}.pickle\"\n",
    "    os.makedirs(os.path.dirname(cache_file_name), exist_ok=True)\n",
    "    with open(cache_file_name, \"wb\") as fd:\n",
    "        pickle.dump(data, fd)\n",
    "\n",
    "def dump_dataframe(movies_dataframe, cache_name=\"tmdb_movies\", ignore_cache=False):\n",
    "    movies = movies_dataframe.rdd.map(lambda row: row.asDict()).collect()\n",
    "    dump(movies, cache_name=cache_name)\n",
    "\n",
    "def read(cache_name=\"tmdb_movies\"):\n",
    "    cache_file_name = f\"../data/tmdb/{cache_name}.pickle\"\n",
    "    with open(cache_file_name, \"rb\") as fd:\n",
    "        return pickle.load(fd)\n",
    "\n",
    "def generate_tmdb_data_with_image_ids():\n",
    "    title_movie_map_file = \"../data/tmdb/movie_data.csv\"\n",
    "    dataframe = from_csv(title_movie_map_file)\n",
    "    movie_image_ids = {}\n",
    "    for k, g in groupby([row.asDict() for row in dataframe.collect()],\n",
    "                        lambda m: m[\"tooltip\"].lower()):\n",
    "        ids = [m[\"path\"].split(\"/\")[-1][:-4] for m in g]\n",
    "        movie_image_ids[k] = ids\n",
    "    \n",
    "    print(movie_image_ids)\n",
    "    movie_dataframe = movies.load_dataframe(\"../data/tmdb.json\", movie_image_ids)\n",
    "    dump_dataframe(movie_dataframe)\n",
    "    \n",
    "def generate_image_embeddings_data():\n",
    "    movie_data = read(\"tmdb_movies\")\n",
    "    image_embeddings = {}\n",
    "    for movie in movie_data:\n",
    "        if movie[\"movie_image_ids\"]:\n",
    "            for image_id in movie[\"movie_image_ids\"].split(\",\"):\n",
    "                embedding = float_conversion(calculate_embeddings(image_id))\n",
    "                image_embeddings[image_id] = {\"movie_id\": movie[\"id\"],\n",
    "                                              \"title\": movie[\"title\"],\n",
    "                                              \"image_id\": image_id,\n",
    "                                              \"image_embeddings\": embedding}\n",
    "    dump(image_embeddings, \"movie_image_embeddings\")\n",
    "\n",
    "def generate_tmdb_with_embeddings_index():\n",
    "    embeddings_data = read(\"movie_image_embeddings\")\n",
    "    collection = engine.create_collection(\"tmdb_with_embeddings\")\n",
    "    movies = [v for k,v in embeddings_data.items()]\n",
    "    collection.add_documents(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/notebooks/ch15\n",
      "Wiping \"tmdb_with_embeddings\" collection\n",
      "Creating \"tmdb_with_embeddings\" collection\n",
      "Status: Success\n",
      "\n",
      "Adding Documents to 'tmdb_with_embeddings' collection\n"
     ]
    }
   ],
   "source": [
    "#generate_tmdb_data_with_image_ids()\n",
    "#generate_image_embeddings_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
