{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Chapter 15: Foundation Models and Emerging Search Paradigms]\n",
    "## Generative AI and the Search Frontier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "import json\n",
    "import pickle\n",
    "import sys\n",
    "import warnings\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import pandas\n",
    "import spacy\n",
    "from aips import *\n",
    "from aips.spark import create_view_from_collection\n",
    "from IPython.display import HTML, display\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "import numpy\n",
    "\n",
    "warnings.filterwarnings(\"ignore\") #Some operations warn inside a loop, we'll only need to see the first warning\n",
    "\n",
    "engine = get_engine()\n",
    "outdoors_collection = engine.get_collection(\"outdoors\")\n",
    "spark = SparkSession.builder.appName(\"AIPS\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: This notebook depends upon the Outdoors dataset. If you have any issues, please rerun the [Setting up the Outdoors Dataset](../ch13/1.setting-up-the-outdoors-dataset.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mocked Generative Output\n",
    "\n",
    "For this notebook, we mock responses from a large generative model.\n",
    "\n",
    "We encourage you to explore connecting to a generative model API of your choice, and comparing results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "mockedGenerativeResponses = pandas.read_csv(\"mockedGenerativeResponses.csv\")\n",
    "def get_generative_response(prompt):\n",
    "    prompt = prompt.replace(\"\"\"\n",
    "\"\"\", \"\\r\\n\")\n",
    "    prompt=prompt.replace(\"\\n\", \"\\n\") #marshall new lines into consistent format\n",
    "    response = mockedGenerativeResponses.loc[mockedGenerativeResponses[\"prompt\"] == prompt, \"response\"].values\n",
    "    if len(response) > 0:\n",
    "        output = f'''\n",
    "        <div style=\"overflow: auto; margin-bottom: 10px;\">\n",
    "          <h3 style=\"float: left; width: 100px; margin-right: 10px;\">Query:</h3>\n",
    "          <p style=\"overflow: hidden; margin-left: 110px;\"><pre>{html.escape(prompt)}<pre></p>\n",
    "        </div>\n",
    "\n",
    "        <div style=\"overflow: auto; margin-bottom: 10px;\">\n",
    "          <h3 style=\"float: left; width: 100px; margin-right: 10px;\">Response:</h3>\n",
    "          <p style=\"overflow: hidden; margin-left: 110px;\"><pre>{html.escape(response[0])}</pre></p>\n",
    "        </div>\n",
    "        '''\n",
    "        display(HTML(output))\n",
    "        return response[0]\n",
    "    else:\n",
    "        print(f\"\\n\\nSorry! your prompt does not have a mocked value.\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever from Listing 14.6\n",
    "\n",
    "See [../ch14/2.ch14-question-answering-CPU-data-preparation.ipynb#Listing-14.6](../ch14/2.ch14-question-answering-CPU-data-preparation.ipynb#Listing-14.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.remove_pipe(\"ner\")\n",
    "nlp.add_pipe(\"merge_noun_chunks\")\n",
    "determiners = \"all an another any both del each either every half la many much nary neither no some such that the them these this those\".split(\" \")\n",
    "def get_query_from_question(question):\n",
    "    query = []\n",
    "    doc = nlp(question)\n",
    "    for tok in doc:\n",
    "        if tok.pos_ in [\"NOUN\",\"VERB\"]:\n",
    "            query.append(tok.text)\n",
    "    if not len(query):\n",
    "        query = [question]\n",
    "    query = \" \".join(query)\n",
    "    for d in determiners:\n",
    "        query = query.replace(\" \"+d+\" \",\"\")\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriever(question):\n",
    "    contexts={\"id\":[],\"question\":[],\"context\":[],\"url\":[]}\n",
    "    query = get_query_from_question(question)\n",
    "    request = {\n",
    "        \"query\": query,\n",
    "        \"query_fields\": [\"body\"],\n",
    "        \"return_fields\": [\"id\", \"url\", \"body\"],\n",
    "        \"filters\": [(\"post_type\", \"answer\")],\n",
    "        \"limit\": 5\n",
    "    }\n",
    "    docs = outdoors_collection.search(**request)[\"docs\"]\n",
    "    for doc in docs:\n",
    "        contexts[\"id\"].append(doc[\"id\"])\n",
    "        contexts[\"url\"].append(doc[\"url\"])\n",
    "        contexts[\"question\"].append(question)\n",
    "        contexts[\"context\"].append(doc[\"body\"])\n",
    "    return pandas.DataFrame(contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 15.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"overflow: auto; margin-bottom: 10px;\">\n",
       "          <h3 style=\"float: left; width: 100px; margin-right: 10px;\">Query:</h3>\n",
       "          <p style=\"overflow: hidden; margin-left: 110px;\"><pre>What is a unicorn?<pre></p>\n",
       "        </div>\n",
       "\n",
       "        <div style=\"overflow: auto; margin-bottom: 10px;\">\n",
       "          <h3 style=\"float: left; width: 100px; margin-right: 10px;\">Response:</h3>\n",
       "          <p style=\"overflow: hidden; margin-left: 110px;\"><pre>A unicorn is a mythical creature that is typically depicted as a horse \r\n",
       "with a single, spiraled horn projecting from its forehead. Unicorns have \r\n",
       "been a part of human mythology and folklore for centuries and are often \r\n",
       "associated with purity, grace, and magic. They are said to possess healing \r\n",
       "powers and to be able to bring good luck to those who encounter them. \r\n",
       "While unicorns are not real animals, they continue to be a popular symbol \r\n",
       "in modern culture, appearing in literature, art, and popular media.</pre></p>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r = get_generative_response(\"What is a unicorn?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 15.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"overflow: auto; margin-bottom: 10px;\">\n",
       "          <h3 style=\"float: left; width: 100px; margin-right: 10px;\">Query:</h3>\n",
       "          <p style=\"overflow: hidden; margin-left: 110px;\"><pre>You are a chatbot named AIPS chat. \r\n",
       "You have a Ph.D. in biology. \r\n",
       "What is a unicorn?<pre></p>\n",
       "        </div>\n",
       "\n",
       "        <div style=\"overflow: auto; margin-bottom: 10px;\">\n",
       "          <h3 style=\"float: left; width: 100px; margin-right: 10px;\">Response:</h3>\n",
       "          <p style=\"overflow: hidden; margin-left: 110px;\"><pre>Hello! I am Chatbot named AIPS Chat, and I have a Ph.D. in biology. While \r\n",
       "unicorns have been depicted in various cultures throughout history, they \r\n",
       "are not real animals that exist in the natural world.\r\n",
       "\r\n",
       "From a biological perspective, a unicorn can be described as a mythical \r\n",
       "creature resembling a horse with a single, spiraled horn on its forehead. \r\n",
       "However, since unicorns are purely fictional, there is no actual \r\n",
       "scientific data or research that can be conducted on them.\r\n",
       "\r\n",
       "It is important to note that while many animals possess unique \r\n",
       "characteristics, such as the horn of a rhinoceros or the tusk of a \r\n",
       "narwhal, these features are not magical and do not provide any special \r\n",
       "abilities or powers to the animal possessing them.\r\n",
       "\r\n",
       "In conclusion, while unicorns are a fascinating aspect of human \r\n",
       "folklore and mythology, they do not have a basis in reality within the \r\n",
       "field of biology.</pre></p>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r = get_generative_response(\"\"\"You are a chatbot named AIPS chat. \n",
    "You have a Ph.D. in biology. \n",
    "What is a unicorn?\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 15.3\n",
    "\n",
    "## Listing 15.4\n",
    "\n",
    "(The prompt and responses for Listings 15.3 and 15.4 are both represented in one cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"overflow: auto; margin-bottom: 10px;\">\n",
       "          <h3 style=\"float: left; width: 100px; margin-right: 10px;\">Query:</h3>\n",
       "          <p style=\"overflow: hidden; margin-left: 110px;\"><pre>Web search results:\r\n",
       "\r\n",
       "[1] &quot;A large language model, or LLM, is a deep learning algorithm that \r\n",
       "can recognize, summarize, translate, predict and generate text and other \r\n",
       "content based on knowledge gained from massive datasets.&quot;\r\n",
       "URL: https://blogs.nvidia.com/blog/2023/01/26/what-are-large-language-models-used-for/\r\n",
       "\r\n",
       "[2] A large language model (LLM) is a language model consisting of a \r\n",
       "neural network with many parameters (typically billions of weights or \r\n",
       "more), trained on large quantities of unlabeled text using \r\n",
       "self-supervised learning. LLMs emerged around 2018 and perform well at a \r\n",
       "wide variety of tasks. This has shifted the focus of natural language \r\n",
       "processing research away from the previous paradigm of training \r\n",
       "specialized supervised models for specific tasks.\r\n",
       "URL: https://en.wikipedia.org/wiki/Large_language_model\r\n",
       "\r\n",
       "[3] &quot;In this guide, we&#x27;ll discuss everything you need to know about \r\n",
       "Large Language Models (LLMs), including key terms, algorithms, \r\n",
       "fine-tuning, and more. As ChatGPT has taken the internet by storm crossing \r\n",
       "1 million users in its first 5 days, you may be wondering what machine \r\n",
       "learning algorithm is running under the hood. While ChatGPT uses a \r\n",
       "specific type of reinforcement learning called &quot;Reinforcement Learning \r\n",
       "from Human Feedback (RLHF)&quot;, at a high level it is an example of a \r\n",
       "Large Language Model (LLM).&quot;\r\n",
       "URL: https://www.mlq.ai/what-is-a-large-language-model-llm/\r\n",
       "\r\n",
       "Instructions: Using the provided web search results, write a comprehensive \r\n",
       "reply to the given query. Make sure to cite results using \r\n",
       "[[number](URL)] notation after the reference. If the provided search \r\n",
       "results refer to multiple subjects with the same name, write separate \r\n",
       "answers for each subject.\r\n",
       "\r\n",
       "Query: What is a large language model?<pre></p>\n",
       "        </div>\n",
       "\n",
       "        <div style=\"overflow: auto; margin-bottom: 10px;\">\n",
       "          <h3 style=\"float: left; width: 100px; margin-right: 10px;\">Response:</h3>\n",
       "          <p style=\"overflow: hidden; margin-left: 110px;\"><pre>A large language model (LLM) is a deep learning algorithm that can \r\n",
       "recognize, summarize, translate, predict, and generate text and other \r\n",
       "content based on knowledge gained from massive datasets [[1](https://blogs.nvidia.com/blog/2023/01/26/what-are-large-language-models-used-for/)]. \r\n",
       "It consists of a neural network with many parameters (typically billions \r\n",
       "of weights or more) and is trained on large quantities of unlabeled text \r\n",
       "using self-supervised learning \r\n",
       "[[2](https://en.wikipedia.org/wiki/Large_language_model)]. LLMs emerged \r\n",
       "around 2018 and have performed well at a wide variety of tasks, shifting \r\n",
       "the focus of natural language processing research away from the previous \r\n",
       "paradigm of training specialized supervised models for specific tasks \r\n",
       "[[2](https://en.wikipedia.org/wiki/Large_language_model)]. One example of \r\n",
       "a LLM is ChatGPT, which uses a specific type of reinforcement learning \r\n",
       "called &quot;Reinforcement Learning from Human Feedback (RLHF)&quot; \r\n",
       "[[3](https://www.mlq.ai/what-is-a-large-language-model-llm/)].</pre></p>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r = get_generative_response(\"\"\"Web search results:\n",
    "\n",
    "[1] \"A large language model, or LLM, is a deep learning algorithm that \n",
    "can recognize, summarize, translate, predict and generate text and other \n",
    "content based on knowledge gained from massive datasets.\"\n",
    "URL: https://blogs.nvidia.com/blog/2023/01/26/what-are-large-language-models-used-for/\n",
    "\n",
    "[2] A large language model (LLM) is a language model consisting of a \n",
    "neural network with many parameters (typically billions of weights or \n",
    "more), trained on large quantities of unlabeled text using \n",
    "self-supervised learning. LLMs emerged around 2018 and perform well at a \n",
    "wide variety of tasks. This has shifted the focus of natural language \n",
    "processing research away from the previous paradigm of training \n",
    "specialized supervised models for specific tasks.\n",
    "URL: https://en.wikipedia.org/wiki/Large_language_model\n",
    "\n",
    "[3] \"In this guide, we'll discuss everything you need to know about \n",
    "Large Language Models (LLMs), including key terms, algorithms, \n",
    "fine-tuning, and more. As ChatGPT has taken the internet by storm crossing \n",
    "1 million users in its first 5 days, you may be wondering what machine \n",
    "learning algorithm is running under the hood. While ChatGPT uses a \n",
    "specific type of reinforcement learning called \"Reinforcement Learning \n",
    "from Human Feedback (RLHF)\", at a high level it is an example of a \n",
    "Large Language Model (LLM).\"\n",
    "URL: https://www.mlq.ai/what-is-a-large-language-model-llm/\n",
    "\n",
    "Instructions: Using the provided web search results, write a comprehensive \n",
    "reply to the given query. Make sure to cite results using \n",
    "[[number](URL)] notation after the reference. If the provided search \n",
    "results refer to multiple subjects with the same name, write separate \n",
    "answers for each subject.\n",
    "\n",
    "Query: What is a large language model?\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 15.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"overflow: auto; margin-bottom: 10px;\">\n",
       "          <h3 style=\"float: left; width: 100px; margin-right: 10px;\">Query:</h3>\n",
       "          <p style=\"overflow: hidden; margin-left: 110px;\"><pre>Web search results:\r\n",
       "\r\n",
       "[1] &quot;A large language model, or LLM, is a deep learning algorithm that \r\n",
       "can recognize, summarize, translate, predict and generate text and other \r\n",
       "content based on knowledge gained from massive datasets.&quot;\r\n",
       "URL: https://blogs.nvidia.com/blog/2023/01/26/what-are-large-language-models-used-for/\r\n",
       "\r\n",
       "[2] A large language model (LLM) is a language model consisting of a \r\n",
       "neural network with many parameters (typically billions of weights or \r\n",
       "more), trained on large quantities of unlabeled text using \r\n",
       "self-supervised learning. LLMs emerged around 2018 and perform well at a \r\n",
       "wide variety of tasks. This has shifted the focus of natural language \r\n",
       "processing research away from the previous paradigm of training \r\n",
       "specialized supervised models for specific tasks.\r\n",
       "URL: https://en.wikipedia.org/wiki/Large_language_model\r\n",
       "\r\n",
       "[3] &quot;In this guide, we&#x27;ll discuss everything you need to know about \r\n",
       "Large Language Models (LLMs), including key terms, algorithms, \r\n",
       "fine-tuning, and more. As ChatGPT has taken the internet by storm crossing \r\n",
       "1 million users in its first 5 days, you may be wondering what machine \r\n",
       "learning algorithm is running under the hood. While ChatGPT uses a \r\n",
       "specific type of reinforcement learning called &quot;Reinforcement Learning \r\n",
       "from Human Feedback (RLHF)&quot;, at a high level it is an example of a \r\n",
       "Large Language Model (LLM).&quot;\r\n",
       "URL: https://www.mlq.ai/what-is-a-large-language-model-llm/\r\n",
       "\r\n",
       "Instructions: Using the provided web search results, write a comprehensive \r\n",
       "reply to the given query. Make sure to cite results using \r\n",
       "[[number](URL)] notation after the reference. If the provided search \r\n",
       "results refer to multiple subjects with the same name, write separate \r\n",
       "answers for each subject.\r\n",
       "\r\n",
       "Query: What is a large language model?  Be concise.<pre></p>\n",
       "        </div>\n",
       "\n",
       "        <div style=\"overflow: auto; margin-bottom: 10px;\">\n",
       "          <h3 style=\"float: left; width: 100px; margin-right: 10px;\">Response:</h3>\n",
       "          <p style=\"overflow: hidden; margin-left: 110px;\"><pre>A large language model (LLM) is a deep learning algorithm that uses neural networks with billions of parameters, trained on massive unlabeled text datasets for various language tasks [[2](https://en.wikipedia.org/wiki/Large_language_model)][[1](https://blogs.nvidia.com/blog/2023/01/26/what-are-large-language-models-used-for/)].</pre></p>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r = get_generative_response(\"\"\"Web search results:\n",
    "\n",
    "[1] \"A large language model, or LLM, is a deep learning algorithm that \n",
    "can recognize, summarize, translate, predict and generate text and other \n",
    "content based on knowledge gained from massive datasets.\"\n",
    "URL: https://blogs.nvidia.com/blog/2023/01/26/what-are-large-language-models-used-for/\n",
    "\n",
    "[2] A large language model (LLM) is a language model consisting of a \n",
    "neural network with many parameters (typically billions of weights or \n",
    "more), trained on large quantities of unlabeled text using \n",
    "self-supervised learning. LLMs emerged around 2018 and perform well at a \n",
    "wide variety of tasks. This has shifted the focus of natural language \n",
    "processing research away from the previous paradigm of training \n",
    "specialized supervised models for specific tasks.\n",
    "URL: https://en.wikipedia.org/wiki/Large_language_model\n",
    "\n",
    "[3] \"In this guide, we'll discuss everything you need to know about \n",
    "Large Language Models (LLMs), including key terms, algorithms, \n",
    "fine-tuning, and more. As ChatGPT has taken the internet by storm crossing \n",
    "1 million users in its first 5 days, you may be wondering what machine \n",
    "learning algorithm is running under the hood. While ChatGPT uses a \n",
    "specific type of reinforcement learning called \"Reinforcement Learning \n",
    "from Human Feedback (RLHF)\", at a high level it is an example of a \n",
    "Large Language Model (LLM).\"\n",
    "URL: https://www.mlq.ai/what-is-a-large-language-model-llm/\n",
    "\n",
    "Instructions: Using the provided web search results, write a comprehensive \n",
    "reply to the given query. Make sure to cite results using \n",
    "[[number](URL)] notation after the reference. If the provided search \n",
    "results refer to multiple subjects with the same name, write separate \n",
    "answers for each subject.\n",
    "\n",
    "Query: What is a large language model?  Be concise.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 15.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are some queries that should find the following documents?  List at least 5 unique queries, where these documents are better than others in an outdoors question and answer dataset.  Be concise and only output the list of queries, and a result number in the format [n] for the best result in the resultset.  Don't print a relevance summary at the end.\n",
      "\n",
      "### Results:\n",
      "{resultset}\n"
     ]
    }
   ],
   "source": [
    "summarize_search_prompt = \"\"\"What are some queries that should find the following documents?  List at least 5 unique queries, where these documents are better than others in an outdoors question and answer dataset.  Be concise and only output the list of queries, and a result number in the format [n] for the best result in the resultset.  Don't print a relevance summary at the end.\n",
    "\n",
    "### Results:\n",
    "{resultset}\"\"\"\n",
    "print(summarize_search_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 15.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/models.py:971\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomplexjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    973\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m example_contexts \u001b[38;5;241m=\u001b[39m \u001b[43mretriever\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat are minimalist shoes?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m resultset \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mctx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, ctx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mlist\u001b[39m(example_contexts[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m5\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m]))]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(resultset))\n",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m, in \u001b[0;36mretriever\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m      3\u001b[0m query \u001b[38;5;241m=\u001b[39m get_query_from_question(question)\n\u001b[1;32m      4\u001b[0m request \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: query,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery_fields\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlimit\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m     10\u001b[0m }\n\u001b[0;32m---> 11\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[43moutdoors_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs:\n\u001b[1;32m     13\u001b[0m     contexts[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/notebooks/engine/solr/SolrCollection.py:104\u001b[0m, in \u001b[0;36mSolrCollection.search\u001b[0;34m(self, **search_args)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msearch_args):\n\u001b[1;32m    103\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_request(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msearch_args)\n\u001b[0;32m--> 104\u001b[0m     search_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnative_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_response(search_response)\n",
      "File \u001b[0;32m~/notebooks/engine/solr/SolrCollection.py:100\u001b[0m, in \u001b[0;36mSolrCollection.native_search\u001b[0;34m(self, request, data)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnative_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mSOLR_URL\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/select\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/models.py:975\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    972\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    973\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[0;32m--> 975\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "example_contexts = retriever(\"What are minimalist shoes?\")\n",
    "resultset = [f\"{idx}. {ctx}\" for idx, ctx in enumerate(list(example_contexts[0:5][\"context\"]))]\n",
    "print(\"\\n\".join(resultset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 15.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultset_text = \"\\n\".join(resultset)\n",
    "resultset_prompt = summarize_search_prompt.replace(\"{resultset}\", resultset_text)\n",
    "generated_relevance_judgments = get_generative_response(resultset_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 15.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_pairwise_judgments(text, contexts):\n",
    "    query_pattern = re.compile(r\"\\d+\\.\\s+(.*)\")\n",
    "    result_pattern = re.compile(r\"\\d+\\.\\s+\\[(\\d+)\\]\")    \n",
    "    lines = text.split(\"\\n\")\n",
    "    queries = []\n",
    "    results = []\n",
    "    for line in lines:\n",
    "        query_match = query_pattern.match(line)\n",
    "        result_match = result_pattern.match(line)\n",
    "        if result_match:\n",
    "            result_index = int(result_match.group(1))\n",
    "            results.append(result_index)            \n",
    "        elif query_match:\n",
    "            query = query_match.group(1)\n",
    "            queries.append(query)            \n",
    "    output = [{\"query\": query, \"relevant_document\": contexts[result][\"id\"]}\n",
    "              for query, result in zip(queries, results)]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 15.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultset_contexts = example_contexts.to_dict(\"records\")\n",
    "extract_pairwise_judgments(generated_relevance_judgments, resultset_contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 15.11\n",
    "\n",
    "Article source: Alan Morrell, Democrat and Chronicle, May 29, 2023 https://www.democratandchronicle.com/story/money/business/2023/05/29/carvel-is-gone-from-rochester-what-happened-to-the-ice-cream-chain/70252613007/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_article_snippet = \"Walter Silverman of Brighton owned one of the most successful local Carvel franchises, at East Ridge Road and Hudson Avenue in Irondequoit. He started working for Carvel in 1952. This is how it appeared in the late 1970s/early 1980s.\"\n",
    "news_article_labelled = \"\"\"<per>Walter Silverman</per> of <loc>Brighton</loc> owned one of the most successful local <org>Carvel</org> franchises, at <loc>East Ridge Road</loc> and <loc>Hudson Avenue</loc> in <loc>Irondequoit</loc>. He started working for <org>Carvel</org> in 1952. This is how it appeared in the late 1970s/early 1980s.\"\"\"\n",
    "print(news_article_labelled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 15.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(text):\n",
    "    entities = []\n",
    "    pattern = r\"<(per|loc|org)>(.*?)<\\/(per|loc|org)>\"\n",
    "    matches = re.finditer(pattern, text)\n",
    "    for match in matches:\n",
    "        entity = {\n",
    "            \"label\": match.group(1).upper(),\n",
    "            \"offset\": [match.start(), match.end() - 1],\n",
    "            \"text\": match.group(2)\n",
    "        }\n",
    "        entities.append(entity)\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our manually labelled article\n",
    "extract_entities(news_article_labelled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 15.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_prompt = \"\"\"For a given passage, please identify and mark the following entities: people with the tag '<per>', locations with the tag '<loc>', and organizations with the tag '<org>'. Please repeat the passage below with the appropriate markup.\n",
    "### {text}\"\"\"\n",
    "entities_prompt_news_article = entities_prompt.replace(\"{text}\",news_article_snippet)\n",
    "news_article_generated_labelled = get_generative_response(entities_prompt_news_article)\n",
    "extract_entities(news_article_generated_labelled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up next: [Multi-modal Search](2.multi-modal-search.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
