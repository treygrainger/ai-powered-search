{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load judgments & pointwise training set\n",
    "\n",
    "Load the dataset generated [from the previous section](http://localhost:8888/notebooks/ch10/3.ch10-pairwise-transform.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.judgments import judgments_open\n",
    "\n",
    "pointwise_predictors = np.load('data/pointwise_predictors.npy')\n",
    "feature_data = np.load('data/feature_data.npy')\n",
    "\n",
    "std_devs = feature_data[-1]\n",
    "means = feature_data[-2]\n",
    "pointwise_features = feature_data[:-2]\n",
    "\n",
    "normed_judgments = []\n",
    "with judgments_open('data/normed_judgments.txt') as judg_list:\n",
    "    for j in judg_list:\n",
    "        normed_judgments.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ltr.judgments import judgments_from_file, judgments_to_nparray\n",
    "\n",
    "def pairwise_transform(normed_judgments):\n",
    "        \n",
    "    from itertools import groupby\n",
    "    pointwise_predictors = []\n",
    "    pointwise_features = []\n",
    "    \n",
    "    # For each query's judgments\n",
    "    for qid, query_judgments in groupby(normed_judgments, key=lambda j: j.qid):\n",
    "\n",
    "        # Annoying issue consuming python iterators, we ensure we have two\n",
    "        # full copies of each query's judgments\n",
    "        query_judgments_copy_1 = list(query_judgments) \n",
    "        query_judgments_copy_2 = list(query_judgments_copy_1)\n",
    "\n",
    "        # Examine every judgment combo for this query, \n",
    "        # if they're different, store the pairwise difference:\n",
    "        # +1 if judgment1 more relevant\n",
    "        # -1 if judgment2 more relevant\n",
    "        for judgment1 in query_judgments_copy_1:\n",
    "            for judgment2 in query_judgments_copy_2:\n",
    "                \n",
    "                j1_features=np.array(judgment1.features)\n",
    "                j2_features=np.array(judgment2.features)\n",
    "                \n",
    "                if judgment1.grade > judgment2.grade:\n",
    "                    pointwise_predictors.append(+1)\n",
    "                    pointwise_features.append(j1_features-j2_features)\n",
    "                elif judgment1.grade < judgment2.grade:\n",
    "                    pointwise_predictors.append(-1)\n",
    "                    pointwise_features.append(j1_features-j2_features)\n",
    "\n",
    "    return np.array(pointwise_features), np.array(pointwise_predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 10.16\n",
    "\n",
    "Train the model with the fully transformed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "model = svm.LinearSVC(max_iter=10000, verbose=1)\n",
    "model.fit(pointwise_features, pointwise_predictors)\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features to eval model on some movies..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you wanted to confirm Wrath of Khans features\n",
    "import requests\n",
    "\n",
    "logging_solr_query = {\n",
    "    \"fl\": \"id,title,[features store=movies efi.keywords=\\\"wrath of khan\\\"]\",\n",
    "    'q': \"id:154\", #social network graded documents\n",
    "    'rows': 10,\n",
    "    'wt': 'json'  \n",
    "}\n",
    "\n",
    "resp = requests.post('http://aips-solr:8983/solr/tmdb/select',\n",
    "                     data=logging_solr_query)\n",
    "\n",
    "# Features Solr returns\n",
    "# Wrath of Khan\n",
    "wok_features = [5.9217176, 3.401492, 1982.0]\n",
    "# Search For Spock\n",
    "spock_features = [0.0,0.0,1984.0]\n",
    "\n",
    "# Wrath of Khan\n",
    "normed_wok_features = [0,0,0]\n",
    "for idx, f in enumerate(wok_features):\n",
    "    normed_wok_features[idx] = (f - means[idx]) / std_devs[idx]\n",
    "\n",
    "normed_spock_features = [0,0,0]\n",
    "for idx, f in enumerate(spock_features):\n",
    "    normed_spock_features[idx] = (f - means[idx]) / std_devs[idx]\n",
    "    \n",
    "normed_spock_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking the model for test drive...\n",
    "\n",
    "Here we score a few documents with the model. This code is omitted from the book, but is explored in section 10.6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_one(features, model):\n",
    "    score = 0.0\n",
    "    for idx, f in enumerate(features):\n",
    "        this_coef = model.coef_[0][idx].item()\n",
    "        score += f * this_coef\n",
    "    \n",
    "    return score\n",
    "\n",
    "def rank(query_judgments, model):\n",
    "    for j in query_judgments:\n",
    "        j.score = score_one(j.features, model)\n",
    "    \n",
    "    return sorted(query_judgments, key=lambda j: j.score, reverse=True)\n",
    "\n",
    "score_one(normed_spock_features, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrath of Khan should score higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_one(normed_wok_features, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 10.17 Test Training Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "all_qids = list(set([j.qid for j in normed_judgments]))\n",
    "random.shuffle(all_qids)\n",
    "\n",
    "proportion_train=0.1\n",
    "\n",
    "test_train_split_idx = int(len(all_qids) * proportion_train)\n",
    "test_qids=all_qids[:test_train_split_idx]\n",
    "train_qids=all_qids[test_train_split_idx:]\n",
    "\n",
    "test_qids,train_qids\n",
    "\n",
    "train_data = []; test_data=[]\n",
    "for j in normed_judgments:\n",
    "    if j.qid in train_qids:\n",
    "        train_data.append(j)\n",
    "    elif j.qid in test_qids:\n",
    "        test_data.append(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 10.18 - train on just train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_features, train_data_predictors = pairwise_transform(train_data)\n",
    "\n",
    "from sklearn import svm\n",
    "model = svm.LinearSVC(max_iter=10000, verbose=1)\n",
    "model.fit(train_data_features, train_data_predictors)\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 10.19 - eval model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(test_data, model):\n",
    "    from itertools import groupby\n",
    "    \n",
    "    tot_prec = 0\n",
    "    num_queries = 0\n",
    "\n",
    "    for qid, query_judgments in groupby(test_data, key=lambda j: j.qid):\n",
    "        query_judgments = list(query_judgments)\n",
    "\n",
    "        ranked = rank(query_judgments, model)\n",
    "\n",
    "        tot_relevant = 0\n",
    "        for j in ranked[:4]:\n",
    "            if j.grade == 1:\n",
    "                tot_relevant += 1\n",
    "        query_prec = tot_relevant/4.0\n",
    "        tot_prec += query_prec\n",
    "        num_queries += 1\n",
    "    \n",
    "    return tot_prec / num_queries\n",
    "\n",
    "eval_model(test_data, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 10.20 - Solr model\n",
    "\n",
    "This turns the model into one usable by Solr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "linear_model = {\n",
    "  \"store\": \"movies\",\n",
    "  \"class\": \"org.apache.solr.ltr.model.LinearModel\",\n",
    "  \"name\": \"movie_model\",\n",
    "  \"features\": [\n",
    "  ],\n",
    "  \"params\": {\n",
    "      \"weights\": {\n",
    "      }\n",
    "  }\n",
    "}\n",
    "\n",
    "import math\n",
    "ftr_model = {}\n",
    "ftr_names = ['title_bm25', 'overview_bm25', 'release_year']\n",
    "for idx, ftr_name in enumerate(ftr_names):\n",
    "    config = {\n",
    "        \"name\": ftr_name,\n",
    "        \"norm\": {\n",
    "            \"class\": \"org.apache.solr.ltr.norm.StandardNormalizer\",\n",
    "            \"params\": {\n",
    "                \"avg\": str(means[idx]),\n",
    "                \"std\": str(std_devs[idx])\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    linear_model['features'].append(config)\n",
    "    linear_model['params']['weights'][ftr_name] =  model.coef_[0][idx] \n",
    "\n",
    "print(\"PUT http://aips-solr:8983/solr/tmdb/schema/model-store\")\n",
    "print(json.dumps(linear_model, indent=2))\n",
    "\n",
    "# Upload the model\n",
    "requests.put('http://aips-solr:8983/solr/tmdb/schema/model-store', json=linear_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 10.21 - Solr query w/ model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = {\n",
    "    \"fields\": [\"title\", \"id\", \"score\"],\n",
    "    \"limit\": 5,\n",
    "    \"params\": {\n",
    "      \"q\": \"{!ltr reRankDocs=60000 model=movie_model efi.keywords=\\\"harry potter\\\"}\",\n",
    "     \n",
    "    }\n",
    "}\n",
    "\n",
    "resp = requests.post('http://aips-solr:8983/solr/tmdb/select', json=request)\n",
    "\n",
    "resp.json()[\"response\"][\"docs\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 10.22 - Solr query w/ model and reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = {\n",
    "    \"fields\": [\"title\", \"id\", \"score\"],\n",
    "    \"limit\": 5,\n",
    "    \"params\": {\n",
    "      \"rq\": \"{!ltr reRankDocs=500 model=movie_model efi.keywords=\\\"harry potter\\\"}\",\n",
    "      \"qf\": \"title overview\",\n",
    "      \"defType\": \"edismax\",\n",
    "      \"q\": \"harry potter\"\n",
    "    }\n",
    "}\n",
    "\n",
    "resp = requests.post('http://aips-solr:8983/solr/tmdb/select', json=request)\n",
    "\n",
    "resp.json()[\"response\"][\"docs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
