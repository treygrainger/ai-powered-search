{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Judgments and Feature Logging\n",
    "\n",
    "In this notebook, we cover the first two steps of Learning to Rank. First we grade some documents as relevant/irrelevant for queries, what we call _judgments_. Second, we retrieve some _features_ - metadata about each graded document in our judgments. We call the process of extracting the features from Solr _feature logging_\n",
    "\n",
    "NOTE: This notebook depends upon TheMovieDB dataset. If you have any issues, please rerun the [Setting up TheMovieDB notebook](1.setup-the-movie-db.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from aips import *\n",
    "engine = get_engine()\n",
    "tmdb_collection = engine.get_collection(\"tmdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ommitted from book\n",
    "A single judgment, grading document 37799 (\"The Social Network\") as relevant (`grade=1`) for the search query string `social network`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Judgment(grade=1,qid=1,keywords=social network,doc_id=37799,features=[],weight=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ltr.judgments import Judgment\n",
    "\n",
    "Judgment(grade=1, keywords='social network', doc_id=37799)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 10.3\n",
    "\n",
    "A bit bigger judgment list. Here two query strings are graded: `social network` and `star wars`. For `social network` a single movie is graded as relevant, three are irrelevant. Two movies are graded as relevant for `star wars`, three others graded as irrelevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Judgment(grade=1,qid=1,keywords=social network,doc_id=37799,features=[],weight=1),\n",
       " Judgment(grade=0,qid=1,keywords=social network,doc_id=267752,features=[],weight=1),\n",
       " Judgment(grade=0,qid=1,keywords=social network,doc_id=38408,features=[],weight=1),\n",
       " Judgment(grade=0,qid=1,keywords=social network,doc_id=28303,features=[],weight=1),\n",
       " Judgment(grade=1,qid=2,keywords=star wars,doc_id=11,features=[],weight=1),\n",
       " Judgment(grade=1,qid=2,keywords=star wars,doc_id=1892,features=[],weight=1),\n",
       " Judgment(grade=0,qid=2,keywords=star wars,doc_id=54138,features=[],weight=1),\n",
       " Judgment(grade=0,qid=2,keywords=star wars,doc_id=85783,features=[],weight=1),\n",
       " Judgment(grade=0,qid=2,keywords=star wars,doc_id=325553,features=[],weight=1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_judgements = [\n",
    "    # for 'social network' query\n",
    "    Judgment(1, 'social network', '37799'),  # The Social Network\n",
    "    Judgment(0, 'social network', '267752'), # #chicagoGirl\n",
    "    Judgment(0, 'social network', '38408'),  # Life As We Know It\n",
    "    Judgment(0, 'social network', '28303'),  # The Cheyenne Social Club\n",
    "    \n",
    "    # for 'star wars' query\n",
    "    Judgment(1, 'star wars', '11'),     # star wars\n",
    "    Judgment(1, 'star wars', '1892'),   # return of jedi\n",
    "    Judgment(0, 'star wars', '54138'),  # Star Trek Into Darkness\n",
    "    Judgment(0, 'star wars', '85783'),  # The Star\n",
    "    Judgment(0, 'star wars', '325553')  # Battlestar Galactica\n",
    "]\n",
    "sample_judgements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrating we have no features for any of our judgments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_judgements[0].features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 10.4\n",
    "\n",
    "Create a feature set, the first feature retrieves the relevance score of the search string in the `title` field (hence `title:(${keywords})`), the second feature the same for `overview`, finally the third feature is simply the `release_year` of the movie. \n",
    "\n",
    "We create a feature store named `movies` in Solr. We'll use the feature store name as a handle when we want to log features farther down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'responseHeader': {'status': 0, 'QTime': 3}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.delete_feature_store(tmdb_collection, \"movies\")\n",
    "\n",
    "feature_set = [\n",
    "    {\n",
    "      \"name\": \"title_bm25\",\n",
    "      \"store\": \"movies\",\n",
    "      \"class\": \"org.apache.solr.ltr.feature.SolrFeature\",\n",
    "      \"params\": {\"q\": \"title:(${keywords})\"}\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"overview_bm25\",\n",
    "      \"store\": \"movies\",\n",
    "      \"class\": \"org.apache.solr.ltr.feature.SolrFeature\",\n",
    "      \"params\": {\"q\": \"overview:(${keywords})\"}\n",
    "    },\n",
    "    {\n",
    "      \"name\" : \"release_year\",\n",
    "      \"store\": \"movies\",\n",
    "      \"class\": \"org.apache.solr.ltr.feature.SolrFeature\",\n",
    "      \"params\": {\"q\": \"{!func}release_year\"}}]\n",
    "\n",
    "resp = engine.create_feature_store(tmdb_collection, feature_set)\n",
    "resp.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 10.5\n",
    "\n",
    "Recall we have one relevant and three irrelevant movies for `social network`. Here we retrieve all three features created above for each of the four movies. The special `[features..`], tells Solr to append the features from `movies` feature store using the template param `efi.keywords=\"social network\"` in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'responseHeader': {'zkConnected': True,\n",
       "  'status': 0,\n",
       "  'QTime': 11,\n",
       "  'params': {'q': 'id:37799 OR id:267752 OR id:38408 OR id:28303',\n",
       "   'fl': 'id,title,[features store=movies efi.keywords=\"social network\"]',\n",
       "   'rows': '10',\n",
       "   'wt': 'json'}},\n",
       " 'response': {'numFound': 4,\n",
       "  'start': 0,\n",
       "  'numFoundExact': True,\n",
       "  'docs': [{'id': '38408',\n",
       "    'title': 'Life As We Know It',\n",
       "    '[features]': 'title_bm25=0.0,overview_bm25=4.353118,release_year=2010.0'},\n",
       "   {'id': '28303',\n",
       "    'title': 'The Cheyenne Social Club',\n",
       "    '[features]': 'title_bm25=3.4286604,overview_bm25=3.1086721,release_year=1970.0'},\n",
       "   {'id': '37799',\n",
       "    'title': 'The Social Network',\n",
       "    '[features]': 'title_bm25=8.243603,overview_bm25=3.8143613,release_year=2010.0'},\n",
       "   {'id': '267752',\n",
       "    'title': '#chicagoGirl',\n",
       "    '[features]': 'title_bm25=0.0,overview_bm25=6.0172443,release_year=2013.0'}]}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "logging_solr_query = {\n",
    "    \"q\": \"id:37799 OR id:267752 OR id:38408 OR id:28303\", #social network graded documents\n",
    "    \"fl\": 'id,title,[features store=movies efi.keywords=\"social network\"]',\n",
    "    \"rows\": 10,\n",
    "    \"wt\": \"json\"  \n",
    "}\n",
    "response = tmdb_collection.search(data=logging_solr_query)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Omitted from book - parse features Solr response\n",
    "\n",
    "**The following code is used to generate Listings below. But this parsing code is omitted from the book itself.**\n",
    "\n",
    "This code simply looks at the solr_resp and populates the corresponding judgments feature vector with the `[features]` from the corresponding solr document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_features_for_qid(qid, docs, judgements):\n",
    "    doc_id_to_features = {}\n",
    "\n",
    "    # Map Doc Id => Features\n",
    "    for doc in docs:\n",
    "        # Parse \"[features] array\", ie\n",
    "        # title_bm25=0.0,overview_bm25=13.237938,vote_average=7.0\"\n",
    "        features = doc[\"[features]\"]\n",
    "        features = features.split(\",\")\n",
    "        features = [float(ftr.split(\"=\")[1]) for ftr in features]\n",
    "\n",
    "        doc_id_to_features[doc[\"id\"]] = features\n",
    "\n",
    "    # Save in correct judgment\n",
    "    for judgment in judgements:\n",
    "        if judgment.qid == qid:\n",
    "            try:\n",
    "                judgment.features = doc_id_to_features[judgment.doc_id]\n",
    "            except KeyError:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Listing 10.6 (output)\n",
    "\n",
    "Listing 10.7 is the output of the following, the resulting processing of logging just for `social network`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Judgment(grade=1,qid=1,keywords=social network,doc_id=37799,features=[8.243603, 3.8143613, 2010.0],weight=1),\n",
       " Judgment(grade=0,qid=1,keywords=social network,doc_id=267752,features=[0.0, 6.0172443, 2013.0],weight=1),\n",
       " Judgment(grade=0,qid=1,keywords=social network,doc_id=38408,features=[0.0, 4.353118, 2010.0],weight=1),\n",
       " Judgment(grade=0,qid=1,keywords=social network,doc_id=28303,features=[3.4286604, 3.1086721, 1970.0],weight=1),\n",
       " Judgment(grade=1,qid=2,keywords=star wars,doc_id=11,features=[],weight=1),\n",
       " Judgment(grade=1,qid=2,keywords=star wars,doc_id=1892,features=[],weight=1),\n",
       " Judgment(grade=0,qid=2,keywords=star wars,doc_id=54138,features=[],weight=1),\n",
       " Judgment(grade=0,qid=2,keywords=star wars,doc_id=85783,features=[],weight=1),\n",
       " Judgment(grade=0,qid=2,keywords=star wars,doc_id=325553,features=[],weight=1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "populate_features_for_qid(1, engine.docs_from_response(response), sample_judgements)\n",
    "sample_judgements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 10.7 (output)\n",
    "\n",
    "Listing 10.8 is the output of the following, which adds features parsed from the `star wars` movies to our judgment list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Judgment(grade=1,qid=1,keywords=social network,doc_id=37799,features=[8.243603, 3.8143613, 2010.0],weight=1),\n",
       " Judgment(grade=0,qid=1,keywords=social network,doc_id=267752,features=[0.0, 6.0172443, 2013.0],weight=1),\n",
       " Judgment(grade=0,qid=1,keywords=social network,doc_id=38408,features=[0.0, 4.353118, 2010.0],weight=1),\n",
       " Judgment(grade=0,qid=1,keywords=social network,doc_id=28303,features=[3.4286604, 3.1086721, 1970.0],weight=1),\n",
       " Judgment(grade=1,qid=2,keywords=star wars,doc_id=11,features=[6.7963624, 0.0, 1977.0],weight=1),\n",
       " Judgment(grade=1,qid=2,keywords=star wars,doc_id=1892,features=[0.0, 1.9681965, 1983.0],weight=1),\n",
       " Judgment(grade=0,qid=2,keywords=star wars,doc_id=54138,features=[2.444128, 0.0, 2013.0],weight=1),\n",
       " Judgment(grade=0,qid=2,keywords=star wars,doc_id=85783,features=[3.1871135, 0.0, 1952.0],weight=1),\n",
       " Judgment(grade=0,qid=2,keywords=star wars,doc_id=325553,features=[0.0, 0.0, 2003.0],weight=1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging_solr_query = {\n",
    "    \"fl\": 'id,title,[features store=movies efi.keywords=\"star wars\"]',\n",
    "    'q': \"id:11 OR id:1892 OR id:54138 OR id:85783 OR id:325553\", #star wars graded documents\n",
    "    'rows': 10,\n",
    "    'wt': 'json'  \n",
    "}\n",
    "\n",
    "response = tmdb_collection.search(data=logging_solr_query)\n",
    "documents = engine.docs_from_response(response)\n",
    "populate_features_for_qid(2, documents, sample_judgements)\n",
    "sample_judgements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading / logging training set (omitted from book)\n",
    "\n",
    "The following downloads a larger judgment list, parses it, and logs features for each graded document. It just repeats the full logging workflow in this notebook but in one loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m judgments_open(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/ai_pow_search_judgments.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m judgements:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m qid, query_judgments \u001b[38;5;129;01min\u001b[39;00m groupby(judgements, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m j: j\u001b[38;5;241m.\u001b[39mqid):\n\u001b[0;32m---> 10\u001b[0m         \u001b[43mftr_logger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_for_qid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_judgments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjudgements\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeywords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m ftr_logger\u001b[38;5;241m.\u001b[39mlogged\n",
      "File \u001b[0;32m~/notebooks/ltr/log.py:63\u001b[0m, in \u001b[0;36mFeatureLogger.log_for_qid\u001b[0;34m(self, judgments, qid, keywords)\u001b[0m\n\u001b[1;32m     56\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m\"\u001b[39m: fixed_keywords,\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfuzzy_keywords\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([x \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m~\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m fixed_keywords\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)]),\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqueezed_keywords\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(fixed_keywords\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     60\u001b[0m }\n\u001b[1;32m     62\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(doc_id) \u001b[38;5;28;01mfor\u001b[39;00m doc_id \u001b[38;5;129;01min\u001b[39;00m ids]\n\u001b[0;32m---> 63\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_query\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid_field\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Add feature back to each judgment\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m res:\n",
      "File \u001b[0;32m~/notebooks/solr.py:310\u001b[0m, in \u001b[0;36mSolrEngine.log_query\u001b[0;34m(self, collection, featureset, ids, options, id_field)\u001b[0m\n\u001b[1;32m    303\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfl\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mid_field\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,[features store=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeatureset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mefi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124m!terms f=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m}}\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(id_field, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(ids)) \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*:*\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrows\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwt\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m }\n\u001b[1;32m    309\u001b[0m resp \u001b[38;5;241m=\u001b[39m collection\u001b[38;5;241m.\u001b[39msearch(data\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m--> 310\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[43mresp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# Clean up features to consistent format\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m docs:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'json'"
     ]
    }
   ],
   "source": [
    "from ltr.log import FeatureLogger\n",
    "from ltr.judgments import judgments_open\n",
    "from itertools import groupby\n",
    "from ltr import download\n",
    "\n",
    "ftr_logger = FeatureLogger(engine, tmdb_collection, feature_set=\"movies\")\n",
    "\n",
    "with judgments_open(\"data/ai_pow_search_judgments.txt\") as judgements:\n",
    "    for qid, query_judgments in groupby(judgements, key=lambda j: j.qid):\n",
    "        ftr_logger.log_for_qid(query_judgments, qid, judgements.keywords(qid))\n",
    "    \n",
    "ftr_logger.logged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next up, prep data for training\n",
    "\n",
    "We now have a dataset extracted from the search engine. Next we need to perform some manipulation to the training data. This manipulation turns our slightly strange looking ranking problem into one that looks more like any-other boring machine learning problem.\n",
    "\n",
    "Up next: [Feature Normalization and Pairwise Transform](3.pairwise-transform.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
