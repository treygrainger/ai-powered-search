{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Related keywords detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: This notebook depends upon the the Retrotech dataset. If you have any issues, please rerun the [Setting up the Retrotech Dataset](../ch4/1.ch4-setting-up-the-retrotech-dataset.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Approcah (1): Utilize user and search query behavior data to find related keyword\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"aips-ch6\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Prepare the data using py-spark and data frames \n",
    "get candidate from wordnet if both query and candidate are included in wordnet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the real signals\n",
    "signals_collection=\"signals\"\n",
    "signals_opts={\"zkhost\": \"aips-zk\", \"collection\": signals_collection}\n",
    "df = spark.read.format(\"solr\").options(**signals_opts).load()\n",
    "df.createOrReplaceTempView(\"signals\")\n",
    "\n",
    "spark.sql(''' select query_id, target as query from signals where type='query' ''').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark.sql(\"\"\"select lower(searches.target) as keyword, searches.user as user, clicks.target as upc \n",
    "from signals as searches right join signals as clicks on searches.query_id = clicks.query_id \n",
    "where searches.type='query' and clicks.type = 'click'\"\"\").createOrReplaceTempView('keyword_click_product')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 : Create Cooccurrence & PMI2  Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MK: Create co-occ model to compute the correlation between keywords based on the products,  \n",
    "###Corr will be computed using pmi, pmi2\n",
    "### (Keyword1, keywords, co-occ) co-occ is how many users uses these keywords with this product  \n",
    "spark.sql('''\n",
    "select k1.keyword as k1, k2.keyword as k2, sum(p1) n_users1,sum(p2) n_users1, \n",
    "sum(p1+p2) as users_cooc, count(1) n_products\n",
    "from\n",
    "(select keyword, upc, count(1) as p1 from keyword_click_product group by keyword, upc) as k1 \n",
    "join\n",
    "(select keyword, upc, count(1) as p2 from keyword_click_product group by keyword, upc) as k2\n",
    "on k1.upc = k2.upc\n",
    "where k1.keyword > k2.keyword \n",
    "group by k1.keyword, k2.keyword\n",
    "''').createOrReplaceTempView('keyword_click_product_cooc')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check co-occ table\n",
    "spark.sql('''select count(1) as keyword_click_product_cooc from keyword_click_product_cooc''').show()\n",
    "\n",
    "spark.sql('''select * from keyword_click_product_cooc order by n_products desc''').show(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mk: compute the pop of each keyword\n",
    "spark.sql('''\n",
    "select keyword, count(1) as n_users from keyword_click_product group by keyword\n",
    "''').registerTempTable('keyword_click_product_oc')\n",
    "\n",
    "spark.sql('''select count(1) as keyword_click_product_oc from keyword_click_product_oc''').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Mk: Based on co-occ and pop for each pair of the keywords , compute the PMI2\n",
    "spark.sql('''\n",
    "select k1.keyword as k1, k2.keyword as k2, k1_k2.users_cooc, k1.n_users as n_users1,k2.n_users as n_users2,\n",
    "k1_k2.users_cooc/(k1.n_users*k2.n_users) as pmi2\n",
    "from\n",
    "keyword_click_product_cooc as k1_k2 \n",
    "join\n",
    "keyword_click_product_oc as k1 on k1_k2.k1 = k1.keyword\n",
    "join\n",
    "keyword_click_product_oc as k2 on k1_k2.k2 = k2.keyword\n",
    "''').registerTempTable('related_keywords_pmi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check pmi table\n",
    "spark.sql('''select count(1) as related_keywords_pmi from related_keywords_pmi''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MK: Get sample data from the table and filter the noisy signals based on Pop and PMI2. \n",
    "## TG to MK - What is this showing? I'm not sue if this data is super useful to display (doesn't look very clear)\n",
    "\n",
    "data = spark.sql('''\n",
    "select * from(\n",
    "select k1, k2, pmi2, \n",
    "row_number() over (PARTITION BY k1 order by pmi2 desc ) rnum\n",
    "from related_keywords_pmi \n",
    "where users_cooc > 10 and pmi2 > 0.01 \n",
    "and k1 in ('lcd tv', 'ipad', 'laptop', 'iphone 4') ) x where rnum <= 20 ''').show(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## ------- need to be fixed \n",
    "#spark.sql('''\n",
    "#select a.* ,(a.r1+a.r2 /(a.r1*a.r2))/2 as comp_score \n",
    "#from ( select *,  row_number() over (order by users_cooc desc )  r1,  \n",
    "#row_number() over ( order by pmi2 desc )  r2 from related_keywords_pmi) as a ''').show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TG - average cooc with pmi2 for now to get a composite score:\n",
    "#TODO @MK: can you replace with Composit rank score based upon position in list? from \"jargon\" paper?\n",
    "spark.sql('''select *, (users_cooc + pmi2)/2 as score from related_keywords_pmi order by score desc''').show(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2:  Using NLP approach  \n",
    "\n",
    "\n",
    "### Step 1: data cleaning\n",
    "perform minimum stemming on queries and drop rare queries with count of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet \n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim.models import Word2Vec\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TG to MK: Can you switch this to use the dataframe instead of the CSV, please? Data is already indexed into Solr.\n",
    "csvFile = \"../data/retrotech/products.csv\"\n",
    "import csv\n",
    "reader = csv.reader(open(csvFile,'r'))\n",
    "products = []\n",
    "prod2 = []\n",
    "for r in reader:\n",
    "    if len(r)> 5:\n",
    "        prod2.append(r)\n",
    "        continue\n",
    "    products.append(r)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(products), len(prod2)\n",
    "products[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggr_signals = aggr_signals[aggr_signals[\"count\"] > 1]\n",
    "aggr_signals.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: find synonym from wordnet\n",
    "get candidate from wordnet if both query and candidate are included in wordnet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonym = []\n",
    "queries = []\n",
    "\n",
    "for query in tqdm(aggr_signals[\"query\"]):\n",
    "    for synset in wordnet.synsets(query):\n",
    "        for lemma in synset.lemmas():\n",
    "            candidate = lemma.name().replace(\"_\", \" \")\n",
    "            if candidate in list(aggr_signals[\"query\"]) and candidate != query:\n",
    "                queries.append(query)\n",
    "                synonym.append(candidate)\n",
    "                \n",
    "wordnet_result = pd.DataFrame({\"queries\":queries,\"synonym\":synonym}).drop_duplicates()\n",
    "wordnet_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: find synonym from word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine queries and product descriptions to be fed into word2vec model\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenized_description = [tokenizer.tokenize(text.lower()) for text in product_description.longDescription]\n",
    "tokenized_query = [tokenizer.tokenize(text) for text in aggr_signals[\"query\"]]\n",
    "tokenized_all = tokenized_query + tokenized_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute word2vec for each word\n",
    "model = Word2Vec(tokenized_all, vector_size=150, window=8, min_count=1, workers=-1)\n",
    "word2vec = dict(zip(model.wv.index_to_key, model.wv.vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build word weights dictionary to weigh word vectors, so that rare words get more weights\n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "tfidf.fit(tokenized_all)\n",
    "\n",
    "max_idf = max(tfidf.idf_) #the default idf is the max of idf's for unseen words\n",
    "weights = defaultdict(lambda: max_idf, [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vectors = []\n",
    "tokenized_query_clean = [x for x in tokenized_query if x != []]\n",
    "queries_cleaned = [\" \".join(x) for x in tokenized_query_clean]\n",
    "\n",
    "for tokens in tokenized_query_clean:\n",
    "    tmp = []\n",
    "    for token in tokens:\n",
    "        if token in word2vec.keys():\n",
    "            tmp.append(word2vec[token] * weights[token])\n",
    "        else:\n",
    "            tmp.append(np.zeros(150))\n",
    "    query_vectors.append(np.mean(tmp, axis=0).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = NearestNeighbors(n_neighbors=5, metric='cosine', algorithm='brute', n_jobs=-1)\n",
    "knn.fit(np.stack(query_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = []\n",
    "synonym = []\n",
    "cosine = []\n",
    "\n",
    "for i in tqdm(range(len(queries_cleaned))):\n",
    "    synonym_candidates = knn.kneighbors(np.expand_dims(query_vectors[i],axis=0), n_neighbors=6)\n",
    "    query_index = synonym_candidates[1][0].tolist()[1:] #drop first candidate which is the same of the original query\n",
    "    cosine_similarity = [1-x for x in synonym_candidates[0][0].tolist()[1:]]\n",
    "    query = queries_cleaned[i]\n",
    "    for j in range(0,5):\n",
    "        queries.append(query)\n",
    "        synonym.append(queries_cleaned[query_index[j]])\n",
    "        cosine.append(cosine_similarity[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonym_candidates = pd.DataFrame({\"queries\":queries, \"synonym\":synonym, \"cosine\":cosine})\n",
    "synonym_candidates.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply cosine similarity threshold to further filter candidate synonyms\n",
    "cosine_threshold = 0.7\n",
    "word2vec_result = synonym_candidates[synonym_candidates.cosine >= cosine_threshold][[\"queries\",\"synonym\"]]\n",
    "word2vec_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: combine wordnet and word2vec lists\n",
    "the pair query A and synonym B is the same as the pair query B and synonym A, sort query and synonym to keep only 1 pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_result = wordnet_result.append(word2vec_result)\n",
    "\n",
    "def sort_pair (row): \n",
    "    return \"_\".join(sorted([row['queries'], row['synonym']])) \n",
    "\n",
    "combined_result[\"sorted_pair\"] = combined_result.apply(lambda row: sort_pair(row), axis=1)\n",
    "final_result = combined_result.groupby(\"sorted_pair\").first().reset_index().drop([\"sorted_pair\"],axis=1)\n",
    "len(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
