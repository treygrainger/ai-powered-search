{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synonym detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: This notebook depends upon the the Retrotech dataset. If you have any issues, please rerun the [Setting up the Retrotech Dataset](../ch4/1.ch4-setting-up-the-retrotech-dataset.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim\n",
    "import pandas as pd\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# from nltk.corpus import wordnet \n",
    "# import re\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "# from gensim.models import Word2Vec\n",
    "# from collections import defaultdict\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.neighbors import NearestNeighbors\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"aips-ch6\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/notebooks/ch6\r\n"
     ]
    }
   ],
   "source": [
    "# !cp 'synonym detection-v2.ipynb' '/Users/kabdelfatah/Downloads/ai-powered-search/'\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals = pd.read_json(\"../data/temp/signal_sample.json\")\n",
    "signals.to_csv(\"../data/temp/signal_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_s</th>\n",
       "      <th>user_id_s</th>\n",
       "      <th>doc_id_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jillian micheals</td>\n",
       "      <td>b9acb9cf6d812521c48260f53f5febede8090e28</td>\n",
       "      <td>2620821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pda</td>\n",
       "      <td>7c2b958123c82df86321be5caf4dd7934f1880c1</td>\n",
       "      <td>1232905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sony radio</td>\n",
       "      <td>abda6be4b049ffb94ae07a6aa67c4ff5391b1125</td>\n",
       "      <td>7022839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>car</td>\n",
       "      <td>1e61403ce24dabe228850f4d7aca4e1f83d22252</td>\n",
       "      <td>7310411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pioneer speakers</td>\n",
       "      <td>872826507ab4c66ef099dc8a05606c131ad5c4fb</td>\n",
       "      <td>2994224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            query_s                                 user_id_s  doc_id_s\n",
       "0  jillian micheals  b9acb9cf6d812521c48260f53f5febede8090e28   2620821\n",
       "1               pda  7c2b958123c82df86321be5caf4dd7934f1880c1   1232905\n",
       "2        sony radio  abda6be4b049ffb94ae07a6aa67c4ff5391b1125   7022839\n",
       "3               car  1e61403ce24dabe228850f4d7aca4e1f83d22252   7310411\n",
       "4  pioneer speakers  872826507ab4c66ef099dc8a05606c131ad5c4fb   2994224"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile = \"../data/temp/signal_sample.csv\"\n",
    "csvDF = spark.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(csvFile)\n",
    "csvDF.registerTempTable('signals')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql('''select count(1) from signals where type='query' ''').show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword_click_product = spark.sql('''\n",
    "# select q.user, keyword,upc \n",
    "# from\n",
    "# (select user,query_id,lower(target) as keyword from signals\n",
    "# where type='query') as q\n",
    "# join\n",
    "# (select user, query_id,target as upc from signals\n",
    "# where type='click') as c on q.query_id = c.query_id and q.user=c.user\n",
    "# group by q.user, keyword,upc \n",
    "# ''')\n",
    "# keyword_click_product.registerTempTable('keyword_click_product')\n",
    "# spark.sql('''select count(1) as keyword_click_product from keyword_click_product''').show() #647684"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql('''\n",
    "# select query_s, count(1) as n_users from signals group by query_s\n",
    "# ''').registerTempTable('keyword_user_oc')\n",
    "# spark.sql('''select count(1) as keyword_user_oc from keyword_user_oc''').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql('''\n",
    "# select query1, query2,count(distinct user_id_s) as cooc from(\n",
    "#     select k1.query_s as query1, k2.query_s as query2, k1.user_id_s\n",
    "#     from\n",
    "#     signals as k1 \n",
    "#     join\n",
    "#     signals as k2\n",
    "#     on k1.user_id_s = k2.user_id_s\n",
    "#     where k1.query_s > k2.query_s \n",
    "# ) query_pair\n",
    "# group by query1, query2\n",
    "# ''').registerTempTable('query_pair_cooc')\n",
    "# spark.sql('''select count(1) as query_pair_cooc from query_pair_cooc''').show()\n",
    "# spark.sql('''select * from query_pair_cooc''').show(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "select lower(query_s) as keyword, user_id_s, doc_id_s as upc from signals\n",
    "''').registerTempTable('keyword_click_product')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|keyword_click_product_cooc|\n",
      "+--------------------------+\n",
      "|                      9318|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "select k1.keyword as k1, k2.keyword as k2, sum(p1) n_users1,sum(p2) n_users1, sum(p1+p2) as users_cooc, count(1) n_products\n",
    "from\n",
    "(select keyword, upc, count(1) as p1 from keyword_click_product group by keyword, upc) as k1 \n",
    "join\n",
    "(select keyword, upc, count(1) as p2 from keyword_click_product group by keyword, upc) as k2\n",
    "on k1.upc = k2.upc\n",
    "where k1.keyword > k2.keyword \n",
    "group by k1.keyword, k2.keyword\n",
    "''').registerTempTable('keyword_click_product_cooc')\n",
    "spark.sql('''select count(1) as keyword_click_product_cooc from keyword_click_product_cooc''').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|keyword_click_product_oc|\n",
      "+------------------------+\n",
      "|                    4886|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "select keyword, count(1) as n_users from keyword_click_product group by keyword\n",
    "''').registerTempTable('keyword_click_product_oc')\n",
    "spark.sql('''select count(1) as keyword_click_product_oc from keyword_click_product_oc''').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|related_keywords_pmi|\n",
      "+--------------------+\n",
      "|                9318|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "select k1.keyword as k1, k2.keyword as k2, k1_k2.users_cooc, k1.n_users as n_users1,k2.n_users as n_users2,\n",
    "k1_k2.users_cooc/(k1.n_users*k2.n_users) as pmi2\n",
    "from\n",
    "keyword_click_product_cooc as k1_k2 \n",
    "join\n",
    "keyword_click_product_oc as k1 on k1_k2.k1 = k1.keyword\n",
    "join\n",
    "keyword_click_product_oc as k2 on k1_k2.k2 = k2.keyword\n",
    "''').registerTempTable('related_keywords_pmi')\n",
    "spark.sql('''select count(1) as related_keywords_pmi from related_keywords_pmi''').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+----+\n",
      "|    k1|                  k2|                pmi2|rnum|\n",
      "+------+--------------------+--------------------+----+\n",
      "|laptop|        apple laptop|              0.0325|   1|\n",
      "|laptop|               apple|0.019444444444444445|   2|\n",
      "|  ipad|   hp with dre beats| 0.36585365853658536|   1|\n",
      "|  ipad|        apple ipad 2| 0.36585365853658536|   2|\n",
      "|  ipad|     apple computers| 0.18292682926829268|   3|\n",
      "|  ipad|             i pad 2| 0.17073170731707318|   4|\n",
      "|  ipad|               i pad| 0.09024390243902439|   5|\n",
      "|  ipad|          apple ipad| 0.08536585365853659|   6|\n",
      "|  ipad|             hp ipad| 0.06707317073170732|   7|\n",
      "|  ipad|            hp touch| 0.03902439024390244|   8|\n",
      "|  ipad|        hp touch pad|0.028455284552845527|   9|\n",
      "|  ipad|               apple| 0.02100271002710027|  10|\n",
      "|  ipad|         hp touchpad|0.018847006651884702|  11|\n",
      "|  ipad|           hp tablet| 0.01662971175166297|  12|\n",
      "|  ipad|                  hp| 0.01125703564727955|  13|\n",
      "|lcd tv|    jillian micheals| 0.47058823529411764|   1|\n",
      "|lcd tv|               dynex| 0.11764705882352941|   2|\n",
      "|lcd tv|               1080p| 0.10588235294117647|   3|\n",
      "|lcd tv|2371074 2126339 2...| 0.01019607843137255|   4|\n",
      "+------+--------------------+--------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.sql('''\n",
    "select * from(\n",
    "select k1, k2, pmi2, \n",
    "row_number() over (PARTITION BY k1 order by pmi2 desc ) rnum\n",
    "from related_keywords_pmi \n",
    "where users_cooc > 10 and pmi2 > 0.01 \n",
    "and k1 in ('lcd tv', 'ipad', 'laptop', 'iphone 4') ) x where rnum <= 20 ''').show(500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: data cleaning\n",
    "perform minimum stemming on queries and drop rare queries with count of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# product_description = pd.read_json(\"../data/temp/product_description.json\")\n",
    "# signals = pd.read_json(\"../data/temp/signal_sample.json\")\n",
    "# signals[\"query\"] = signals[\"query_s\"].apply(lambda x: re.sub(\"s$\",\"\", x.lower())) #conduct minimum stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile = \"../data/retrotech/products.csv\"\n",
    "import csv\n",
    "reader = csv.reader(open(csvFile,'r'))\n",
    "products = []\n",
    "prod2 = []\n",
    "for r in reader:\n",
    "    if len(r)> 5:\n",
    "        prod2.append(r)\n",
    "        continue\n",
    "    products.append(r)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['upc', 'name', 'manufacturer', 'shortDescription', 'longDescription'],\n",
       " ['096009010836', 'Fists of Bruce Lee - Dolby - DVD', '\\\\N', '\\\\N', '\\\\N']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(products), len(prod2)\n",
    "products[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aggr_signals' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d3fd6808fb26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maggr_signals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maggr_signals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maggr_signals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"count\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0maggr_signals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'aggr_signals' is not defined"
     ]
    }
   ],
   "source": [
    "aggr_signals = aggr_signals[aggr_signals[\"count\"] > 1]\n",
    "aggr_signals.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: find synonym from wordnet\n",
    "get candidate from wordnet if both query and candidate are included in wordnet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-38afd5779e3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mqueries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maggr_signals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"query\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msynset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlemma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "synonym = []\n",
    "queries = []\n",
    "\n",
    "for query in tqdm(aggr_signals[\"query\"]):\n",
    "    for synset in wordnet.synsets(query):\n",
    "        for lemma in synset.lemmas():\n",
    "            candidate = lemma.name().replace(\"_\", \" \")\n",
    "            if candidate in list(aggr_signals[\"query\"]) and candidate != query:\n",
    "                queries.append(query)\n",
    "                synonym.append(candidate)\n",
    "                \n",
    "wordnet_result = pd.DataFrame({\"queries\":queries,\"synonym\":synonym}).drop_duplicates()\n",
    "wordnet_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: find synonym from word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RegexpTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-2907b474705e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# combine queries and product descriptions to be fed into word2vec model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\w+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtokenized_description\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproduct_description\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlongDescription\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenized_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maggr_signals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"query\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenized_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenized_query\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtokenized_description\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RegexpTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# combine queries and product descriptions to be fed into word2vec model\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenized_description = [tokenizer.tokenize(text.lower()) for text in product_description.longDescription]\n",
    "tokenized_query = [tokenizer.tokenize(text) for text in aggr_signals[\"query\"]]\n",
    "tokenized_all = tokenized_query + tokenized_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Word2Vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-c936f0bb71be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# compute word2vec for each word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mword2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_to_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Word2Vec' is not defined"
     ]
    }
   ],
   "source": [
    "# compute word2vec for each word\n",
    "model = Word2Vec(tokenized_all, vector_size=150, window=8, min_count=1, workers=-1)\n",
    "word2vec = dict(zip(model.wv.index_to_key, model.wv.vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfidfVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-775b1262ff78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# build word weights dictionary to weigh word vectors, so that rare words get more weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TfidfVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "# build word weights dictionary to weigh word vectors, so that rare words get more weights\n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "tfidf.fit(tokenized_all)\n",
    "\n",
    "max_idf = max(tfidf.idf_) #the default idf is the max of idf's for unseen words\n",
    "weights = defaultdict(lambda: max_idf, [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c0273fa9909d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mquery_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenized_query_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_query\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mqueries_cleaned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_query_clean\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_query_clean\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_query' is not defined"
     ]
    }
   ],
   "source": [
    "query_vectors = []\n",
    "tokenized_query_clean = [x for x in tokenized_query if x != []]\n",
    "queries_cleaned = [\" \".join(x) for x in tokenized_query_clean]\n",
    "\n",
    "for tokens in tokenized_query_clean:\n",
    "    tmp = []\n",
    "    for token in tokens:\n",
    "        if token in word2vec.keys():\n",
    "            tmp.append(word2vec[token] * weights[token])\n",
    "        else:\n",
    "            tmp.append(np.zeros(150))\n",
    "    query_vectors.append(np.mean(tmp, axis=0).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NearestNeighbors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-901cd7fd43f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mknn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNearestNeighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cosine'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'brute'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NearestNeighbors' is not defined"
     ]
    }
   ],
   "source": [
    "knn = NearestNeighbors(n_neighbors=5, metric='cosine', algorithm='brute', n_jobs=-1)\n",
    "knn.fit(np.stack(query_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-51b1c3d932af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcosine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries_cleaned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0msynonym_candidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mquery_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msynonym_candidates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#drop first candidate which is the same of the original query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "queries = []\n",
    "synonym = []\n",
    "cosine = []\n",
    "\n",
    "for i in tqdm(range(len(queries_cleaned))):\n",
    "    synonym_candidates = knn.kneighbors(np.expand_dims(query_vectors[i],axis=0), n_neighbors=6)\n",
    "    query_index = synonym_candidates[1][0].tolist()[1:] #drop first candidate which is the same of the original query\n",
    "    cosine_similarity = [1-x for x in synonym_candidates[0][0].tolist()[1:]]\n",
    "    query = queries_cleaned[i]\n",
    "    for j in range(0,5):\n",
    "        queries.append(query)\n",
    "        synonym.append(queries_cleaned[query_index[j]])\n",
    "        cosine.append(cosine_similarity[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>queries</th>\n",
       "      <th>synonym</th>\n",
       "      <th>cosine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [queries, synonym, cosine]\n",
       "Index: []"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonym_candidates = pd.DataFrame({\"queries\":queries, \"synonym\":synonym, \"cosine\":cosine})\n",
    "synonym_candidates.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>queries</th>\n",
       "      <th>synonym</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [queries, synonym]\n",
       "Index: []"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply cosine similarity threshold to further filter candidate synonyms\n",
    "cosine_threshold = 0.7\n",
    "word2vec_result = synonym_candidates[synonym_candidates.cosine >= cosine_threshold][[\"queries\",\"synonym\"]]\n",
    "word2vec_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: combine wordnet and word2vec lists\n",
    "the pair query A and synonym B is the same as the pair query B and synonym A, sort query and synonym to keep only 1 pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wordnet_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-8bd50a95cbd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcombined_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordnet_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2vec_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msort_pair\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"_\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'queries'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'synonym'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wordnet_result' is not defined"
     ]
    }
   ],
   "source": [
    "combined_result = wordnet_result.append(word2vec_result)\n",
    "\n",
    "def sort_pair (row): \n",
    "    return \"_\".join(sorted([row['queries'], row['synonym']])) \n",
    "\n",
    "combined_result[\"sorted_pair\"] = combined_result.apply(lambda row: sort_pair(row), axis=1)\n",
    "final_result = combined_result.groupby(\"sorted_pair\").first().reset_index().drop([\"sorted_pair\"],axis=1)\n",
    "len(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-5ad65733a493>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinal_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'final_result' is not defined"
     ]
    }
   ],
   "source": [
    "final_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
