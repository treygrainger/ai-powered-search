{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synonym detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: This notebook depends upon the the Retrotech dataset. If you have any issues, please rerun the [Setting up the Retrotech Dataset](../ch4/1.ch4-setting-up-the-retrotech-dataset.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim\n",
    "import pandas as pd\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# from nltk.corpus import wordnet \n",
    "# import re\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "# from gensim.models import Word2Vec\n",
    "# from collections import defaultdict\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.neighbors import NearestNeighbors\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"aips-ch6\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile = \"../data/retrotech/signals.csv\" \n",
    "csvDF = spark.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(csvFile)\n",
    "csvDF.registerTempTable('signals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      11|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''select count(1) from signals where target='3184708' and type='query' ''').show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|keyword_click_product|\n",
      "+---------------------+\n",
      "|               647553|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "keyword_click_product = spark.sql('''\n",
    "select q.user, keyword,upc \n",
    "from\n",
    "(select user,query_id,lower(target) as keyword from signals\n",
    "where type='query') as q\n",
    "join\n",
    "(select user, query_id,target as upc from signals\n",
    "where type='click') as c on q.query_id = c.query_id and q.user=c.user\n",
    "group by q.user, keyword,upc \n",
    "''')\n",
    "keyword_click_product.registerTempTable('keyword_click_product')\n",
    "spark.sql('''select count(1) as keyword_click_product from keyword_click_product''').show() #647684"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|keyword_click_product_oc|\n",
      "+------------------------+\n",
      "|                   13744|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "select keyword, count(1) as n_users from keyword_click_product group by keyword\n",
    "''').registerTempTable('keyword_click_product_oc')\n",
    "spark.sql('''select count(1) as keyword_click_product_oc from keyword_click_product_oc''').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|keyword_click_product_cooc|\n",
      "+--------------------------+\n",
      "|                   1579710|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "select k1.keyword as k1, k2.keyword as k2, sum(p1) n_users1,sum(p2) n_users1, sum(p1+p2) as users_cooc, count(1) n_products\n",
    "from\n",
    "(select keyword, upc, count(1) as p1 from keyword_click_product group by keyword, upc) as k1 \n",
    "join\n",
    "(select keyword, upc, count(1) as p2 from keyword_click_product group by keyword, upc) as k2\n",
    "on k1.upc = k2.upc\n",
    "where k1.keyword > k2.keyword \n",
    "group by k1.keyword, k2.keyword\n",
    "''').registerTempTable('keyword_click_product_cooc')\n",
    "spark.sql('''select count(1) as keyword_click_product_cooc from keyword_click_product_cooc''').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "select k1.keyword as k1, k2.keyword as k2, k1_k2.users_cooc, k1.n_users as n_users1,k2.n_users as n_users2,\n",
    "k1_k2.users_cooc/(k1.n_users*k2.n_users) as pmi2\n",
    "from\n",
    "keyword_click_product_cooc as k1_k2 \n",
    "join\n",
    "keyword_click_product_oc as k1 on k1_k2.k1 = k1.keyword\n",
    "join\n",
    "keyword_click_product_oc as k2 on k1_k2.k2 = k2.keyword\n",
    "''').registerTempTable('related_keywords_pmi')\n",
    "spark.sql('''select count(1) as related_keywords_pmi from related_keywords_pmi''').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.sql('''\n",
    "select * from(\n",
    "select k1, k2, pmi2, \n",
    "row_number() over (PARTITION BY k1 order by pmi2 desc ) rnum\n",
    "from related_keywords_pmi \n",
    "where users_cooc > 10 and pmi2 > 0.01 \n",
    "and k1 in ('lcd tv', 'ipad', 'laptop', 'iphone 4') ) x where rnum <= 20 ''').show(500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''select * from related_keywords_pmi where users_cooc > 10 and pmi2 > 0.01 limit 500''').show(500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: data cleaning\n",
    "perform minimum stemming on queries and drop rare queries with count of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# product_description = pd.read_json(\"../data/temp/product_description.json\")\n",
    "# signals = pd.read_json(\"../data/temp/signal_sample.json\")\n",
    "# signals[\"query\"] = signals[\"query_s\"].apply(lambda x: re.sub(\"s$\",\"\", x.lower())) #conduct minimum stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile = \"../data/retrotech/products.csv\"\n",
    "import csv\n",
    "reader = csv.reader(open(csvFile,'r'))\n",
    "products = []\n",
    "prod2 = []\n",
    "for r in reader:\n",
    "    if len(r)> 5:\n",
    "        prod2.append(r)\n",
    "        continue\n",
    "    products.append(r)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['upc', 'name', 'manufacturer', 'shortDescription', 'longDescription'],\n",
       " ['096009010836', 'Fists of Bruce Lee - Dolby - DVD', '\\\\N', '\\\\N', '\\\\N']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(products), len(prod2)\n",
    "products[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggr_signals = aggr_signals[aggr_signals[\"count\"] > 1]\n",
    "aggr_signals.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: find synonym from wordnet\n",
    "get candidate from wordnet if both query and candidate are included in wordnet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonym = []\n",
    "queries = []\n",
    "\n",
    "for query in tqdm(aggr_signals[\"query\"]):\n",
    "    for synset in wordnet.synsets(query):\n",
    "        for lemma in synset.lemmas():\n",
    "            candidate = lemma.name().replace(\"_\", \" \")\n",
    "            if candidate in list(aggr_signals[\"query\"]) and candidate != query:\n",
    "                queries.append(query)\n",
    "                synonym.append(candidate)\n",
    "                \n",
    "wordnet_result = pd.DataFrame({\"queries\":queries,\"synonym\":synonym}).drop_duplicates()\n",
    "wordnet_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: find synonym from word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine queries and product descriptions to be fed into word2vec model\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenized_description = [tokenizer.tokenize(text.lower()) for text in product_description.longDescription]\n",
    "tokenized_query = [tokenizer.tokenize(text) for text in aggr_signals[\"query\"]]\n",
    "tokenized_all = tokenized_query + tokenized_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute word2vec for each word\n",
    "model = Word2Vec(tokenized_all, vector_size=150, window=8, min_count=1, workers=-1)\n",
    "word2vec = dict(zip(model.wv.index_to_key, model.wv.vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build word weights dictionary to weigh word vectors, so that rare words get more weights\n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "tfidf.fit(tokenized_all)\n",
    "\n",
    "max_idf = max(tfidf.idf_) #the default idf is the max of idf's for unseen words\n",
    "weights = defaultdict(lambda: max_idf, [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vectors = []\n",
    "tokenized_query_clean = [x for x in tokenized_query if x != []]\n",
    "queries_cleaned = [\" \".join(x) for x in tokenized_query_clean]\n",
    "\n",
    "for tokens in tokenized_query_clean:\n",
    "    tmp = []\n",
    "    for token in tokens:\n",
    "        if token in word2vec.keys():\n",
    "            tmp.append(word2vec[token] * weights[token])\n",
    "        else:\n",
    "            tmp.append(np.zeros(150))\n",
    "    query_vectors.append(np.mean(tmp, axis=0).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = NearestNeighbors(n_neighbors=5, metric='cosine', algorithm='brute', n_jobs=-1)\n",
    "knn.fit(np.stack(query_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = []\n",
    "synonym = []\n",
    "cosine = []\n",
    "\n",
    "for i in tqdm(range(len(queries_cleaned))):\n",
    "    synonym_candidates = knn.kneighbors(np.expand_dims(query_vectors[i],axis=0), n_neighbors=6)\n",
    "    query_index = synonym_candidates[1][0].tolist()[1:] #drop first candidate which is the same of the original query\n",
    "    cosine_similarity = [1-x for x in synonym_candidates[0][0].tolist()[1:]]\n",
    "    query = queries_cleaned[i]\n",
    "    for j in range(0,5):\n",
    "        queries.append(query)\n",
    "        synonym.append(queries_cleaned[query_index[j]])\n",
    "        cosine.append(cosine_similarity[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonym_candidates = pd.DataFrame({\"queries\":queries, \"synonym\":synonym, \"cosine\":cosine})\n",
    "synonym_candidates.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply cosine similarity threshold to further filter candidate synonyms\n",
    "cosine_threshold = 0.7\n",
    "word2vec_result = synonym_candidates[synonym_candidates.cosine >= cosine_threshold][[\"queries\",\"synonym\"]]\n",
    "word2vec_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: combine wordnet and word2vec lists\n",
    "the pair query A and synonym B is the same as the pair query B and synonym A, sort query and synonym to keep only 1 pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_result = wordnet_result.append(word2vec_result)\n",
    "\n",
    "def sort_pair (row): \n",
    "    return \"_\".join(sorted([row['queries'], row['synonym']])) \n",
    "\n",
    "combined_result[\"sorted_pair\"] = combined_result.apply(lambda row: sort_pair(row), axis=1)\n",
    "final_result = combined_result.groupby(\"sorted_pair\").first().reset_index().drop([\"sorted_pair\"],axis=1)\n",
    "len(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
